{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import RawTextHelpFormatter\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes \n",
    "from torch_geometric.loader import DataLoader\n",
    "from math import ceil\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv,GCNConv, MLP, DenseGINConv,DenseGCNConv, PANConv,dense_mincut_pool, dense_diff_pool, DMoNPooling,global_add_pool,TopKPooling, PANPooling, SAGPooling, ASAPooling, EdgePooling,graclus,MemPooling\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch,add_self_loops, scatter,degree, to_undirected,softmax\n",
    "from typing import Callable, Optional, Tuple, Union,Any\n",
    "from torch_geometric.data import Batch, Data,InMemoryDataset\n",
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "from torch_geometric.nn.pool.pool import pool_batch, pool_edge, pool_pos\n",
    "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
    "from torch.nn import Module,Linear\n",
    "from torch_scatter import scatter, scatter_add, scatter_min ,scatter_max\n",
    "from torch_sparse import SparseTensor, remove_diag\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.nn.dense import Linear\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(pooling='comp-graclus', pool_ratio=0.1, batch_size=32, hidden_channels=64, num_layers_pre=2, num_layers_post=1, lr=0.0001, epochs=500, runs=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n",
    "parser.add_argument('--pooling', type=str, default='comp-graclus',\n",
    "                    help=\"Options:\\n None (no-pool)\\n 'diffpool'\\n 'mincut'\\n\" \n",
    "                    \" 'dmon'\\n 'edgepool'\\n 'graclus'\\n 'kmis'\\n 'topk'\\n 'panpool'\\n\"\n",
    "                    \" 'asapool'\\n 'sagpool'\\n 'dense-random'\\n 'sparse-random'\\n\"\n",
    "                    \" 'comp-graclus'\\n\")\n",
    "parser.add_argument('--pool_ratio', type=float, default=0.1)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--hidden_channels', type=int, default=64)\n",
    "parser.add_argument('--num_layers_pre', type=int, default=2)\n",
    "parser.add_argument('--num_layers_post', type=int, default=1)\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--epochs', type=int, default=500)\n",
    "parser.add_argument('--runs', type=int, default=1)\n",
    "\n",
    "args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataToFloat(BaseTransform):\n",
    "    def __call__(self, data):\n",
    "        data.x = data.x.to(torch.float32)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXPWL1Dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super(EXPWL1Dataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\"EXPWL1.pkl\"]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt' \n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = pickle.load(open(os.path.join(self.root, \"raw/EXPWL1.pkl\"), \"rb\"))\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"EXPWL1/\"\n",
    "dataset = EXPWL1Dataset(path, transform=DataToFloat()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\un\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# compute avg number of nodes\n",
    "avg_nodes = int(dataset.data.num_nodes/len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute max number of nodes\n",
    "max_nodes = 0\n",
    "for d in dataset:\n",
    "    max_nodes = max(d.num_nodes, max_nodes)\n",
    "if args.pooling == 'sparse-random':\n",
    "    max_nodes *= args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_edges(edge_index, num_nodes=None, num_neg_samples=None,\n",
    "                      force_undirected=False):\n",
    "\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    # Handle '|V|^2 - |E| < |E|' case for G = (V, E).\n",
    "    num_neg_samples = num_nodes * num_nodes - edge_index.size(1)\n",
    "\n",
    "    if force_undirected:\n",
    "        num_neg_samples = num_neg_samples // 2\n",
    "\n",
    "        # Upper triangle indices: N + ... + 1 = N (N + 1) / 2\n",
    "        rng = range((num_nodes * (num_nodes + 1)) // 2)\n",
    "\n",
    "        # Remove edges in the lower triangle matrix.\n",
    "        row, col = edge_index\n",
    "        mask = row <= col\n",
    "        row, col = row[mask], col[mask]\n",
    "\n",
    "        # idx = N * i + j - i * (i+1) / 2\n",
    "        idx = (row * num_nodes + col - row * (row + 1) // 2).to('cpu')\n",
    "    else:\n",
    "        rng = range(num_nodes**2)\n",
    "        # idx = N * i + j\n",
    "        idx = (edge_index[0] * num_nodes + edge_index[1]).to('cpu')\n",
    "\n",
    "    perm = torch.tensor(rng)\n",
    "    mask = torch.from_numpy(np.isin(perm, idx)).to(torch.bool)\n",
    "    rest = mask.nonzero().view(-1)\n",
    "    while rest.numel() > 0:  # pragma: no cover\n",
    "        tmp = torch.tensor(random.sample(rng, rest.size(0)))\n",
    "        mask = torch.from_numpy(np.isin(tmp, idx)).to(torch.bool)\n",
    "        perm[rest] = tmp\n",
    "        rest = rest[mask.nonzero().view(-1)]\n",
    "\n",
    "    if force_undirected:\n",
    "        # (-sqrt((2 * N + 1)^2 - 8 * perm) + 2 * N + 1) / 2\n",
    "        row = torch.floor((-torch.sqrt((2. * num_nodes + 1.)**2 - 8. * perm) +\n",
    "                           2 * num_nodes + 1) / 2)\n",
    "        col = perm - row * (2 * num_nodes - row - 1) // 2\n",
    "        neg_edge_index = torch.stack([row, col], dim=0).long()\n",
    "        neg_edge_index = to_undirected(neg_edge_index)\n",
    "    else:\n",
    "        row = perm / num_nodes\n",
    "        col = perm % num_nodes\n",
    "        neg_edge_index = torch.stack([row, col], dim=0).long()\n",
    "\n",
    "    return neg_edge_index.to(edge_index.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_negative_edges(edge_index, batch, num_neg_samples=None,\n",
    "                              force_undirected=False):\n",
    "  \n",
    "    split = degree(batch[edge_index[0]], dtype=torch.long).tolist()\n",
    "    edge_indices = torch.split(edge_index, split, dim=1)\n",
    "    num_nodes = degree(batch, dtype=torch.long)\n",
    "    cum_nodes = torch.cat([batch.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]])\n",
    "\n",
    "    neg_edge_indices = []\n",
    "    for edge_index, N, C in zip(edge_indices, num_nodes.tolist(),\n",
    "                                cum_nodes.tolist()):\n",
    "        neg_edge_index = negative_edges(edge_index - C, N, num_neg_samples,\n",
    "                                           force_undirected) + C\n",
    "        neg_edge_indices.append(neg_edge_index)\n",
    "\n",
    "    return torch.cat(neg_edge_indices, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sum_pool_x(\n",
    "    cluster: Tensor,\n",
    "    x: Tensor,\n",
    "    size: Optional[int] = None,\n",
    ") -> Tensor:\n",
    "    return scatter(x, cluster, dim=0, dim_size=size, reduce='add')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sum_pool_x(\n",
    "    cluster: Tensor,\n",
    "    x: Tensor,\n",
    "    batch: Tensor,\n",
    "    size: Optional[int] = None,\n",
    ") -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "    if size is not None:\n",
    "        batch_size = int(batch.max().item()) + 1\n",
    "        return _sum_pool_x(cluster, x, batch_size * size), None\n",
    "\n",
    "    cluster, perm = consecutive_cluster(cluster)\n",
    "    x = _sum_pool_x(cluster, x)\n",
    "    batch = pool_batch(perm, batch)\n",
    "\n",
    "    return x, batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_pool(cluster: Tensor,data: Data,transform: Optional[Callable] = None,) -> Data:\n",
    "\n",
    "    cluster, perm = consecutive_cluster(cluster)\n",
    "\n",
    "    x = None if data.x is None else _sum_pool_x(cluster, data.x)\n",
    "    index, attr = pool_edge(cluster, data.edge_index, data.edge_attr)\n",
    "    batch = None if data.batch is None else pool_batch(perm, data.batch)\n",
    "    pos = None if data.pos is None else pool_pos(cluster, data.pos)\n",
    "\n",
    "    data = Batch(batch=batch, x=x, edge_index=index, edge_attr=attr, pos=pos)\n",
    "\n",
    "    if transform is not None:\n",
    "        data = transform(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RndSparse(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ratio: Union[int, float] = 0.5,\n",
    "        max_nodes=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ratio = ratio\n",
    "        self.min_score = None\n",
    "        self.score = torch.randn(max_nodes)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        edge_attr: Optional[Tensor] = None,\n",
    "        batch: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Tensor, Optional[Tensor], Tensor, Tensor, Tensor]:\n",
    "\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        \n",
    "        score = softmax(self.score[:x.size(0)].to(x.device), batch)\n",
    "\n",
    "        perm = topk(score, self.ratio, batch, self.min_score)\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_attr = filter_adj(edge_index, edge_attr, perm,\n",
    "                                           num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, perm, score[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_independent_set(edge_index: Adj, k: int = 1,\n",
    "                            perm: OptTensor = None) -> Tensor:\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "        device = edge_index.device()\n",
    "        n = edge_index.size(0)\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        device = row.device\n",
    "        n = edge_index.max().item() + 1\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    mis = torch.zeros(n, dtype=torch.bool, device=device)\n",
    "    mask = mis.clone()\n",
    "    min_rank = rank.clone()\n",
    "\n",
    "    while not mask.all():\n",
    "        for _ in range(k):\n",
    "            min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "            scatter_min(min_rank[row], col, out=min_neigh)\n",
    "            torch.minimum(min_neigh, min_rank, out=min_rank)  # self-loops\n",
    "\n",
    "        mis = mis | torch.eq(rank, min_rank)\n",
    "        mask = mis.clone().byte()\n",
    "\n",
    "        for _ in range(k):\n",
    "            max_neigh = torch.full_like(mask, fill_value=0)\n",
    "            scatter_max(mask[row], col, out=max_neigh)\n",
    "            torch.maximum(max_neigh, mask, out=mask)  # self-loops\n",
    "\n",
    "        mask = mask.to(dtype=torch.bool)\n",
    "        min_rank = rank.clone()\n",
    "        min_rank[mask] = n\n",
    "\n",
    "    return mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n",
    "                                    perm: OptTensor = None) -> PairTensor:\n",
    "    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n",
    "    n, device = mis.size(0), mis.device\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n",
    "    rank_mis = rank[mis]\n",
    "    min_rank[mis] = rank_mis\n",
    "\n",
    "    for _ in range(k):\n",
    "        min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "        scatter_min(min_rank[row], col, out=min_neigh)\n",
    "        torch.minimum(min_neigh, min_rank, out=min_rank)\n",
    "\n",
    "    _, clusters = torch.unique(min_rank, return_inverse=True)\n",
    "    perm = torch.argsort(rank_mis)\n",
    "    return mis, perm[clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMISPooling(Module):\n",
    "\n",
    "    _heuristics = {None, 'greedy', 'w-greedy'}\n",
    "    _passthroughs = {None, 'before', 'after'}\n",
    "    _scorers = {\n",
    "        'linear',\n",
    "        'random',\n",
    "        'constant',\n",
    "        'canonical',\n",
    "        'first',\n",
    "        'last',\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n",
    "                 scorer: Union[Scorer, str] = 'linear',\n",
    "                 score_heuristic: Optional[str] = 'greedy',\n",
    "                 score_passthrough: Optional[str] = 'before',\n",
    "                 aggr_x: Optional[Union[str, Aggregation]] = None,\n",
    "                 aggr_edge: str = 'sum',\n",
    "                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n",
    "                 remove_self_loops: bool = True) -> None:\n",
    "        super(KMISPooling, self).__init__()\n",
    "        assert score_heuristic in self._heuristics, \\\n",
    "            \"Unrecognized `score_heuristic` value.\"\n",
    "        assert score_passthrough in self._passthroughs, \\\n",
    "            \"Unrecognized `score_passthrough` value.\"\n",
    "\n",
    "        if not callable(scorer):\n",
    "            assert scorer in self._scorers, \\\n",
    "                \"Unrecognized `scorer` value.\"\n",
    "\n",
    "        self.k = k\n",
    "        self.scorer = scorer\n",
    "        self.score_heuristic = score_heuristic\n",
    "        self.score_passthrough = score_passthrough\n",
    "\n",
    "        self.aggr_x = aggr_x\n",
    "        self.aggr_edge = aggr_edge\n",
    "        self.aggr_score = aggr_score\n",
    "        self.remove_self_loops = remove_self_loops\n",
    "\n",
    "        if scorer == 'linear':\n",
    "            assert self.score_passthrough is not None, \\\n",
    "                \"`'score_passthrough'` must not be `None`\" \\\n",
    "                \" when using `'linear'` scorer\"\n",
    "\n",
    "            self.lin = Linear(in_channels=in_channels, out_channels=1,\n",
    "                              weight_initializer='uniform')\n",
    "\n",
    "    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n",
    "        if self.score_heuristic is None:\n",
    "            return x\n",
    "\n",
    "        row, col, _ = adj.coo()\n",
    "        x = x.view(-1)\n",
    "\n",
    "        if self.score_heuristic == 'greedy':\n",
    "            k_sums = torch.ones_like(x)\n",
    "        else:\n",
    "            k_sums = x.clone()\n",
    "\n",
    "        for _ in range(self.k):\n",
    "            scatter_add(k_sums[row], col, out=k_sums)\n",
    "\n",
    "        return x / k_sums\n",
    "\n",
    "    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) -> Tensor:\n",
    "        if self.scorer == 'linear':\n",
    "            return self.lin(x).sigmoid()\n",
    "\n",
    "        if self.scorer == 'random':\n",
    "            return torch.rand((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'constant':\n",
    "            return torch.ones((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'canonical':\n",
    "            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n",
    "\n",
    "        if self.scorer == 'first':\n",
    "            return x[..., [0]]\n",
    "\n",
    "        if self.scorer == 'last':\n",
    "            return x[..., [-1]]\n",
    "\n",
    "        return self.scorer(x, edge_index, edge_attr, batch)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) \\\n",
    "            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n",
    "        adj, n = edge_index, x.size(0)\n",
    "\n",
    "        if not isinstance(edge_index, SparseTensor):\n",
    "            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n",
    "\n",
    "        score = self._scorer(x, edge_index, edge_attr, batch)\n",
    "        updated_score = self._apply_heuristic(score, adj)\n",
    "        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n",
    "        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n",
    "\n",
    "        row, col, val = adj.coo()\n",
    "        c = mis.sum()\n",
    "\n",
    "        if val is None:\n",
    "            val = torch.ones_like(row, dtype=torch.float)\n",
    "\n",
    "        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n",
    "                           is_sorted=False,\n",
    "                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n",
    "\n",
    "        if self.remove_self_loops:\n",
    "            adj = remove_diag(adj)\n",
    "\n",
    "        if self.score_passthrough == 'before':\n",
    "            x = self.aggr_score(x, score)\n",
    "\n",
    "        if self.aggr_x is None:\n",
    "            x = x[mis]\n",
    "        elif isinstance(self.aggr_x, str):\n",
    "            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n",
    "                        reduce=self.aggr_x)\n",
    "        else:\n",
    "            x = self.aggr_x(x, cluster, dim_size=c)\n",
    "\n",
    "        if self.score_passthrough == 'after':\n",
    "            x = self.aggr_score(x, score[mis])\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_index, edge_attr = adj, None\n",
    "            \n",
    "        else:\n",
    "            row, col, edge_attr = adj.coo()\n",
    "            edge_index = torch.stack([row, col])\n",
    "\n",
    "        if batch is not None:\n",
    "            batch = batch[mis]\n",
    "            \n",
    "\n",
    "        return x, edge_index, edge_attr, batch, mis, cluster\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.scorer == 'linear':\n",
    "            channels = f\"in_channels={self.lin.in_channels}, \"\n",
    "        else:\n",
    "            channels = \"\"\n",
    "\n",
    "        return f'{self.__class__.__name__}({channels}k={self.k})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN_Pool_Net(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,           # Size of node features\n",
    "                 out_channels,          # Number of classes\n",
    "                 num_layers_pre=1,      # Number of GIN layers before pooling\n",
    "                 num_layers_post=1,     # Number of GIN layers after pooling\n",
    "                 hidden_channels=64,    # Dimensionality of node embeddings\n",
    "                 norm=True,             # Normalise Layers in the GIN MLP\n",
    "                 activation='ELU',      # Activation of the MLP in GIN \n",
    "                 average_nodes=None,    # Needed for dense pooling methods\n",
    "                 max_nodes=None,        # Needed for random pool\n",
    "                 pooling=None,          # Pooling method\n",
    "                 pool_ratio=0.1,        # Ratio = nodes_after_pool/nodes_before_pool\n",
    "                 ):\n",
    "        super(GIN_Pool_Net, self).__init__()\n",
    "        \n",
    "        self.num_layers_pre = num_layers_pre\n",
    "        self.num_layers_post = num_layers_post\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.act = activation_resolver(activation)\n",
    "        self.pooling = pooling\n",
    "        self.pool_ratio = pool_ratio\n",
    "  \n",
    "        # Pre-pooling block            \n",
    "        self.conv_layers_pre = torch.nn.ModuleList()\n",
    "        if pooling=='panpool':\n",
    "            for _ in range(num_layers_pre):\n",
    "                self.conv_layers_pre.append(PANConv(in_channels, hidden_channels, filter_size=2))\n",
    "                in_channels = hidden_channels\n",
    "        else:\n",
    "            for _ in range(num_layers_pre):\n",
    "                mlp = MLP([in_channels, hidden_channels, hidden_channels], act=activation)\n",
    "                self.conv_layers_pre.append(GINConv(nn=mlp, train_eps=False))\n",
    "                in_channels = hidden_channels\n",
    "                        \n",
    "        # Pooling block\n",
    "        pooled_nodes = ceil(pool_ratio * average_nodes)\n",
    "        if pooling in ['diffpool','mincut']:\n",
    "            self.pool = Linear(hidden_channels, pooled_nodes)\n",
    "        elif pooling=='dmon':\n",
    "            self.pool = DMoNPooling(hidden_channels, pooled_nodes)\n",
    "        elif pooling=='dense-random':\n",
    "            self.s_rnd = torch.randn(max_nodes, pooled_nodes)\n",
    "            self.s_rnd.requires_grad = False\n",
    "        elif pooling=='topk':\n",
    "            self.pool = TopKPooling(hidden_channels, ratio=pool_ratio)\n",
    "        elif pooling=='panpool':\n",
    "            self.pool = PANPooling(hidden_channels, ratio=pool_ratio)\n",
    "        elif pooling=='sagpool':\n",
    "            self.pool = SAGPooling(hidden_channels, ratio=pool_ratio)\n",
    "        elif pooling=='asapool':\n",
    "            self.pool = ASAPooling(hidden_channels, ratio=pool_ratio)  \n",
    "        elif pooling=='edgepool':\n",
    "            self.pool = EdgePooling(hidden_channels)\n",
    "        elif pooling=='kmis':\n",
    "            self.pool = KMISPooling(hidden_channels, k=5, aggr_x='sum')\n",
    "        elif pooling in ['graclus', 'comp-graclus']:\n",
    "            pass\n",
    "        elif pooling=='sparse-random':\n",
    "            self.pool = RndSparse(pool_ratio, max_nodes)\n",
    "        else:\n",
    "            assert pooling==None\n",
    "        \n",
    "        # Post-pooling block\n",
    "        self.conv_layers_post = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers_post):\n",
    "            mlp = MLP([hidden_channels, hidden_channels, hidden_channels], act=activation, norm=None)\n",
    "            if pooling in ['diffpool','mincut','dmon','dense-random']:\n",
    "                self.conv_layers_post.append(DenseGINConv(nn=mlp, train_eps=False))                \n",
    "            else:\n",
    "                self.conv_layers_post.append(GINConv(nn=mlp, train_eps=False))\n",
    "\n",
    "        # Readout\n",
    "        self.mlp = MLP([hidden_channels, hidden_channels, hidden_channels//2, out_channels], \n",
    "                        act=activation,\n",
    "                        norm=None,\n",
    "                        dropout=0.5)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for (name, module) in self._modules.items():\n",
    "            try:\n",
    "                module.reset_parameters()\n",
    "            except AttributeError:\n",
    "                if name != 'act':\n",
    "                    for x in module:\n",
    "                        x.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x    \n",
    "        adj = data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        ### pre-pooling block\n",
    "        if self.pooling=='panpool':\n",
    "            M = adj\n",
    "            for layer in self.conv_layers_pre:  \n",
    "                x, M = layer(x, M)\n",
    "                x = self.act(x)\n",
    "        else:\n",
    "            for layer in self.conv_layers_pre:  \n",
    "                x = self.act(layer(x, adj))\n",
    "    \n",
    "        ### pooling block\n",
    "        if self.pooling in ['diffpool','mincut','dmon','dense-random']:\n",
    "            x, mask = to_dense_batch(x, batch)\n",
    "            adj = to_dense_adj(adj, batch)\n",
    "            if self.pooling=='diffpool':\n",
    "                s = self.pool(x)\n",
    "                x, adj, l1, l2 = dense_diff_pool(x, adj, s, mask)\n",
    "                aux_loss = 0.1*l1 + 0.1*l2\n",
    "            elif self.pooling=='mincut':\n",
    "                s = self.pool(x)\n",
    "                x, adj, l1, l2 = dense_mincut_pool(x, adj, s, mask)\n",
    "                aux_loss = 0.5*l1 + l2\n",
    "            elif self.pooling=='dmon':  \n",
    "                _, x, adj, l1, l2, l3 = self.pool(x, adj, mask)\n",
    "                aux_loss = 0.3*l1 + 0.3*l2 + 0.3*l3\n",
    "            elif self.pooling=='dense-random':\n",
    "                s = self.s_rnd[:x.size(1), :].unsqueeze(dim=0).expand(x.size(0), -1, -1).to(x.device)\n",
    "                x, adj, _, _ = dense_diff_pool(x, adj, s, mask)\n",
    "        elif self.pooling in ['topk', 'sagpool', 'sparse-random']:\n",
    "            x, adj, _, batch, _, _ = self.pool(x, adj, edge_attr=None, batch=batch)\n",
    "        elif self.pooling=='asapool':\n",
    "            x, adj, _, batch, _ = self.pool(x, adj, batch=batch)\n",
    "        elif self.pooling=='panpool':\n",
    "            x, adj, _, batch, _, _ = self.pool(x, M, batch=batch)\n",
    "        elif self.pooling=='edgepool':\n",
    "            x, adj, batch, _ = self.pool(x, adj, batch=batch)\n",
    "        elif self.pooling=='kmis':\n",
    "            x, adj, _, batch, _, _ = self.pool(x, adj, None, batch=batch)\n",
    "        elif self.pooling in ['graclus', 'comp-graclus']:\n",
    "            data.x = x    \n",
    "            if self.pooling == 'graclus':\n",
    "                cluster = graclus(adj, num_nodes=data.x.size(0))\n",
    "            else:\n",
    "                complement = batched_negative_edges(edge_index=adj, batch=batch, force_undirected=True)\n",
    "                cluster = graclus(complement, num_nodes=data.x.size(0))\n",
    "            data = sum_pool(cluster, data)\n",
    "            x = data.x    \n",
    "            adj = data.edge_index\n",
    "            batch = data.batch\n",
    "        elif self.pooling==None:\n",
    "            pass\n",
    "        else:\n",
    "            raise KeyError(\"unrecognized pooling method\")\n",
    "                \n",
    "        ### post-pooling block\n",
    "        for layer in self.conv_layers_post:  \n",
    "            x = self.act(layer(x, adj))\n",
    "\n",
    "        ### readout\n",
    "        if self.pooling in ['diffpool','mincut','dmon','dense-random']:\n",
    "            x = torch.sum(x, dim=1)\n",
    "        else:\n",
    "            x = global_add_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        if 'aux_loss' not in locals():\n",
    "            aux_loss=0\n",
    "        return F.log_softmax(x, dim=-1), aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, aux_loss = model(data)\n",
    "        loss = F.nll_loss(out, data.y) + aux_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out, _ = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        pred = out.argmax(dim=-1)\n",
    "        total_correct += int((pred == data.y).sum())\n",
    "    return total_correct / len(loader.dataset), total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(**kwargs):\n",
    "    def _map(value: Any) -> str:\n",
    "        if isinstance(value, int) and not isinstance(value, bool):\n",
    "            return f'{value:03d}'\n",
    "        if isinstance(value, float):\n",
    "            return f'{value:.4f}'\n",
    "        return value\n",
    "\n",
    "    print(', '.join(f'{key}: {_map(value)}' for key, value in kwargs.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch: 001, Loss: 0.7193, Train: 0.6325, Val: 0.6300, Test: 0.5300\n",
      "Epoch: 002, Loss: 0.6026, Train: 0.7283, Val: 0.7133, Test: 0.7033\n",
      "Epoch: 003, Loss: 0.5530, Train: 0.6221, Val: 0.6200, Test: 0.5300\n",
      "Epoch: 004, Loss: 0.5325, Train: 0.8042, Val: 0.7667, Test: 0.7567\n",
      "Epoch: 005, Loss: 0.4778, Train: 0.7833, Val: 0.7733, Test: 0.7900\n",
      "Epoch: 006, Loss: 0.4676, Train: 0.6879, Val: 0.7333, Test: 0.7400\n",
      "Epoch: 007, Loss: 0.4268, Train: 0.6471, Val: 0.6433, Test: 0.5400\n",
      "Epoch: 008, Loss: 0.3969, Train: 0.6871, Val: 0.7233, Test: 0.7400\n",
      "Epoch: 009, Loss: 0.3576, Train: 0.5958, Val: 0.5967, Test: 0.4900\n",
      "Epoch: 010, Loss: 0.3727, Train: 0.8617, Val: 0.8467, Test: 0.8200\n",
      "Epoch: 011, Loss: 0.3391, Train: 0.8429, Val: 0.8233, Test: 0.7867\n",
      "Epoch: 012, Loss: 0.3276, Train: 0.8929, Val: 0.8867, Test: 0.8800\n",
      "Epoch: 013, Loss: 0.3091, Train: 0.8008, Val: 0.8167, Test: 0.8567\n",
      "Epoch: 014, Loss: 0.2854, Train: 0.8567, Val: 0.8467, Test: 0.7967\n",
      "Epoch: 015, Loss: 0.2700, Train: 0.9217, Val: 0.9400, Test: 0.9200\n",
      "Epoch: 016, Loss: 0.2817, Train: 0.8113, Val: 0.7967, Test: 0.7300\n",
      "Epoch: 017, Loss: 0.2457, Train: 0.8658, Val: 0.8800, Test: 0.9000\n",
      "Epoch: 018, Loss: 0.2350, Train: 0.9117, Val: 0.9367, Test: 0.9300\n",
      "Epoch: 019, Loss: 0.2359, Train: 0.9350, Val: 0.9400, Test: 0.9233\n",
      "Epoch: 020, Loss: 0.2273, Train: 0.9279, Val: 0.9367, Test: 0.9433\n",
      "Epoch: 021, Loss: 0.2203, Train: 0.9396, Val: 0.9600, Test: 0.9400\n",
      "Epoch: 022, Loss: 0.2238, Train: 0.9375, Val: 0.9700, Test: 0.9433\n",
      "Epoch: 023, Loss: 0.1997, Train: 0.9475, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 024, Loss: 0.1854, Train: 0.9371, Val: 0.9333, Test: 0.9167\n",
      "Epoch: 025, Loss: 0.1840, Train: 0.9421, Val: 0.9333, Test: 0.9500\n",
      "Epoch: 026, Loss: 0.1628, Train: 0.9408, Val: 0.9500, Test: 0.9333\n",
      "Epoch: 027, Loss: 0.1597, Train: 0.9179, Val: 0.9000, Test: 0.8867\n",
      "Epoch: 028, Loss: 0.1860, Train: 0.9400, Val: 0.9733, Test: 0.9500\n",
      "Epoch: 029, Loss: 0.1706, Train: 0.9313, Val: 0.9300, Test: 0.9367\n",
      "Epoch: 030, Loss: 0.1745, Train: 0.9587, Val: 0.9900, Test: 0.9633\n",
      "Epoch: 031, Loss: 0.1476, Train: 0.9621, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 032, Loss: 0.1892, Train: 0.8458, Val: 0.8533, Test: 0.8900\n",
      "Epoch: 033, Loss: 0.1690, Train: 0.9450, Val: 0.9533, Test: 0.9600\n",
      "Epoch: 034, Loss: 0.1440, Train: 0.9558, Val: 0.9333, Test: 0.9600\n",
      "Epoch: 035, Loss: 0.1525, Train: 0.9646, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 036, Loss: 0.1376, Train: 0.9633, Val: 0.9933, Test: 0.9567\n",
      "Epoch: 037, Loss: 0.1356, Train: 0.9050, Val: 0.8900, Test: 0.8700\n",
      "Epoch: 038, Loss: 0.1347, Train: 0.9283, Val: 0.9433, Test: 0.9333\n",
      "Epoch: 039, Loss: 0.1154, Train: 0.9487, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 040, Loss: 0.1627, Train: 0.9463, Val: 0.9433, Test: 0.9533\n",
      "Epoch: 041, Loss: 0.1218, Train: 0.9496, Val: 0.9433, Test: 0.9467\n",
      "Epoch: 042, Loss: 0.1068, Train: 0.9742, Val: 0.9900, Test: 0.9767\n",
      "Epoch: 043, Loss: 0.1368, Train: 0.8717, Val: 0.8800, Test: 0.9133\n",
      "Epoch: 044, Loss: 0.1528, Train: 0.9496, Val: 0.9633, Test: 0.9567\n",
      "Epoch: 045, Loss: 0.1196, Train: 0.9183, Val: 0.8967, Test: 0.8800\n",
      "Epoch: 046, Loss: 0.1011, Train: 0.9513, Val: 0.9733, Test: 0.9567\n",
      "Epoch: 047, Loss: 0.1263, Train: 0.8042, Val: 0.8133, Test: 0.7600\n",
      "Epoch: 048, Loss: 0.1081, Train: 0.9587, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 049, Loss: 0.0971, Train: 0.9629, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 050, Loss: 0.1052, Train: 0.9700, Val: 0.9833, Test: 0.9700\n",
      "Epoch: 051, Loss: 0.1063, Train: 0.9383, Val: 0.9300, Test: 0.9200\n",
      "Epoch: 052, Loss: 0.1029, Train: 0.9229, Val: 0.9167, Test: 0.9467\n",
      "Epoch: 053, Loss: 0.1491, Train: 0.9571, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 054, Loss: 0.1348, Train: 0.9712, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 055, Loss: 0.1092, Train: 0.9792, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 056, Loss: 0.0923, Train: 0.9658, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 057, Loss: 0.0759, Train: 0.9750, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 058, Loss: 0.1080, Train: 0.9450, Val: 0.9400, Test: 0.9533\n",
      "Epoch: 059, Loss: 0.1029, Train: 0.9408, Val: 0.9467, Test: 0.9433\n",
      "Epoch: 060, Loss: 0.0899, Train: 0.9721, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 061, Loss: 0.0941, Train: 0.9683, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 062, Loss: 0.0923, Train: 0.9317, Val: 0.9333, Test: 0.9467\n",
      "Epoch: 063, Loss: 0.0878, Train: 0.9696, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 064, Loss: 0.0791, Train: 0.9471, Val: 0.9500, Test: 0.9667\n",
      "Epoch: 065, Loss: 0.1006, Train: 0.9825, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 066, Loss: 0.0885, Train: 0.9317, Val: 0.9467, Test: 0.9233\n",
      "Epoch: 067, Loss: 0.0827, Train: 0.9733, Val: 0.9900, Test: 0.9733\n",
      "Epoch: 068, Loss: 0.0810, Train: 0.9858, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 069, Loss: 0.0768, Train: 0.9542, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 070, Loss: 0.0724, Train: 0.9767, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 071, Loss: 0.0951, Train: 0.7433, Val: 0.7500, Test: 0.6667\n",
      "Epoch: 072, Loss: 0.1040, Train: 0.9437, Val: 0.9400, Test: 0.9533\n",
      "Epoch: 073, Loss: 0.0712, Train: 0.9567, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 074, Loss: 0.0769, Train: 0.9846, Val: 1.0000, Test: 0.9833\n",
      "Epoch: 075, Loss: 0.0842, Train: 0.9829, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 076, Loss: 0.0770, Train: 0.9854, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 077, Loss: 0.0634, Train: 0.9558, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 078, Loss: 0.0656, Train: 0.9625, Val: 0.9600, Test: 0.9500\n",
      "Epoch: 079, Loss: 0.0830, Train: 0.9792, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 080, Loss: 0.0827, Train: 0.9487, Val: 0.9533, Test: 0.9700\n",
      "Epoch: 081, Loss: 0.0720, Train: 0.9804, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 082, Loss: 0.0911, Train: 0.9833, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 083, Loss: 0.0771, Train: 0.9800, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 084, Loss: 0.0674, Train: 0.9833, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 085, Loss: 0.0895, Train: 0.9796, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 086, Loss: 0.0960, Train: 0.9492, Val: 0.9600, Test: 0.9633\n",
      "Epoch: 087, Loss: 0.1042, Train: 0.8950, Val: 0.8867, Test: 0.8767\n",
      "Epoch: 088, Loss: 0.0788, Train: 0.9817, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 089, Loss: 0.0806, Train: 0.9613, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 090, Loss: 0.0860, Train: 0.9642, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 091, Loss: 0.0799, Train: 0.9800, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 092, Loss: 0.0779, Train: 0.9788, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 093, Loss: 0.0745, Train: 0.9746, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 094, Loss: 0.0733, Train: 0.9842, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 095, Loss: 0.0664, Train: 0.9317, Val: 0.9233, Test: 0.9467\n",
      "Epoch: 096, Loss: 0.0679, Train: 0.9871, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 097, Loss: 0.0693, Train: 0.9583, Val: 0.9500, Test: 0.9667\n",
      "Epoch: 098, Loss: 0.0784, Train: 0.9821, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 099, Loss: 0.1048, Train: 0.9838, Val: 0.9867, Test: 0.9767\n",
      "Epoch: 100, Loss: 0.0838, Train: 0.9821, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 101, Loss: 0.0643, Train: 0.9817, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 102, Loss: 0.0833, Train: 0.9825, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 103, Loss: 0.0654, Train: 0.9779, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 104, Loss: 0.0697, Train: 0.9842, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 105, Loss: 0.0717, Train: 0.9646, Val: 0.9700, Test: 0.9567\n",
      "Epoch: 106, Loss: 0.0683, Train: 0.9546, Val: 0.9533, Test: 0.9467\n",
      "Epoch: 107, Loss: 0.0634, Train: 0.9804, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 108, Loss: 0.0703, Train: 0.9742, Val: 0.9633, Test: 0.9800\n",
      "Epoch: 109, Loss: 0.0749, Train: 0.9475, Val: 0.9400, Test: 0.9667\n",
      "Epoch: 110, Loss: 0.0611, Train: 0.9517, Val: 0.9467, Test: 0.9367\n",
      "Epoch: 111, Loss: 0.0539, Train: 0.9850, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 112, Loss: 0.0578, Train: 0.9696, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 113, Loss: 0.0689, Train: 0.9679, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 114, Loss: 0.0536, Train: 0.9800, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 115, Loss: 0.0869, Train: 0.9725, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 116, Loss: 0.0730, Train: 0.9804, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 117, Loss: 0.0534, Train: 0.9767, Val: 0.9800, Test: 0.9667\n",
      "Epoch: 118, Loss: 0.0744, Train: 0.9846, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 119, Loss: 0.0591, Train: 0.9821, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 120, Loss: 0.0576, Train: 0.9875, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 121, Loss: 0.0901, Train: 0.9817, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 122, Loss: 0.0738, Train: 0.9900, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 123, Loss: 0.0672, Train: 0.9854, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 124, Loss: 0.0614, Train: 0.9879, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 125, Loss: 0.0555, Train: 0.9729, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 126, Loss: 0.0618, Train: 0.9650, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 127, Loss: 0.0541, Train: 0.9862, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 128, Loss: 0.0606, Train: 0.9796, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 129, Loss: 0.0708, Train: 0.9450, Val: 0.9500, Test: 0.9600\n",
      "Epoch: 130, Loss: 0.0567, Train: 0.9900, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 131, Loss: 0.0547, Train: 0.9892, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 132, Loss: 0.0554, Train: 0.9679, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 133, Loss: 0.0552, Train: 0.9733, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 134, Loss: 0.0549, Train: 0.9829, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 135, Loss: 0.0630, Train: 0.9804, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 136, Loss: 0.0530, Train: 0.9908, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 137, Loss: 0.0503, Train: 0.9896, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 138, Loss: 0.0555, Train: 0.9871, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 139, Loss: 0.0572, Train: 0.9879, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 140, Loss: 0.0506, Train: 0.9892, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 141, Loss: 0.0490, Train: 0.9879, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 142, Loss: 0.0670, Train: 0.9912, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 143, Loss: 0.0566, Train: 0.9854, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 144, Loss: 0.0510, Train: 0.9850, Val: 0.9900, Test: 0.9767\n",
      "Epoch: 145, Loss: 0.0677, Train: 0.9817, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 146, Loss: 0.0612, Train: 0.9721, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 147, Loss: 0.0472, Train: 0.9796, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 148, Loss: 0.0581, Train: 0.9825, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 149, Loss: 0.0490, Train: 0.9908, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 150, Loss: 0.0491, Train: 0.9883, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 151, Loss: 0.0537, Train: 0.9817, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 152, Loss: 0.0495, Train: 0.9875, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 153, Loss: 0.0664, Train: 0.9750, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 154, Loss: 0.0568, Train: 0.9896, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 155, Loss: 0.0526, Train: 0.9900, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 156, Loss: 0.0412, Train: 0.9896, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 157, Loss: 0.0485, Train: 0.9875, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 158, Loss: 0.0460, Train: 0.9900, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 159, Loss: 0.0428, Train: 0.9892, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 160, Loss: 0.0456, Train: 0.9775, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 161, Loss: 0.0593, Train: 0.9917, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 162, Loss: 0.0671, Train: 0.9883, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 163, Loss: 0.0462, Train: 0.9879, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 164, Loss: 0.0566, Train: 0.9692, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 165, Loss: 0.0432, Train: 0.9375, Val: 0.9367, Test: 0.9267\n",
      "Epoch: 166, Loss: 0.0508, Train: 0.9912, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 167, Loss: 0.0402, Train: 0.9917, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 168, Loss: 0.0425, Train: 0.9804, Val: 0.9900, Test: 0.9733\n",
      "Epoch: 169, Loss: 0.0569, Train: 0.9550, Val: 0.9567, Test: 0.9433\n",
      "Epoch: 170, Loss: 0.0481, Train: 0.9888, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 171, Loss: 0.0731, Train: 0.9812, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 172, Loss: 0.0440, Train: 0.9888, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 173, Loss: 0.0395, Train: 0.9871, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 174, Loss: 0.0491, Train: 0.9896, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 175, Loss: 0.0569, Train: 0.9671, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 176, Loss: 0.0433, Train: 0.9817, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 177, Loss: 0.0454, Train: 0.9867, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 178, Loss: 0.0449, Train: 0.9904, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 179, Loss: 0.0446, Train: 0.9900, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 180, Loss: 0.0425, Train: 0.9850, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 181, Loss: 0.0510, Train: 0.9938, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 182, Loss: 0.0500, Train: 0.9838, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 183, Loss: 0.0522, Train: 0.9888, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 184, Loss: 0.0514, Train: 0.9900, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 185, Loss: 0.0497, Train: 0.9908, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 186, Loss: 0.0546, Train: 0.9733, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 187, Loss: 0.0441, Train: 0.9912, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 188, Loss: 0.0444, Train: 0.9925, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 189, Loss: 0.0448, Train: 0.9771, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 190, Loss: 0.0409, Train: 0.9817, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 191, Loss: 0.0539, Train: 0.9879, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 192, Loss: 0.0404, Train: 0.9908, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 193, Loss: 0.0503, Train: 0.9933, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 194, Loss: 0.0401, Train: 0.9917, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 195, Loss: 0.0408, Train: 0.9908, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 196, Loss: 0.0389, Train: 0.9812, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 197, Loss: 0.0450, Train: 0.9879, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 198, Loss: 0.0386, Train: 0.9867, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 199, Loss: 0.0439, Train: 0.9871, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 200, Loss: 0.0441, Train: 0.9896, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 201, Loss: 0.0435, Train: 0.9821, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 202, Loss: 0.0444, Train: 0.9900, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 203, Loss: 0.0349, Train: 0.9792, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 204, Loss: 0.0627, Train: 0.9908, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 205, Loss: 0.0447, Train: 0.9904, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 206, Loss: 0.0705, Train: 0.8896, Val: 0.8867, Test: 0.8733\n",
      "Epoch: 207, Loss: 0.0505, Train: 0.9879, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 208, Loss: 0.0398, Train: 0.9938, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 209, Loss: 0.0492, Train: 0.9933, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 210, Loss: 0.0368, Train: 0.9908, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 211, Loss: 0.0300, Train: 0.9925, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 212, Loss: 0.0362, Train: 0.9904, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 213, Loss: 0.0463, Train: 0.9925, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 214, Loss: 0.0442, Train: 0.9862, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 215, Loss: 0.0393, Train: 0.9871, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 216, Loss: 0.0371, Train: 0.9825, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 217, Loss: 0.0351, Train: 0.9900, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 218, Loss: 0.0409, Train: 0.9850, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 219, Loss: 0.0597, Train: 0.9738, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 220, Loss: 0.0461, Train: 0.9925, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 221, Loss: 0.0400, Train: 0.9871, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 222, Loss: 0.0477, Train: 0.9929, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 223, Loss: 0.0402, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 224, Loss: 0.0470, Train: 0.9892, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 225, Loss: 0.0365, Train: 0.9933, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 226, Loss: 0.0518, Train: 0.9921, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 227, Loss: 0.0404, Train: 0.9929, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 228, Loss: 0.0519, Train: 0.9933, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 229, Loss: 0.0389, Train: 0.9838, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 230, Loss: 0.0339, Train: 0.9900, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 231, Loss: 0.0387, Train: 0.9917, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 232, Loss: 0.0405, Train: 0.9862, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 233, Loss: 0.0371, Train: 0.9929, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 234, Loss: 0.0357, Train: 0.9825, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 235, Loss: 0.0328, Train: 0.9883, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 236, Loss: 0.0475, Train: 0.9867, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 237, Loss: 0.0465, Train: 0.9929, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 238, Loss: 0.0340, Train: 0.9933, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 239, Loss: 0.0402, Train: 0.9900, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 240, Loss: 0.0358, Train: 0.9867, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 241, Loss: 0.0407, Train: 0.9912, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 242, Loss: 0.0408, Train: 0.9904, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 243, Loss: 0.0409, Train: 0.9912, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 244, Loss: 0.0374, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 245, Loss: 0.0358, Train: 0.9904, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 246, Loss: 0.0376, Train: 0.9875, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 247, Loss: 0.0453, Train: 0.9742, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 248, Loss: 0.0415, Train: 0.9933, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 249, Loss: 0.0358, Train: 0.9833, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 250, Loss: 0.0444, Train: 0.9896, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 251, Loss: 0.0410, Train: 0.9833, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 252, Loss: 0.0423, Train: 0.9917, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 253, Loss: 0.0359, Train: 0.9846, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 254, Loss: 0.0367, Train: 0.9912, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 255, Loss: 0.0310, Train: 0.9908, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 256, Loss: 0.0373, Train: 0.9904, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 257, Loss: 0.0509, Train: 0.9825, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 258, Loss: 0.0450, Train: 0.9929, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 259, Loss: 0.0356, Train: 0.9896, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 260, Loss: 0.0466, Train: 0.9854, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 261, Loss: 0.0455, Train: 0.9875, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 262, Loss: 0.0300, Train: 0.9933, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 263, Loss: 0.0418, Train: 0.9908, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 264, Loss: 0.0341, Train: 0.9942, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 265, Loss: 0.0324, Train: 0.9925, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 266, Loss: 0.0372, Train: 0.9921, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 267, Loss: 0.0331, Train: 0.9871, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 268, Loss: 0.0412, Train: 0.9879, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 269, Loss: 0.0487, Train: 0.9892, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 270, Loss: 0.0310, Train: 0.9933, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 271, Loss: 0.0284, Train: 0.9929, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 272, Loss: 0.0300, Train: 0.9904, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 273, Loss: 0.0302, Train: 0.9917, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 274, Loss: 0.0448, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 275, Loss: 0.0321, Train: 0.9858, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 276, Loss: 0.0410, Train: 0.9883, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 277, Loss: 0.0440, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 278, Loss: 0.0334, Train: 0.9925, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 279, Loss: 0.0392, Train: 0.9858, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 280, Loss: 0.0414, Train: 0.9942, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 281, Loss: 0.0331, Train: 0.9904, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 282, Loss: 0.0376, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 283, Loss: 0.0317, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 284, Loss: 0.0299, Train: 0.9929, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 285, Loss: 0.0260, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 286, Loss: 0.0350, Train: 0.9825, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 287, Loss: 0.0389, Train: 0.9912, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 288, Loss: 0.0431, Train: 0.9933, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 289, Loss: 0.0300, Train: 0.9942, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 290, Loss: 0.0448, Train: 0.9725, Val: 0.9667, Test: 0.9767\n",
      "Epoch: 291, Loss: 0.0326, Train: 0.9938, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 292, Loss: 0.0275, Train: 0.9867, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 293, Loss: 0.0316, Train: 0.9912, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 294, Loss: 0.0297, Train: 0.9921, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 295, Loss: 0.0337, Train: 0.9946, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 296, Loss: 0.0319, Train: 0.9812, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 297, Loss: 0.0395, Train: 0.9925, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 298, Loss: 0.0250, Train: 0.9942, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 299, Loss: 0.0254, Train: 0.9950, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 300, Loss: 0.0390, Train: 0.9904, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 301, Loss: 0.0514, Train: 0.9950, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 302, Loss: 0.0279, Train: 0.9946, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 303, Loss: 0.0331, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 304, Loss: 0.0358, Train: 0.9908, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 305, Loss: 0.0363, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 306, Loss: 0.0321, Train: 0.9883, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 307, Loss: 0.0341, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 308, Loss: 0.0304, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 309, Loss: 0.0367, Train: 0.9933, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 310, Loss: 0.0300, Train: 0.9904, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 311, Loss: 0.0290, Train: 0.9933, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 312, Loss: 0.0282, Train: 0.9917, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 313, Loss: 0.0316, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 314, Loss: 0.0273, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 315, Loss: 0.0264, Train: 0.9938, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 316, Loss: 0.0337, Train: 0.9925, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 317, Loss: 0.0353, Train: 0.9950, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 318, Loss: 0.0237, Train: 0.9946, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 319, Loss: 0.0387, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 320, Loss: 0.0309, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 321, Loss: 0.0232, Train: 0.9938, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 322, Loss: 0.0311, Train: 0.9942, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 323, Loss: 0.0316, Train: 0.9858, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 324, Loss: 0.0260, Train: 0.9929, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 325, Loss: 0.0332, Train: 0.9788, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 326, Loss: 0.0353, Train: 0.9933, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 327, Loss: 0.0256, Train: 0.9912, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 328, Loss: 0.0258, Train: 0.9929, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 329, Loss: 0.0340, Train: 0.9821, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 330, Loss: 0.0385, Train: 0.9950, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 331, Loss: 0.0345, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 332, Loss: 0.0306, Train: 0.9888, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 333, Loss: 0.0355, Train: 0.9925, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 334, Loss: 0.0251, Train: 0.9942, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 335, Loss: 0.0267, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 336, Loss: 0.0261, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 337, Loss: 0.0246, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 338, Loss: 0.0358, Train: 0.9938, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 339, Loss: 0.0320, Train: 0.9929, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 340, Loss: 0.0241, Train: 0.9946, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 341, Loss: 0.0285, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 342, Loss: 0.0242, Train: 0.9908, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 343, Loss: 0.0331, Train: 0.9958, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 344, Loss: 0.0295, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 345, Loss: 0.0255, Train: 0.9954, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 346, Loss: 0.0255, Train: 0.9929, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 347, Loss: 0.0264, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 348, Loss: 0.0500, Train: 0.9950, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 349, Loss: 0.0284, Train: 0.9933, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 350, Loss: 0.0322, Train: 0.9933, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 351, Loss: 0.0320, Train: 0.9946, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 352, Loss: 0.0403, Train: 0.9942, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 353, Loss: 0.0240, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 354, Loss: 0.0209, Train: 0.9942, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 355, Loss: 0.0409, Train: 0.9942, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 356, Loss: 0.0375, Train: 0.9950, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 357, Loss: 0.0301, Train: 0.9912, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 358, Loss: 0.0226, Train: 0.9942, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 359, Loss: 0.0270, Train: 0.9954, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 360, Loss: 0.0253, Train: 0.9942, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 361, Loss: 0.0441, Train: 0.9929, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 362, Loss: 0.0335, Train: 0.9950, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 363, Loss: 0.0348, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 364, Loss: 0.0230, Train: 0.9929, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 365, Loss: 0.0205, Train: 0.9912, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 366, Loss: 0.0301, Train: 0.9933, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 367, Loss: 0.0269, Train: 0.9942, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 368, Loss: 0.0277, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 369, Loss: 0.0318, Train: 0.9921, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 370, Loss: 0.0290, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 371, Loss: 0.0226, Train: 0.9950, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 372, Loss: 0.0317, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 373, Loss: 0.0264, Train: 0.9942, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 374, Loss: 0.0349, Train: 0.9904, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 375, Loss: 0.0313, Train: 0.9954, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 376, Loss: 0.0263, Train: 0.9838, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 377, Loss: 0.0227, Train: 0.9954, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 378, Loss: 0.0195, Train: 0.9950, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 379, Loss: 0.0232, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 380, Loss: 0.0258, Train: 0.9942, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 381, Loss: 0.0290, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 382, Loss: 0.0254, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 383, Loss: 0.0292, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 384, Loss: 0.0226, Train: 0.9958, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 385, Loss: 0.0302, Train: 0.9933, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 386, Loss: 0.0181, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 387, Loss: 0.0273, Train: 0.9946, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 388, Loss: 0.0256, Train: 0.9925, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 389, Loss: 0.0306, Train: 0.9950, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 390, Loss: 0.0303, Train: 0.9792, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 391, Loss: 0.0336, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 392, Loss: 0.0247, Train: 0.9946, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 393, Loss: 0.0250, Train: 0.9950, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 394, Loss: 0.0232, Train: 0.9942, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 395, Loss: 0.0273, Train: 0.9908, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 396, Loss: 0.0247, Train: 0.9946, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 397, Loss: 0.0344, Train: 0.9942, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 398, Loss: 0.0261, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 399, Loss: 0.0199, Train: 0.9954, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 400, Loss: 0.0329, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 401, Loss: 0.0231, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 402, Loss: 0.0259, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 403, Loss: 0.0344, Train: 0.9808, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 404, Loss: 0.0299, Train: 0.9942, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 405, Loss: 0.0289, Train: 0.9946, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 406, Loss: 0.0186, Train: 0.9938, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 407, Loss: 0.0230, Train: 0.9938, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 408, Loss: 0.0250, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 409, Loss: 0.0296, Train: 0.9917, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 410, Loss: 0.0345, Train: 0.9892, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 411, Loss: 0.0254, Train: 0.9942, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 412, Loss: 0.0227, Train: 0.9954, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 413, Loss: 0.0271, Train: 0.9954, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 414, Loss: 0.0311, Train: 0.9946, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 415, Loss: 0.0303, Train: 0.9950, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 416, Loss: 0.0201, Train: 0.9958, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 417, Loss: 0.0232, Train: 0.9962, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 418, Loss: 0.0246, Train: 0.9929, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 419, Loss: 0.0206, Train: 0.9933, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 420, Loss: 0.0354, Train: 0.9833, Val: 0.9867, Test: 0.9767\n",
      "Epoch: 421, Loss: 0.0233, Train: 0.9950, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 422, Loss: 0.0210, Train: 0.9875, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 423, Loss: 0.0252, Train: 0.9892, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 424, Loss: 0.0220, Train: 0.9942, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 425, Loss: 0.0207, Train: 0.9967, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 426, Loss: 0.0309, Train: 0.9933, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 427, Loss: 0.0204, Train: 0.9938, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 428, Loss: 0.0213, Train: 0.9954, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 429, Loss: 0.0205, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 430, Loss: 0.0343, Train: 0.9821, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 431, Loss: 0.0500, Train: 0.9950, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 432, Loss: 0.0195, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 433, Loss: 0.0239, Train: 0.9929, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 434, Loss: 0.0232, Train: 0.9942, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 435, Loss: 0.0236, Train: 0.9925, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 436, Loss: 0.0205, Train: 0.9954, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 437, Loss: 0.0266, Train: 0.9929, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 438, Loss: 0.0208, Train: 0.9942, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 439, Loss: 0.0247, Train: 0.9908, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 440, Loss: 0.0189, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 441, Loss: 0.0201, Train: 0.9950, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 442, Loss: 0.0199, Train: 0.9967, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 443, Loss: 0.0184, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 444, Loss: 0.0250, Train: 0.9942, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 445, Loss: 0.0397, Train: 0.9858, Val: 0.9767, Test: 0.9900\n",
      "Epoch: 446, Loss: 0.0282, Train: 0.9946, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 447, Loss: 0.0230, Train: 0.9962, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 448, Loss: 0.0201, Train: 0.9946, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 449, Loss: 0.0353, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 450, Loss: 0.0238, Train: 0.9942, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 451, Loss: 0.0214, Train: 0.9962, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 452, Loss: 0.0245, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 453, Loss: 0.0207, Train: 0.9958, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 454, Loss: 0.0233, Train: 0.9938, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 455, Loss: 0.0197, Train: 0.9954, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 456, Loss: 0.0167, Train: 0.9954, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 457, Loss: 0.0297, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 458, Loss: 0.0210, Train: 0.9958, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 459, Loss: 0.0223, Train: 0.9946, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 460, Loss: 0.0189, Train: 0.9962, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 461, Loss: 0.0294, Train: 0.9962, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 462, Loss: 0.0231, Train: 0.9962, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 463, Loss: 0.0231, Train: 0.9950, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 464, Loss: 0.0207, Train: 0.9954, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 465, Loss: 0.0260, Train: 0.9962, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 466, Loss: 0.0188, Train: 0.9946, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 467, Loss: 0.0267, Train: 0.9908, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 468, Loss: 0.0235, Train: 0.9958, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 469, Loss: 0.0247, Train: 0.9950, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 470, Loss: 0.0233, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 471, Loss: 0.0226, Train: 0.9958, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 472, Loss: 0.0192, Train: 0.9954, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 473, Loss: 0.0283, Train: 0.9950, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 474, Loss: 0.0206, Train: 0.9962, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 475, Loss: 0.0184, Train: 0.9962, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 476, Loss: 0.0203, Train: 0.9942, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 477, Loss: 0.0178, Train: 0.9950, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 478, Loss: 0.0205, Train: 0.9958, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 479, Loss: 0.0211, Train: 0.9946, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 480, Loss: 0.0170, Train: 0.9833, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 481, Loss: 0.0291, Train: 0.9962, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 482, Loss: 0.0218, Train: 0.9954, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 483, Loss: 0.0188, Train: 0.9967, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 484, Loss: 0.0171, Train: 0.9962, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 485, Loss: 0.0238, Train: 0.9938, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 486, Loss: 0.0192, Train: 0.9967, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 487, Loss: 0.0229, Train: 0.9938, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 488, Loss: 0.0175, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 489, Loss: 0.0221, Train: 0.9958, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 490, Loss: 0.0246, Train: 0.9938, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 491, Loss: 0.0178, Train: 0.9958, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 492, Loss: 0.0251, Train: 0.9954, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 493, Loss: 0.0199, Train: 0.9962, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 494, Loss: 0.0205, Train: 0.9958, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 495, Loss: 0.0178, Train: 0.9967, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 496, Loss: 0.0205, Train: 0.9967, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 497, Loss: 0.0264, Train: 0.9938, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 498, Loss: 0.0212, Train: 0.9967, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 499, Loss: 0.0218, Train: 0.9958, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 500, Loss: 0.0199, Train: 0.9958, Val: 0.9967, Test: 0.9900\n",
      "### Run 0 - val loss: 0.003, test acc: 0.997\n",
      "Accuracies in each run:  [0.9966666666666667]\n",
      "test acc - mean: 0.997, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = None\n",
    "for r in range(args.runs):  \n",
    "    print(args.pooling)\n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffpool\n",
      "Epoch: 001, Loss: 1.4717, Train: 0.5188, Val: 0.5467, Test: 0.5167\n",
      "Epoch: 002, Loss: 0.9326, Train: 0.7029, Val: 0.7467, Test: 0.6667\n",
      "Epoch: 003, Loss: 0.8324, Train: 0.6967, Val: 0.7200, Test: 0.6933\n",
      "Epoch: 004, Loss: 0.7807, Train: 0.6742, Val: 0.6967, Test: 0.6733\n",
      "Epoch: 005, Loss: 0.7794, Train: 0.7571, Val: 0.7500, Test: 0.7167\n",
      "Epoch: 006, Loss: 0.7395, Train: 0.5296, Val: 0.5533, Test: 0.5300\n",
      "Epoch: 007, Loss: 0.7032, Train: 0.7733, Val: 0.7800, Test: 0.7667\n",
      "Epoch: 008, Loss: 0.6860, Train: 0.7750, Val: 0.7933, Test: 0.7733\n",
      "Epoch: 009, Loss: 0.6342, Train: 0.6854, Val: 0.6667, Test: 0.6933\n",
      "Epoch: 010, Loss: 0.6242, Train: 0.7992, Val: 0.8300, Test: 0.8133\n",
      "Epoch: 011, Loss: 0.6259, Train: 0.5383, Val: 0.5667, Test: 0.5500\n",
      "Epoch: 012, Loss: 0.5869, Train: 0.8246, Val: 0.8167, Test: 0.8533\n",
      "Epoch: 013, Loss: 0.5920, Train: 0.7096, Val: 0.7200, Test: 0.7133\n",
      "Epoch: 014, Loss: 0.5574, Train: 0.8021, Val: 0.7933, Test: 0.7833\n",
      "Epoch: 015, Loss: 0.5579, Train: 0.7808, Val: 0.7500, Test: 0.7767\n",
      "Epoch: 016, Loss: 0.5489, Train: 0.8550, Val: 0.8433, Test: 0.8600\n",
      "Epoch: 017, Loss: 0.5830, Train: 0.7692, Val: 0.7933, Test: 0.7933\n",
      "Epoch: 018, Loss: 0.5503, Train: 0.6733, Val: 0.6733, Test: 0.6667\n",
      "Epoch: 019, Loss: 0.5445, Train: 0.8529, Val: 0.8467, Test: 0.8567\n",
      "Epoch: 020, Loss: 0.5572, Train: 0.8375, Val: 0.8367, Test: 0.8333\n",
      "Epoch: 021, Loss: 0.5377, Train: 0.7517, Val: 0.7633, Test: 0.7833\n",
      "Epoch: 022, Loss: 0.5437, Train: 0.8550, Val: 0.8600, Test: 0.8700\n",
      "Epoch: 023, Loss: 0.5204, Train: 0.8250, Val: 0.8167, Test: 0.8633\n",
      "Epoch: 024, Loss: 0.5232, Train: 0.8496, Val: 0.8267, Test: 0.8267\n",
      "Epoch: 025, Loss: 0.5411, Train: 0.8492, Val: 0.8333, Test: 0.8833\n",
      "Epoch: 026, Loss: 0.4896, Train: 0.7708, Val: 0.7933, Test: 0.7900\n",
      "Epoch: 027, Loss: 0.5041, Train: 0.8596, Val: 0.8667, Test: 0.8867\n",
      "Epoch: 028, Loss: 0.4943, Train: 0.7779, Val: 0.7900, Test: 0.8233\n",
      "Epoch: 029, Loss: 0.4745, Train: 0.8883, Val: 0.8767, Test: 0.8933\n",
      "Epoch: 030, Loss: 0.4735, Train: 0.9008, Val: 0.8733, Test: 0.9033\n",
      "Epoch: 031, Loss: 0.4764, Train: 0.8912, Val: 0.8767, Test: 0.8967\n",
      "Epoch: 032, Loss: 0.4647, Train: 0.8363, Val: 0.8367, Test: 0.8800\n",
      "Epoch: 033, Loss: 0.5233, Train: 0.8013, Val: 0.8167, Test: 0.8500\n",
      "Epoch: 034, Loss: 0.4715, Train: 0.8925, Val: 0.8767, Test: 0.8967\n",
      "Epoch: 035, Loss: 0.4863, Train: 0.7654, Val: 0.7800, Test: 0.8033\n",
      "Epoch: 036, Loss: 0.4605, Train: 0.8175, Val: 0.8433, Test: 0.8433\n",
      "Epoch: 037, Loss: 0.4785, Train: 0.8875, Val: 0.8667, Test: 0.8933\n",
      "Epoch: 038, Loss: 0.4734, Train: 0.8983, Val: 0.8900, Test: 0.9100\n",
      "Epoch: 039, Loss: 0.4916, Train: 0.8237, Val: 0.8067, Test: 0.8100\n",
      "Epoch: 040, Loss: 0.4722, Train: 0.7550, Val: 0.7533, Test: 0.7900\n",
      "Epoch: 041, Loss: 0.4808, Train: 0.8812, Val: 0.8633, Test: 0.8767\n",
      "Epoch: 042, Loss: 0.4553, Train: 0.8358, Val: 0.8067, Test: 0.8000\n",
      "Epoch: 043, Loss: 0.4766, Train: 0.8729, Val: 0.8600, Test: 0.8667\n",
      "Epoch: 044, Loss: 0.4544, Train: 0.7275, Val: 0.7500, Test: 0.7333\n",
      "Epoch: 045, Loss: 0.4704, Train: 0.9062, Val: 0.8933, Test: 0.8967\n",
      "Epoch: 046, Loss: 0.4392, Train: 0.7163, Val: 0.7033, Test: 0.7267\n",
      "Epoch: 047, Loss: 0.4355, Train: 0.6075, Val: 0.6133, Test: 0.6200\n",
      "Epoch: 048, Loss: 0.4512, Train: 0.7646, Val: 0.7700, Test: 0.8000\n",
      "Epoch: 049, Loss: 0.4675, Train: 0.8204, Val: 0.7833, Test: 0.7967\n",
      "Epoch: 050, Loss: 0.4235, Train: 0.8496, Val: 0.8533, Test: 0.8800\n",
      "Epoch: 051, Loss: 0.4273, Train: 0.8854, Val: 0.8767, Test: 0.8733\n",
      "Epoch: 052, Loss: 0.4415, Train: 0.8263, Val: 0.7833, Test: 0.8200\n",
      "Epoch: 053, Loss: 0.4513, Train: 0.8579, Val: 0.8533, Test: 0.8700\n",
      "Epoch: 054, Loss: 0.4249, Train: 0.8983, Val: 0.8933, Test: 0.8900\n",
      "Epoch: 055, Loss: 0.4452, Train: 0.9075, Val: 0.9000, Test: 0.8900\n",
      "Epoch: 056, Loss: 0.4265, Train: 0.8429, Val: 0.8133, Test: 0.8133\n",
      "Epoch: 057, Loss: 0.4378, Train: 0.8996, Val: 0.8867, Test: 0.9000\n",
      "Epoch: 058, Loss: 0.4471, Train: 0.8279, Val: 0.8567, Test: 0.8700\n",
      "Epoch: 059, Loss: 0.4506, Train: 0.9092, Val: 0.9033, Test: 0.9200\n",
      "Epoch: 060, Loss: 0.4092, Train: 0.8879, Val: 0.8833, Test: 0.8800\n",
      "Epoch: 061, Loss: 0.4196, Train: 0.6117, Val: 0.6200, Test: 0.6300\n",
      "Epoch: 062, Loss: 0.4464, Train: 0.8575, Val: 0.8467, Test: 0.8600\n",
      "Epoch: 063, Loss: 0.4235, Train: 0.8454, Val: 0.8500, Test: 0.8733\n",
      "Epoch: 064, Loss: 0.4215, Train: 0.8717, Val: 0.8467, Test: 0.8700\n",
      "Epoch: 065, Loss: 0.4096, Train: 0.8842, Val: 0.8767, Test: 0.8700\n",
      "Epoch: 066, Loss: 0.4227, Train: 0.8825, Val: 0.8700, Test: 0.8700\n",
      "Epoch: 067, Loss: 0.4189, Train: 0.9058, Val: 0.8967, Test: 0.9033\n",
      "Epoch: 068, Loss: 0.4163, Train: 0.8425, Val: 0.8567, Test: 0.8733\n",
      "Epoch: 069, Loss: 0.4138, Train: 0.9087, Val: 0.9000, Test: 0.9100\n",
      "Epoch: 070, Loss: 0.4219, Train: 0.7821, Val: 0.8233, Test: 0.7833\n",
      "Epoch: 071, Loss: 0.4096, Train: 0.8871, Val: 0.8833, Test: 0.8867\n",
      "Epoch: 072, Loss: 0.4317, Train: 0.9087, Val: 0.8933, Test: 0.9333\n",
      "Epoch: 073, Loss: 0.4103, Train: 0.8921, Val: 0.8767, Test: 0.8800\n",
      "Epoch: 074, Loss: 0.4045, Train: 0.8442, Val: 0.8600, Test: 0.8767\n",
      "Epoch: 075, Loss: 0.4054, Train: 0.8850, Val: 0.8767, Test: 0.9067\n",
      "Epoch: 076, Loss: 0.4377, Train: 0.9017, Val: 0.9000, Test: 0.9067\n",
      "Epoch: 077, Loss: 0.4191, Train: 0.8533, Val: 0.8567, Test: 0.8800\n",
      "Epoch: 078, Loss: 0.4065, Train: 0.8962, Val: 0.8867, Test: 0.8933\n",
      "Epoch: 079, Loss: 0.4104, Train: 0.8867, Val: 0.8833, Test: 0.8700\n",
      "Epoch: 080, Loss: 0.4181, Train: 0.9021, Val: 0.8967, Test: 0.9267\n",
      "Epoch: 081, Loss: 0.4149, Train: 0.9129, Val: 0.8967, Test: 0.9267\n",
      "Epoch: 082, Loss: 0.3909, Train: 0.9113, Val: 0.9033, Test: 0.9100\n",
      "Epoch: 083, Loss: 0.4068, Train: 0.8917, Val: 0.8700, Test: 0.9133\n",
      "Epoch: 084, Loss: 0.3910, Train: 0.9171, Val: 0.9100, Test: 0.9033\n",
      "Epoch: 085, Loss: 0.4173, Train: 0.8817, Val: 0.8733, Test: 0.8767\n",
      "Epoch: 086, Loss: 0.4028, Train: 0.9142, Val: 0.8967, Test: 0.9233\n",
      "Epoch: 087, Loss: 0.4086, Train: 0.8179, Val: 0.8167, Test: 0.8367\n",
      "Epoch: 088, Loss: 0.4014, Train: 0.8129, Val: 0.8567, Test: 0.8433\n",
      "Epoch: 089, Loss: 0.4130, Train: 0.9054, Val: 0.9000, Test: 0.8933\n",
      "Epoch: 090, Loss: 0.3872, Train: 0.9129, Val: 0.8967, Test: 0.9300\n",
      "Epoch: 091, Loss: 0.4075, Train: 0.9187, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 092, Loss: 0.3987, Train: 0.7750, Val: 0.7300, Test: 0.7500\n",
      "Epoch: 093, Loss: 0.4059, Train: 0.9092, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 094, Loss: 0.4111, Train: 0.9087, Val: 0.8900, Test: 0.9067\n",
      "Epoch: 095, Loss: 0.3761, Train: 0.9175, Val: 0.9167, Test: 0.9000\n",
      "Epoch: 096, Loss: 0.3880, Train: 0.8908, Val: 0.8967, Test: 0.8867\n",
      "Epoch: 097, Loss: 0.3794, Train: 0.8638, Val: 0.8467, Test: 0.8567\n",
      "Epoch: 098, Loss: 0.3834, Train: 0.8917, Val: 0.8867, Test: 0.9033\n",
      "Epoch: 099, Loss: 0.4002, Train: 0.9108, Val: 0.9167, Test: 0.9167\n",
      "Epoch: 100, Loss: 0.3749, Train: 0.9121, Val: 0.9033, Test: 0.9100\n",
      "Epoch: 101, Loss: 0.3712, Train: 0.9096, Val: 0.8967, Test: 0.9167\n",
      "Epoch: 102, Loss: 0.3823, Train: 0.9225, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 103, Loss: 0.3759, Train: 0.7850, Val: 0.7800, Test: 0.8100\n",
      "Epoch: 104, Loss: 0.3750, Train: 0.9187, Val: 0.9067, Test: 0.9200\n",
      "Epoch: 105, Loss: 0.4000, Train: 0.9000, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 106, Loss: 0.3945, Train: 0.8862, Val: 0.8700, Test: 0.9200\n",
      "Epoch: 107, Loss: 0.3752, Train: 0.9083, Val: 0.8967, Test: 0.8967\n",
      "Epoch: 108, Loss: 0.3923, Train: 0.9033, Val: 0.9000, Test: 0.9033\n",
      "Epoch: 109, Loss: 0.3780, Train: 0.8371, Val: 0.8600, Test: 0.8633\n",
      "Epoch: 110, Loss: 0.3976, Train: 0.8879, Val: 0.8833, Test: 0.8900\n",
      "Epoch: 111, Loss: 0.3767, Train: 0.9225, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 112, Loss: 0.4074, Train: 0.8708, Val: 0.8567, Test: 0.8667\n",
      "Epoch: 113, Loss: 0.3855, Train: 0.8838, Val: 0.8800, Test: 0.8800\n",
      "Epoch: 114, Loss: 0.3791, Train: 0.9200, Val: 0.9200, Test: 0.9167\n",
      "Epoch: 115, Loss: 0.3912, Train: 0.8992, Val: 0.8867, Test: 0.9100\n",
      "Epoch: 116, Loss: 0.3645, Train: 0.7550, Val: 0.7400, Test: 0.7467\n",
      "Epoch: 117, Loss: 0.3729, Train: 0.8492, Val: 0.8467, Test: 0.8400\n",
      "Epoch: 118, Loss: 0.3641, Train: 0.9062, Val: 0.8967, Test: 0.8967\n",
      "Epoch: 119, Loss: 0.3593, Train: 0.9029, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 120, Loss: 0.3631, Train: 0.9025, Val: 0.8767, Test: 0.9267\n",
      "Epoch: 121, Loss: 0.3627, Train: 0.9062, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 122, Loss: 0.3720, Train: 0.9158, Val: 0.8933, Test: 0.9167\n",
      "Epoch: 123, Loss: 0.3797, Train: 0.8996, Val: 0.8833, Test: 0.9167\n",
      "Epoch: 124, Loss: 0.3464, Train: 0.9154, Val: 0.9100, Test: 0.9100\n",
      "Epoch: 125, Loss: 0.3583, Train: 0.9125, Val: 0.9100, Test: 0.9067\n",
      "Epoch: 126, Loss: 0.3433, Train: 0.9075, Val: 0.8933, Test: 0.9300\n",
      "Epoch: 127, Loss: 0.3618, Train: 0.9229, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 128, Loss: 0.3634, Train: 0.8929, Val: 0.8700, Test: 0.8800\n",
      "Epoch: 129, Loss: 0.3621, Train: 0.9204, Val: 0.9000, Test: 0.9167\n",
      "Epoch: 130, Loss: 0.3650, Train: 0.9113, Val: 0.8900, Test: 0.9300\n",
      "Epoch: 131, Loss: 0.3545, Train: 0.8962, Val: 0.8933, Test: 0.8967\n",
      "Epoch: 132, Loss: 0.4042, Train: 0.9137, Val: 0.9100, Test: 0.9000\n",
      "Epoch: 133, Loss: 0.3645, Train: 0.9158, Val: 0.8900, Test: 0.9233\n",
      "Epoch: 134, Loss: 0.3591, Train: 0.8321, Val: 0.8133, Test: 0.8167\n",
      "Epoch: 135, Loss: 0.3656, Train: 0.9133, Val: 0.9033, Test: 0.9033\n",
      "Epoch: 136, Loss: 0.3700, Train: 0.9171, Val: 0.9133, Test: 0.9233\n",
      "Epoch: 137, Loss: 0.3485, Train: 0.9167, Val: 0.9067, Test: 0.9300\n",
      "Epoch: 138, Loss: 0.3516, Train: 0.9229, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 139, Loss: 0.3575, Train: 0.9183, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 140, Loss: 0.3459, Train: 0.9242, Val: 0.9133, Test: 0.9300\n",
      "Epoch: 141, Loss: 0.3504, Train: 0.9083, Val: 0.8967, Test: 0.9033\n",
      "Epoch: 142, Loss: 0.3618, Train: 0.8438, Val: 0.8300, Test: 0.8667\n",
      "Epoch: 143, Loss: 0.3664, Train: 0.9087, Val: 0.9000, Test: 0.9267\n",
      "Epoch: 144, Loss: 0.3501, Train: 0.9104, Val: 0.8933, Test: 0.9267\n",
      "Epoch: 145, Loss: 0.3430, Train: 0.8779, Val: 0.8600, Test: 0.9100\n",
      "Epoch: 146, Loss: 0.3318, Train: 0.9296, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 147, Loss: 0.3339, Train: 0.9204, Val: 0.9100, Test: 0.9200\n",
      "Epoch: 148, Loss: 0.3446, Train: 0.8846, Val: 0.8833, Test: 0.8767\n",
      "Epoch: 149, Loss: 0.3509, Train: 0.9083, Val: 0.8867, Test: 0.9300\n",
      "Epoch: 150, Loss: 0.3398, Train: 0.8988, Val: 0.8700, Test: 0.9200\n",
      "Epoch: 151, Loss: 0.3405, Train: 0.8875, Val: 0.8833, Test: 0.8900\n",
      "Epoch: 152, Loss: 0.3494, Train: 0.9133, Val: 0.8967, Test: 0.9267\n",
      "Epoch: 153, Loss: 0.3455, Train: 0.8962, Val: 0.8767, Test: 0.9267\n",
      "Epoch: 154, Loss: 0.3509, Train: 0.9075, Val: 0.8967, Test: 0.9267\n",
      "Epoch: 155, Loss: 0.3546, Train: 0.8858, Val: 0.8900, Test: 0.8900\n",
      "Epoch: 156, Loss: 0.3416, Train: 0.8821, Val: 0.8800, Test: 0.9133\n",
      "Epoch: 157, Loss: 0.3392, Train: 0.9129, Val: 0.8833, Test: 0.9067\n",
      "Epoch: 158, Loss: 0.3341, Train: 0.8246, Val: 0.8133, Test: 0.8500\n",
      "Epoch: 159, Loss: 0.3377, Train: 0.9125, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 160, Loss: 0.3562, Train: 0.8912, Val: 0.8833, Test: 0.8867\n",
      "Epoch: 161, Loss: 0.3339, Train: 0.8775, Val: 0.8733, Test: 0.9067\n",
      "Epoch: 162, Loss: 0.3231, Train: 0.9275, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 163, Loss: 0.3236, Train: 0.8300, Val: 0.7933, Test: 0.8033\n",
      "Epoch: 164, Loss: 0.3301, Train: 0.9258, Val: 0.9267, Test: 0.9400\n",
      "Epoch: 165, Loss: 0.3356, Train: 0.8442, Val: 0.8100, Test: 0.8300\n",
      "Epoch: 166, Loss: 0.3233, Train: 0.8821, Val: 0.8733, Test: 0.8800\n",
      "Epoch: 167, Loss: 0.3225, Train: 0.8958, Val: 0.8733, Test: 0.9067\n",
      "Epoch: 168, Loss: 0.3429, Train: 0.9125, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 169, Loss: 0.3344, Train: 0.9217, Val: 0.9033, Test: 0.9300\n",
      "Epoch: 170, Loss: 0.3355, Train: 0.8429, Val: 0.8633, Test: 0.8700\n",
      "Epoch: 171, Loss: 0.3238, Train: 0.9221, Val: 0.9167, Test: 0.9167\n",
      "Epoch: 172, Loss: 0.3218, Train: 0.9275, Val: 0.9167, Test: 0.9300\n",
      "Epoch: 173, Loss: 0.3360, Train: 0.9254, Val: 0.9133, Test: 0.9233\n",
      "Epoch: 174, Loss: 0.3149, Train: 0.9154, Val: 0.9100, Test: 0.9233\n",
      "Epoch: 175, Loss: 0.3246, Train: 0.9100, Val: 0.8900, Test: 0.9267\n",
      "Epoch: 176, Loss: 0.3298, Train: 0.8754, Val: 0.8467, Test: 0.8667\n",
      "Epoch: 177, Loss: 0.3095, Train: 0.9008, Val: 0.8933, Test: 0.9033\n",
      "Epoch: 178, Loss: 0.3474, Train: 0.9183, Val: 0.9067, Test: 0.9500\n",
      "Epoch: 179, Loss: 0.3339, Train: 0.9100, Val: 0.9000, Test: 0.9067\n",
      "Epoch: 180, Loss: 0.3134, Train: 0.8758, Val: 0.8633, Test: 0.9033\n",
      "Epoch: 181, Loss: 0.3291, Train: 0.9283, Val: 0.9067, Test: 0.9333\n",
      "Epoch: 182, Loss: 0.3198, Train: 0.9187, Val: 0.9000, Test: 0.9433\n",
      "Epoch: 183, Loss: 0.3093, Train: 0.9071, Val: 0.8933, Test: 0.9133\n",
      "Epoch: 184, Loss: 0.3109, Train: 0.8854, Val: 0.8567, Test: 0.9100\n",
      "Epoch: 185, Loss: 0.3188, Train: 0.8746, Val: 0.8567, Test: 0.8633\n",
      "Epoch: 186, Loss: 0.3196, Train: 0.9104, Val: 0.8800, Test: 0.9133\n",
      "Epoch: 187, Loss: 0.3162, Train: 0.9271, Val: 0.9033, Test: 0.9300\n",
      "Epoch: 188, Loss: 0.3367, Train: 0.9250, Val: 0.9167, Test: 0.9367\n",
      "Epoch: 189, Loss: 0.3131, Train: 0.9308, Val: 0.9133, Test: 0.9233\n",
      "Epoch: 190, Loss: 0.3124, Train: 0.8638, Val: 0.8733, Test: 0.9167\n",
      "Epoch: 191, Loss: 0.3149, Train: 0.8621, Val: 0.8533, Test: 0.8900\n",
      "Epoch: 192, Loss: 0.3031, Train: 0.8712, Val: 0.8567, Test: 0.9033\n",
      "Epoch: 193, Loss: 0.3106, Train: 0.8867, Val: 0.8700, Test: 0.8700\n",
      "Epoch: 194, Loss: 0.3033, Train: 0.9246, Val: 0.9033, Test: 0.9200\n",
      "Epoch: 195, Loss: 0.2963, Train: 0.9283, Val: 0.9133, Test: 0.9267\n",
      "Epoch: 196, Loss: 0.2923, Train: 0.9329, Val: 0.9100, Test: 0.9367\n",
      "Epoch: 197, Loss: 0.3292, Train: 0.9167, Val: 0.9133, Test: 0.9067\n",
      "Epoch: 198, Loss: 0.3145, Train: 0.8892, Val: 0.8733, Test: 0.9133\n",
      "Epoch: 199, Loss: 0.2849, Train: 0.9179, Val: 0.8933, Test: 0.9300\n",
      "Epoch: 200, Loss: 0.2909, Train: 0.9246, Val: 0.8967, Test: 0.9267\n",
      "Epoch: 201, Loss: 0.2906, Train: 0.8125, Val: 0.8367, Test: 0.8433\n",
      "Epoch: 202, Loss: 0.3020, Train: 0.9379, Val: 0.9133, Test: 0.9300\n",
      "Epoch: 203, Loss: 0.2918, Train: 0.9075, Val: 0.9000, Test: 0.9333\n",
      "Epoch: 204, Loss: 0.3192, Train: 0.9392, Val: 0.9133, Test: 0.9333\n",
      "Epoch: 205, Loss: 0.2793, Train: 0.9008, Val: 0.8867, Test: 0.9000\n",
      "Epoch: 206, Loss: 0.2878, Train: 0.9325, Val: 0.9167, Test: 0.9333\n",
      "Epoch: 207, Loss: 0.2949, Train: 0.9242, Val: 0.9233, Test: 0.9467\n",
      "Epoch: 208, Loss: 0.2749, Train: 0.9133, Val: 0.8900, Test: 0.9233\n",
      "Epoch: 209, Loss: 0.2821, Train: 0.9004, Val: 0.8900, Test: 0.8967\n",
      "Epoch: 210, Loss: 0.2924, Train: 0.9163, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 211, Loss: 0.2864, Train: 0.9379, Val: 0.9200, Test: 0.9400\n",
      "Epoch: 212, Loss: 0.2860, Train: 0.8954, Val: 0.8900, Test: 0.8867\n",
      "Epoch: 213, Loss: 0.2724, Train: 0.8979, Val: 0.8733, Test: 0.8900\n",
      "Epoch: 214, Loss: 0.2776, Train: 0.9146, Val: 0.8933, Test: 0.9400\n",
      "Epoch: 215, Loss: 0.2800, Train: 0.9346, Val: 0.9133, Test: 0.9367\n",
      "Epoch: 216, Loss: 0.2818, Train: 0.8612, Val: 0.8233, Test: 0.8467\n",
      "Epoch: 217, Loss: 0.2858, Train: 0.9100, Val: 0.9067, Test: 0.9100\n",
      "Epoch: 218, Loss: 0.2945, Train: 0.8942, Val: 0.8933, Test: 0.9200\n",
      "Epoch: 219, Loss: 0.2852, Train: 0.9317, Val: 0.9233, Test: 0.9500\n",
      "Epoch: 220, Loss: 0.2679, Train: 0.9408, Val: 0.9167, Test: 0.9433\n",
      "Epoch: 221, Loss: 0.2673, Train: 0.9350, Val: 0.9167, Test: 0.9200\n",
      "Epoch: 222, Loss: 0.2817, Train: 0.9229, Val: 0.9167, Test: 0.9467\n",
      "Epoch: 223, Loss: 0.2698, Train: 0.8263, Val: 0.8100, Test: 0.8167\n",
      "Epoch: 224, Loss: 0.2823, Train: 0.9283, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 225, Loss: 0.2664, Train: 0.9267, Val: 0.9000, Test: 0.9433\n",
      "Epoch: 226, Loss: 0.2748, Train: 0.9392, Val: 0.9100, Test: 0.9400\n",
      "Epoch: 227, Loss: 0.2616, Train: 0.9038, Val: 0.8867, Test: 0.8833\n",
      "Epoch: 228, Loss: 0.2818, Train: 0.9308, Val: 0.9233, Test: 0.9467\n",
      "Epoch: 229, Loss: 0.2774, Train: 0.8638, Val: 0.8833, Test: 0.8967\n",
      "Epoch: 230, Loss: 0.2683, Train: 0.9283, Val: 0.9267, Test: 0.9500\n",
      "Epoch: 231, Loss: 0.2580, Train: 0.9275, Val: 0.8967, Test: 0.9233\n",
      "Epoch: 232, Loss: 0.2625, Train: 0.9167, Val: 0.9133, Test: 0.9133\n",
      "Epoch: 233, Loss: 0.2663, Train: 0.9371, Val: 0.9233, Test: 0.9367\n",
      "Epoch: 234, Loss: 0.2772, Train: 0.9383, Val: 0.9233, Test: 0.9567\n",
      "Epoch: 235, Loss: 0.2573, Train: 0.9167, Val: 0.8933, Test: 0.9300\n",
      "Epoch: 236, Loss: 0.2547, Train: 0.9367, Val: 0.9267, Test: 0.9500\n",
      "Epoch: 237, Loss: 0.2569, Train: 0.9437, Val: 0.9133, Test: 0.9400\n",
      "Epoch: 238, Loss: 0.2656, Train: 0.9096, Val: 0.9033, Test: 0.9333\n",
      "Epoch: 239, Loss: 0.2643, Train: 0.9354, Val: 0.9033, Test: 0.9300\n",
      "Epoch: 240, Loss: 0.2549, Train: 0.9121, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 241, Loss: 0.2428, Train: 0.9454, Val: 0.9267, Test: 0.9400\n",
      "Epoch: 242, Loss: 0.2630, Train: 0.9375, Val: 0.9200, Test: 0.9200\n",
      "Epoch: 243, Loss: 0.2549, Train: 0.9237, Val: 0.9067, Test: 0.9200\n",
      "Epoch: 244, Loss: 0.2559, Train: 0.9358, Val: 0.9033, Test: 0.9300\n",
      "Epoch: 245, Loss: 0.2588, Train: 0.9175, Val: 0.8900, Test: 0.9067\n",
      "Epoch: 246, Loss: 0.2527, Train: 0.9379, Val: 0.9200, Test: 0.9467\n",
      "Epoch: 247, Loss: 0.2536, Train: 0.9346, Val: 0.9067, Test: 0.9200\n",
      "Epoch: 248, Loss: 0.2614, Train: 0.9404, Val: 0.9100, Test: 0.9167\n",
      "Epoch: 249, Loss: 0.2528, Train: 0.9433, Val: 0.9133, Test: 0.9333\n",
      "Epoch: 250, Loss: 0.2568, Train: 0.9146, Val: 0.8967, Test: 0.9400\n",
      "Epoch: 251, Loss: 0.2432, Train: 0.8512, Val: 0.8467, Test: 0.8767\n",
      "Epoch: 252, Loss: 0.2422, Train: 0.8942, Val: 0.8733, Test: 0.8800\n",
      "Epoch: 253, Loss: 0.2390, Train: 0.9383, Val: 0.9267, Test: 0.9500\n",
      "Epoch: 254, Loss: 0.2490, Train: 0.9396, Val: 0.9167, Test: 0.9433\n",
      "Epoch: 255, Loss: 0.2322, Train: 0.9175, Val: 0.8967, Test: 0.9367\n",
      "Epoch: 256, Loss: 0.2827, Train: 0.9350, Val: 0.9267, Test: 0.9300\n",
      "Epoch: 257, Loss: 0.2391, Train: 0.9221, Val: 0.9067, Test: 0.9333\n",
      "Epoch: 258, Loss: 0.2408, Train: 0.8358, Val: 0.7933, Test: 0.8300\n",
      "Epoch: 259, Loss: 0.2468, Train: 0.9413, Val: 0.9200, Test: 0.9267\n",
      "Epoch: 260, Loss: 0.2334, Train: 0.9296, Val: 0.9233, Test: 0.9400\n",
      "Epoch: 261, Loss: 0.2338, Train: 0.9404, Val: 0.9267, Test: 0.9500\n",
      "Epoch: 262, Loss: 0.2302, Train: 0.9475, Val: 0.9200, Test: 0.9300\n",
      "Epoch: 263, Loss: 0.2482, Train: 0.9471, Val: 0.9100, Test: 0.9567\n",
      "Epoch: 264, Loss: 0.2305, Train: 0.9454, Val: 0.9233, Test: 0.9600\n",
      "Epoch: 265, Loss: 0.2403, Train: 0.9458, Val: 0.9167, Test: 0.9533\n",
      "Epoch: 266, Loss: 0.2271, Train: 0.9513, Val: 0.9267, Test: 0.9567\n",
      "Epoch: 267, Loss: 0.2323, Train: 0.9425, Val: 0.9067, Test: 0.9167\n",
      "Epoch: 268, Loss: 0.2255, Train: 0.9292, Val: 0.9133, Test: 0.9267\n",
      "Epoch: 269, Loss: 0.2302, Train: 0.9479, Val: 0.9233, Test: 0.9600\n",
      "Epoch: 270, Loss: 0.2190, Train: 0.9475, Val: 0.9100, Test: 0.9433\n",
      "Epoch: 271, Loss: 0.2158, Train: 0.9467, Val: 0.9367, Test: 0.9567\n",
      "Epoch: 272, Loss: 0.2138, Train: 0.9467, Val: 0.9200, Test: 0.9600\n",
      "Epoch: 273, Loss: 0.2232, Train: 0.8838, Val: 0.8700, Test: 0.8700\n",
      "Epoch: 274, Loss: 0.2196, Train: 0.9471, Val: 0.9167, Test: 0.9467\n",
      "Epoch: 275, Loss: 0.2145, Train: 0.9137, Val: 0.8867, Test: 0.9267\n",
      "Epoch: 276, Loss: 0.2164, Train: 0.8892, Val: 0.8567, Test: 0.8700\n",
      "Epoch: 277, Loss: 0.2179, Train: 0.9163, Val: 0.9167, Test: 0.9200\n",
      "Epoch: 278, Loss: 0.2051, Train: 0.8712, Val: 0.8467, Test: 0.9033\n",
      "Epoch: 279, Loss: 0.2049, Train: 0.9500, Val: 0.9200, Test: 0.9333\n",
      "Epoch: 280, Loss: 0.2074, Train: 0.9596, Val: 0.9400, Test: 0.9633\n",
      "Epoch: 281, Loss: 0.2178, Train: 0.9179, Val: 0.9033, Test: 0.9067\n",
      "Epoch: 282, Loss: 0.2245, Train: 0.9417, Val: 0.8967, Test: 0.9333\n",
      "Epoch: 283, Loss: 0.2030, Train: 0.9508, Val: 0.9433, Test: 0.9600\n",
      "Epoch: 284, Loss: 0.1919, Train: 0.9571, Val: 0.9367, Test: 0.9567\n",
      "Epoch: 285, Loss: 0.1934, Train: 0.9354, Val: 0.9133, Test: 0.9200\n",
      "Epoch: 286, Loss: 0.1820, Train: 0.9571, Val: 0.9333, Test: 0.9367\n",
      "Epoch: 287, Loss: 0.1989, Train: 0.9563, Val: 0.9433, Test: 0.9533\n",
      "Epoch: 288, Loss: 0.1930, Train: 0.9167, Val: 0.8900, Test: 0.9033\n",
      "Epoch: 289, Loss: 0.1749, Train: 0.8871, Val: 0.8833, Test: 0.9100\n",
      "Epoch: 290, Loss: 0.1904, Train: 0.9592, Val: 0.9467, Test: 0.9567\n",
      "Epoch: 291, Loss: 0.1802, Train: 0.9463, Val: 0.9300, Test: 0.9400\n",
      "Epoch: 292, Loss: 0.1808, Train: 0.9246, Val: 0.9133, Test: 0.9367\n",
      "Epoch: 293, Loss: 0.1692, Train: 0.9633, Val: 0.9333, Test: 0.9667\n",
      "Epoch: 294, Loss: 0.1698, Train: 0.9688, Val: 0.9567, Test: 0.9733\n",
      "Epoch: 295, Loss: 0.1767, Train: 0.9696, Val: 0.9500, Test: 0.9667\n",
      "Epoch: 296, Loss: 0.1795, Train: 0.9654, Val: 0.9367, Test: 0.9500\n",
      "Epoch: 297, Loss: 0.1719, Train: 0.9533, Val: 0.9400, Test: 0.9500\n",
      "Epoch: 298, Loss: 0.1680, Train: 0.9504, Val: 0.9333, Test: 0.9500\n",
      "Epoch: 299, Loss: 0.1527, Train: 0.9675, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 300, Loss: 0.1593, Train: 0.9692, Val: 0.9500, Test: 0.9600\n",
      "Epoch: 301, Loss: 0.1403, Train: 0.9708, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 302, Loss: 0.1547, Train: 0.9717, Val: 0.9633, Test: 0.9733\n",
      "Epoch: 303, Loss: 0.1531, Train: 0.9696, Val: 0.9600, Test: 0.9733\n",
      "Epoch: 304, Loss: 0.1518, Train: 0.9667, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 305, Loss: 0.1365, Train: 0.9550, Val: 0.9500, Test: 0.9500\n",
      "Epoch: 306, Loss: 0.1407, Train: 0.9688, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 307, Loss: 0.1682, Train: 0.9371, Val: 0.9367, Test: 0.9567\n",
      "Epoch: 308, Loss: 0.1436, Train: 0.9688, Val: 0.9600, Test: 0.9800\n",
      "Epoch: 309, Loss: 0.1515, Train: 0.9788, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 310, Loss: 0.1361, Train: 0.9788, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 311, Loss: 0.1304, Train: 0.9683, Val: 0.9600, Test: 0.9833\n",
      "Epoch: 312, Loss: 0.1389, Train: 0.9688, Val: 0.9567, Test: 0.9833\n",
      "Epoch: 313, Loss: 0.1544, Train: 0.8954, Val: 0.8967, Test: 0.8867\n",
      "Epoch: 314, Loss: 0.1233, Train: 0.9854, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 315, Loss: 0.1219, Train: 0.9821, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 316, Loss: 0.1228, Train: 0.9808, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 317, Loss: 0.1238, Train: 0.9450, Val: 0.9433, Test: 0.9400\n",
      "Epoch: 318, Loss: 0.1374, Train: 0.9625, Val: 0.9367, Test: 0.9567\n",
      "Epoch: 319, Loss: 0.1293, Train: 0.9867, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 320, Loss: 0.1154, Train: 0.9700, Val: 0.9600, Test: 0.9700\n",
      "Epoch: 321, Loss: 0.1279, Train: 0.9808, Val: 0.9667, Test: 0.9767\n",
      "Epoch: 322, Loss: 0.1065, Train: 0.9825, Val: 0.9667, Test: 0.9833\n",
      "Epoch: 323, Loss: 0.0962, Train: 0.9742, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 324, Loss: 0.1035, Train: 0.9804, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 325, Loss: 0.1122, Train: 0.9758, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 326, Loss: 0.1216, Train: 0.9121, Val: 0.9067, Test: 0.9067\n",
      "Epoch: 327, Loss: 0.1095, Train: 0.9842, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 328, Loss: 0.1120, Train: 0.9867, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 329, Loss: 0.1017, Train: 0.9850, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 330, Loss: 0.0987, Train: 0.9846, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 331, Loss: 0.0956, Train: 0.9825, Val: 0.9667, Test: 0.9833\n",
      "Epoch: 332, Loss: 0.0868, Train: 0.9846, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 333, Loss: 0.0940, Train: 0.9788, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 334, Loss: 0.1119, Train: 0.9113, Val: 0.9000, Test: 0.9067\n",
      "Epoch: 335, Loss: 0.0970, Train: 0.9908, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 336, Loss: 0.1153, Train: 0.9738, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 337, Loss: 0.0875, Train: 0.9404, Val: 0.9467, Test: 0.9333\n",
      "Epoch: 338, Loss: 0.0861, Train: 0.9788, Val: 0.9633, Test: 0.9767\n",
      "Epoch: 339, Loss: 0.0803, Train: 0.9867, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 340, Loss: 0.0813, Train: 0.9896, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 341, Loss: 0.1231, Train: 0.9817, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 342, Loss: 0.0851, Train: 0.9871, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 343, Loss: 0.0907, Train: 0.9900, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 344, Loss: 0.0827, Train: 0.9925, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 345, Loss: 0.1017, Train: 0.9888, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 346, Loss: 0.0869, Train: 0.9575, Val: 0.9500, Test: 0.9567\n",
      "Epoch: 347, Loss: 0.0920, Train: 0.9929, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 348, Loss: 0.0866, Train: 0.9875, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 349, Loss: 0.0805, Train: 0.9725, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 350, Loss: 0.0889, Train: 0.9800, Val: 0.9667, Test: 0.9833\n",
      "Epoch: 351, Loss: 0.1018, Train: 0.9858, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 352, Loss: 0.0801, Train: 0.9850, Val: 0.9733, Test: 0.9867\n",
      "Epoch: 353, Loss: 0.0821, Train: 0.9925, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 354, Loss: 0.0844, Train: 0.9467, Val: 0.9667, Test: 0.9500\n",
      "Epoch: 355, Loss: 0.0884, Train: 0.9925, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 356, Loss: 0.0778, Train: 0.9846, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 357, Loss: 0.0664, Train: 0.9846, Val: 0.9700, Test: 0.9867\n",
      "Epoch: 358, Loss: 0.0735, Train: 0.9804, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 359, Loss: 0.0681, Train: 0.9908, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 360, Loss: 0.0723, Train: 0.9933, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 361, Loss: 0.0853, Train: 0.9933, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 362, Loss: 0.0736, Train: 0.9933, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 363, Loss: 0.0818, Train: 0.9429, Val: 0.9367, Test: 0.9400\n",
      "Epoch: 364, Loss: 0.0796, Train: 0.9950, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 365, Loss: 0.0829, Train: 0.9908, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 366, Loss: 0.0717, Train: 0.9933, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 367, Loss: 0.0679, Train: 0.9942, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 368, Loss: 0.0913, Train: 0.9933, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 369, Loss: 0.0710, Train: 0.9958, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 370, Loss: 0.0611, Train: 0.9962, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 371, Loss: 0.0776, Train: 0.9900, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 372, Loss: 0.0700, Train: 0.9942, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 373, Loss: 0.0615, Train: 0.9929, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 374, Loss: 0.0779, Train: 0.9954, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 375, Loss: 0.0678, Train: 0.9904, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 376, Loss: 0.0710, Train: 0.9946, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 377, Loss: 0.0563, Train: 0.9908, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 378, Loss: 0.0807, Train: 0.9537, Val: 0.9600, Test: 0.9767\n",
      "Epoch: 379, Loss: 0.0783, Train: 0.9896, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 380, Loss: 0.0763, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 381, Loss: 0.0683, Train: 0.9921, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 382, Loss: 0.0614, Train: 0.9938, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 383, Loss: 0.0807, Train: 0.9921, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 384, Loss: 0.0839, Train: 0.9842, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 385, Loss: 0.0630, Train: 0.9971, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 386, Loss: 0.0558, Train: 0.9950, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 387, Loss: 0.0722, Train: 0.9908, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 388, Loss: 0.0674, Train: 0.9862, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 389, Loss: 0.0584, Train: 0.9942, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 390, Loss: 0.0613, Train: 0.9904, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 391, Loss: 0.0617, Train: 0.9950, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 392, Loss: 0.0579, Train: 0.9962, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 393, Loss: 0.0688, Train: 0.9938, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 394, Loss: 0.0639, Train: 0.9854, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 395, Loss: 0.0717, Train: 0.9954, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 396, Loss: 0.0546, Train: 0.9958, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 397, Loss: 0.0611, Train: 0.9742, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 398, Loss: 0.0694, Train: 0.9704, Val: 0.9567, Test: 0.9733\n",
      "Epoch: 399, Loss: 0.0656, Train: 0.9812, Val: 0.9633, Test: 0.9767\n",
      "Epoch: 400, Loss: 0.0581, Train: 0.9846, Val: 0.9867, Test: 0.9767\n",
      "Epoch: 401, Loss: 0.0934, Train: 0.9900, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 402, Loss: 0.0601, Train: 0.9912, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 403, Loss: 0.0812, Train: 0.9862, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 404, Loss: 0.0794, Train: 0.9979, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 405, Loss: 0.0671, Train: 0.9942, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 406, Loss: 0.0571, Train: 0.9979, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 407, Loss: 0.0534, Train: 0.9971, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 408, Loss: 0.0540, Train: 0.9875, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 409, Loss: 0.0683, Train: 0.9967, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 410, Loss: 0.0548, Train: 0.9842, Val: 0.9800, Test: 0.9700\n",
      "Epoch: 411, Loss: 0.0796, Train: 0.9921, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 412, Loss: 0.0525, Train: 0.9946, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 413, Loss: 0.0636, Train: 0.9733, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 414, Loss: 0.0729, Train: 0.9904, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 415, Loss: 0.0557, Train: 0.9883, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 416, Loss: 0.0518, Train: 0.9958, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 417, Loss: 0.0482, Train: 0.9954, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 418, Loss: 0.0526, Train: 0.9942, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 419, Loss: 0.0688, Train: 0.9954, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 420, Loss: 0.0532, Train: 0.9950, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 421, Loss: 0.0645, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 422, Loss: 0.0473, Train: 0.9962, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 423, Loss: 0.0485, Train: 0.9975, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 424, Loss: 0.0483, Train: 0.9979, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 425, Loss: 0.0555, Train: 0.9750, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 426, Loss: 0.0553, Train: 0.9975, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 427, Loss: 0.0509, Train: 0.9967, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 428, Loss: 0.0485, Train: 0.9958, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 429, Loss: 0.0512, Train: 0.9925, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 430, Loss: 0.0650, Train: 0.9613, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 431, Loss: 0.0566, Train: 0.9946, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 432, Loss: 0.0436, Train: 0.9817, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 433, Loss: 0.0643, Train: 0.9938, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 434, Loss: 0.0642, Train: 0.9888, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 435, Loss: 0.0532, Train: 0.9950, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 436, Loss: 0.0539, Train: 0.9967, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 437, Loss: 0.0407, Train: 0.9979, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 438, Loss: 0.0634, Train: 0.9688, Val: 0.9600, Test: 0.9733\n",
      "Epoch: 439, Loss: 0.0761, Train: 0.9938, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 440, Loss: 0.0488, Train: 0.9888, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 441, Loss: 0.0526, Train: 0.9596, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 442, Loss: 0.0719, Train: 0.9912, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 443, Loss: 0.0650, Train: 0.9750, Val: 0.9800, Test: 0.9667\n",
      "Epoch: 444, Loss: 0.0534, Train: 0.9971, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 445, Loss: 0.0710, Train: 0.9779, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 446, Loss: 0.0559, Train: 0.9962, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 447, Loss: 0.0435, Train: 0.9962, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 448, Loss: 0.0809, Train: 0.9750, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 449, Loss: 0.0627, Train: 0.9954, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 450, Loss: 0.0403, Train: 0.9967, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 451, Loss: 0.0399, Train: 0.9892, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 452, Loss: 0.0503, Train: 0.9958, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 453, Loss: 0.0663, Train: 0.9663, Val: 0.9633, Test: 0.9700\n",
      "Epoch: 454, Loss: 0.0570, Train: 0.9954, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 455, Loss: 0.0444, Train: 0.9971, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 456, Loss: 0.0416, Train: 0.9925, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 457, Loss: 0.0449, Train: 0.9979, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 458, Loss: 0.0580, Train: 0.9838, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 459, Loss: 0.0537, Train: 0.9917, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 460, Loss: 0.0370, Train: 0.9950, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 461, Loss: 0.0446, Train: 0.9758, Val: 0.9867, Test: 0.9733\n",
      "Epoch: 462, Loss: 0.0655, Train: 0.9929, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 463, Loss: 0.0514, Train: 0.9921, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 464, Loss: 0.0523, Train: 0.9979, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 465, Loss: 0.0635, Train: 0.9888, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 466, Loss: 0.0486, Train: 0.9946, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 467, Loss: 0.0419, Train: 0.9988, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 468, Loss: 0.0384, Train: 0.9975, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 469, Loss: 0.0455, Train: 0.9971, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 470, Loss: 0.0435, Train: 0.9988, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 471, Loss: 0.0458, Train: 0.9979, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 472, Loss: 0.0436, Train: 0.9983, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 473, Loss: 0.0471, Train: 0.9975, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 474, Loss: 0.1099, Train: 0.9871, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 475, Loss: 0.0473, Train: 0.9946, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 476, Loss: 0.0551, Train: 0.9958, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 477, Loss: 0.0477, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 478, Loss: 0.0464, Train: 0.9950, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 479, Loss: 0.0372, Train: 0.9942, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 480, Loss: 0.0645, Train: 0.9933, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 481, Loss: 0.0447, Train: 0.9942, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 482, Loss: 0.0416, Train: 0.9979, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 483, Loss: 0.0475, Train: 0.9975, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 484, Loss: 0.0456, Train: 0.9971, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 485, Loss: 0.0403, Train: 0.9962, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 486, Loss: 0.0364, Train: 0.9971, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 487, Loss: 0.0428, Train: 0.9925, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 488, Loss: 0.0465, Train: 0.9950, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 489, Loss: 0.0387, Train: 0.9950, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 490, Loss: 0.0499, Train: 0.9962, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 491, Loss: 0.0370, Train: 0.9954, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 492, Loss: 0.0349, Train: 0.9979, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 493, Loss: 0.0415, Train: 0.9992, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 494, Loss: 0.0349, Train: 0.9958, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 495, Loss: 0.0679, Train: 0.9867, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 496, Loss: 0.0414, Train: 0.9971, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 497, Loss: 0.0374, Train: 0.9988, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 498, Loss: 0.0341, Train: 0.9979, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 499, Loss: 0.0822, Train: 0.9908, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 500, Loss: 0.0610, Train: 0.9925, Val: 0.9900, Test: 0.9833\n",
      "### Run 0 - val loss: 0.002, test acc: 0.993\n",
      "Accuracies in each run:  [0.9933333333333333]\n",
      "test acc - mean: 0.993, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'diffpool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mincut\n",
      "Epoch: 001, Loss: 1.3456, Train: 0.6683, Val: 0.7133, Test: 0.6467\n",
      "Epoch: 002, Loss: 1.2571, Train: 0.6883, Val: 0.7500, Test: 0.6700\n",
      "Epoch: 003, Loss: 1.2319, Train: 0.6671, Val: 0.7233, Test: 0.6567\n",
      "Epoch: 004, Loss: 1.1923, Train: 0.7471, Val: 0.8000, Test: 0.7500\n",
      "Epoch: 005, Loss: 1.1746, Train: 0.7612, Val: 0.8133, Test: 0.7833\n",
      "Epoch: 006, Loss: 1.1041, Train: 0.5896, Val: 0.5900, Test: 0.6133\n",
      "Epoch: 007, Loss: 1.0735, Train: 0.7700, Val: 0.8100, Test: 0.8000\n",
      "Epoch: 008, Loss: 1.0247, Train: 0.7479, Val: 0.7867, Test: 0.7567\n",
      "Epoch: 009, Loss: 0.9955, Train: 0.7562, Val: 0.7667, Test: 0.7667\n",
      "Epoch: 010, Loss: 0.9727, Train: 0.8096, Val: 0.8200, Test: 0.8200\n",
      "Epoch: 011, Loss: 0.9352, Train: 0.8329, Val: 0.8667, Test: 0.8533\n",
      "Epoch: 012, Loss: 0.9222, Train: 0.7000, Val: 0.6967, Test: 0.7000\n",
      "Epoch: 013, Loss: 0.9029, Train: 0.7854, Val: 0.7933, Test: 0.7900\n",
      "Epoch: 014, Loss: 0.9089, Train: 0.8538, Val: 0.8700, Test: 0.8600\n",
      "Epoch: 015, Loss: 0.8691, Train: 0.8758, Val: 0.9033, Test: 0.8833\n",
      "Epoch: 016, Loss: 0.8565, Train: 0.8596, Val: 0.8833, Test: 0.8633\n",
      "Epoch: 017, Loss: 0.8435, Train: 0.8888, Val: 0.9000, Test: 0.9033\n",
      "Epoch: 018, Loss: 0.8518, Train: 0.7575, Val: 0.7533, Test: 0.7633\n",
      "Epoch: 019, Loss: 0.8442, Train: 0.8683, Val: 0.8767, Test: 0.8600\n",
      "Epoch: 020, Loss: 0.8005, Train: 0.9017, Val: 0.9200, Test: 0.9233\n",
      "Epoch: 021, Loss: 0.7867, Train: 0.8921, Val: 0.9133, Test: 0.9167\n",
      "Epoch: 022, Loss: 0.7613, Train: 0.9158, Val: 0.9267, Test: 0.9233\n",
      "Epoch: 023, Loss: 0.7668, Train: 0.9062, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 024, Loss: 0.7631, Train: 0.9171, Val: 0.9200, Test: 0.9267\n",
      "Epoch: 025, Loss: 0.7257, Train: 0.9108, Val: 0.9133, Test: 0.9167\n",
      "Epoch: 026, Loss: 0.7363, Train: 0.8888, Val: 0.8933, Test: 0.9133\n",
      "Epoch: 027, Loss: 0.7264, Train: 0.8654, Val: 0.8733, Test: 0.8633\n",
      "Epoch: 028, Loss: 0.7074, Train: 0.9175, Val: 0.9333, Test: 0.9300\n",
      "Epoch: 029, Loss: 0.7077, Train: 0.8604, Val: 0.8600, Test: 0.8633\n",
      "Epoch: 030, Loss: 0.7364, Train: 0.9196, Val: 0.9267, Test: 0.9300\n",
      "Epoch: 031, Loss: 0.7137, Train: 0.8712, Val: 0.8767, Test: 0.8800\n",
      "Epoch: 032, Loss: 0.7427, Train: 0.9279, Val: 0.9433, Test: 0.9333\n",
      "Epoch: 033, Loss: 0.6924, Train: 0.9396, Val: 0.9533, Test: 0.9300\n",
      "Epoch: 034, Loss: 0.6876, Train: 0.9375, Val: 0.9567, Test: 0.9367\n",
      "Epoch: 035, Loss: 0.6839, Train: 0.9308, Val: 0.9433, Test: 0.9433\n",
      "Epoch: 036, Loss: 0.6820, Train: 0.9108, Val: 0.9100, Test: 0.9167\n",
      "Epoch: 037, Loss: 0.6869, Train: 0.9413, Val: 0.9533, Test: 0.9500\n",
      "Epoch: 038, Loss: 0.6716, Train: 0.9463, Val: 0.9533, Test: 0.9500\n",
      "Epoch: 039, Loss: 0.6775, Train: 0.9117, Val: 0.9167, Test: 0.9167\n",
      "Epoch: 040, Loss: 0.6951, Train: 0.9458, Val: 0.9600, Test: 0.9367\n",
      "Epoch: 041, Loss: 0.6604, Train: 0.9050, Val: 0.9133, Test: 0.9067\n",
      "Epoch: 042, Loss: 0.6941, Train: 0.9108, Val: 0.9233, Test: 0.9133\n",
      "Epoch: 043, Loss: 0.6852, Train: 0.8696, Val: 0.8833, Test: 0.8800\n",
      "Epoch: 044, Loss: 0.6580, Train: 0.9213, Val: 0.9233, Test: 0.9200\n",
      "Epoch: 045, Loss: 0.6628, Train: 0.9450, Val: 0.9533, Test: 0.9400\n",
      "Epoch: 046, Loss: 0.6609, Train: 0.9496, Val: 0.9567, Test: 0.9500\n",
      "Epoch: 047, Loss: 0.6711, Train: 0.9342, Val: 0.9467, Test: 0.9267\n",
      "Epoch: 048, Loss: 0.6591, Train: 0.9421, Val: 0.9500, Test: 0.9400\n",
      "Epoch: 049, Loss: 0.6383, Train: 0.9371, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 050, Loss: 0.6413, Train: 0.9496, Val: 0.9600, Test: 0.9467\n",
      "Epoch: 051, Loss: 0.6509, Train: 0.9442, Val: 0.9633, Test: 0.9467\n",
      "Epoch: 052, Loss: 0.6627, Train: 0.9492, Val: 0.9633, Test: 0.9400\n",
      "Epoch: 053, Loss: 0.6246, Train: 0.9537, Val: 0.9600, Test: 0.9467\n",
      "Epoch: 054, Loss: 0.6501, Train: 0.9546, Val: 0.9633, Test: 0.9433\n",
      "Epoch: 055, Loss: 0.6411, Train: 0.9437, Val: 0.9600, Test: 0.9367\n",
      "Epoch: 056, Loss: 0.6193, Train: 0.9463, Val: 0.9533, Test: 0.9367\n",
      "Epoch: 057, Loss: 0.6262, Train: 0.9596, Val: 0.9667, Test: 0.9567\n",
      "Epoch: 058, Loss: 0.6116, Train: 0.9554, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 059, Loss: 0.5971, Train: 0.9600, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 060, Loss: 0.5896, Train: 0.9463, Val: 0.9533, Test: 0.9600\n",
      "Epoch: 061, Loss: 0.5743, Train: 0.9621, Val: 0.9633, Test: 0.9467\n",
      "Epoch: 062, Loss: 0.6024, Train: 0.9450, Val: 0.9533, Test: 0.9433\n",
      "Epoch: 063, Loss: 0.5914, Train: 0.9533, Val: 0.9567, Test: 0.9500\n",
      "Epoch: 064, Loss: 0.5613, Train: 0.9367, Val: 0.9567, Test: 0.9433\n",
      "Epoch: 065, Loss: 0.5589, Train: 0.9629, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 066, Loss: 0.5749, Train: 0.9642, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 067, Loss: 0.5566, Train: 0.9688, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 068, Loss: 0.5665, Train: 0.9371, Val: 0.9467, Test: 0.9367\n",
      "Epoch: 069, Loss: 0.5410, Train: 0.9025, Val: 0.9233, Test: 0.9267\n",
      "Epoch: 070, Loss: 0.5581, Train: 0.9446, Val: 0.9733, Test: 0.9533\n",
      "Epoch: 071, Loss: 0.5312, Train: 0.9654, Val: 0.9800, Test: 0.9600\n",
      "Epoch: 072, Loss: 0.5379, Train: 0.9646, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 073, Loss: 0.5498, Train: 0.9704, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 074, Loss: 0.5344, Train: 0.9621, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 075, Loss: 0.5426, Train: 0.9508, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 076, Loss: 0.5194, Train: 0.9733, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 077, Loss: 0.5235, Train: 0.9654, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 078, Loss: 0.5186, Train: 0.9725, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 079, Loss: 0.5167, Train: 0.9717, Val: 0.9867, Test: 0.9667\n",
      "Epoch: 080, Loss: 0.5160, Train: 0.9733, Val: 0.9800, Test: 0.9667\n",
      "Epoch: 081, Loss: 0.5189, Train: 0.9742, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 082, Loss: 0.5085, Train: 0.9533, Val: 0.9667, Test: 0.9500\n",
      "Epoch: 083, Loss: 0.5170, Train: 0.9733, Val: 0.9833, Test: 0.9700\n",
      "Epoch: 084, Loss: 0.5161, Train: 0.9729, Val: 0.9833, Test: 0.9700\n",
      "Epoch: 085, Loss: 0.5005, Train: 0.9637, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 086, Loss: 0.5187, Train: 0.9771, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 087, Loss: 0.4945, Train: 0.9650, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 088, Loss: 0.4966, Train: 0.9788, Val: 0.9900, Test: 0.9700\n",
      "Epoch: 089, Loss: 0.4935, Train: 0.9679, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 090, Loss: 0.4978, Train: 0.9792, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 091, Loss: 0.4875, Train: 0.9738, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 092, Loss: 0.4924, Train: 0.9804, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 093, Loss: 0.4817, Train: 0.9721, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 094, Loss: 0.5038, Train: 0.9729, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 095, Loss: 0.4897, Train: 0.9754, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 096, Loss: 0.4880, Train: 0.9658, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 097, Loss: 0.4883, Train: 0.9704, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 098, Loss: 0.4836, Train: 0.9817, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 099, Loss: 0.4975, Train: 0.9754, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 100, Loss: 0.4806, Train: 0.9796, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 101, Loss: 0.4782, Train: 0.9808, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 102, Loss: 0.4774, Train: 0.9804, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 103, Loss: 0.4862, Train: 0.9754, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 104, Loss: 0.4744, Train: 0.9613, Val: 0.9833, Test: 0.9533\n",
      "Epoch: 105, Loss: 0.4799, Train: 0.9762, Val: 0.9867, Test: 0.9700\n",
      "Epoch: 106, Loss: 0.4915, Train: 0.9817, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 107, Loss: 0.4872, Train: 0.9563, Val: 0.9600, Test: 0.9700\n",
      "Epoch: 108, Loss: 0.4850, Train: 0.9458, Val: 0.9567, Test: 0.9433\n",
      "Epoch: 109, Loss: 0.4907, Train: 0.9667, Val: 0.9733, Test: 0.9867\n",
      "Epoch: 110, Loss: 0.5192, Train: 0.9825, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 111, Loss: 0.4802, Train: 0.9792, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 112, Loss: 0.4773, Train: 0.9796, Val: 0.9933, Test: 0.9733\n",
      "Epoch: 113, Loss: 0.4876, Train: 0.9688, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 114, Loss: 0.4738, Train: 0.9604, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 115, Loss: 0.4791, Train: 0.9658, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 116, Loss: 0.4854, Train: 0.9762, Val: 0.9933, Test: 0.9633\n",
      "Epoch: 117, Loss: 0.4680, Train: 0.9717, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 118, Loss: 0.4826, Train: 0.9788, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 119, Loss: 0.4695, Train: 0.9779, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 120, Loss: 0.4779, Train: 0.9600, Val: 0.9800, Test: 0.9467\n",
      "Epoch: 121, Loss: 0.4804, Train: 0.9742, Val: 0.9867, Test: 0.9700\n",
      "Epoch: 122, Loss: 0.4758, Train: 0.9704, Val: 0.9900, Test: 0.9600\n",
      "Epoch: 123, Loss: 0.4829, Train: 0.9492, Val: 0.9533, Test: 0.9733\n",
      "Epoch: 124, Loss: 0.4593, Train: 0.9796, Val: 0.9900, Test: 0.9633\n",
      "Epoch: 125, Loss: 0.4696, Train: 0.9829, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 126, Loss: 0.4620, Train: 0.9817, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 127, Loss: 0.4649, Train: 0.9838, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 128, Loss: 0.4624, Train: 0.9783, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 129, Loss: 0.4824, Train: 0.9554, Val: 0.9667, Test: 0.9767\n",
      "Epoch: 130, Loss: 0.4750, Train: 0.9817, Val: 0.9900, Test: 0.9767\n",
      "Epoch: 131, Loss: 0.4721, Train: 0.9850, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 132, Loss: 0.4802, Train: 0.9800, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 133, Loss: 0.4582, Train: 0.9829, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 134, Loss: 0.4626, Train: 0.9783, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 135, Loss: 0.4662, Train: 0.9779, Val: 0.9933, Test: 0.9667\n",
      "Epoch: 136, Loss: 0.4627, Train: 0.9838, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 137, Loss: 0.4637, Train: 0.9783, Val: 0.9833, Test: 0.9633\n",
      "Epoch: 138, Loss: 0.4681, Train: 0.9821, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 139, Loss: 0.4597, Train: 0.9804, Val: 0.9933, Test: 0.9667\n",
      "Epoch: 140, Loss: 0.4540, Train: 0.9738, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 141, Loss: 0.4607, Train: 0.9846, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 142, Loss: 0.4676, Train: 0.9842, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 143, Loss: 0.4617, Train: 0.9846, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 144, Loss: 0.4661, Train: 0.9850, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 145, Loss: 0.4699, Train: 0.9838, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 146, Loss: 0.4600, Train: 0.9842, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 147, Loss: 0.4588, Train: 0.9758, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 148, Loss: 0.4676, Train: 0.9854, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 149, Loss: 0.4592, Train: 0.9833, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 150, Loss: 0.4628, Train: 0.9833, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 151, Loss: 0.4685, Train: 0.9800, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 152, Loss: 0.4609, Train: 0.9767, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 153, Loss: 0.4582, Train: 0.9750, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 154, Loss: 0.4549, Train: 0.9833, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 155, Loss: 0.4717, Train: 0.9846, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 156, Loss: 0.4509, Train: 0.9850, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 157, Loss: 0.4654, Train: 0.9775, Val: 0.9900, Test: 0.9667\n",
      "Epoch: 158, Loss: 0.4543, Train: 0.9821, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 159, Loss: 0.4670, Train: 0.9842, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 160, Loss: 0.4550, Train: 0.9825, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 161, Loss: 0.4608, Train: 0.9862, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 162, Loss: 0.4512, Train: 0.9838, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 163, Loss: 0.4492, Train: 0.9812, Val: 0.9933, Test: 0.9700\n",
      "Epoch: 164, Loss: 0.4498, Train: 0.9842, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 165, Loss: 0.4521, Train: 0.9771, Val: 0.9800, Test: 0.9667\n",
      "Epoch: 166, Loss: 0.4489, Train: 0.9825, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 167, Loss: 0.4597, Train: 0.9838, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 168, Loss: 0.4448, Train: 0.9862, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 169, Loss: 0.4482, Train: 0.9850, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 170, Loss: 0.4453, Train: 0.9812, Val: 0.9967, Test: 0.9700\n",
      "Epoch: 171, Loss: 0.4513, Train: 0.9879, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 172, Loss: 0.4515, Train: 0.9858, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 173, Loss: 0.4649, Train: 0.9758, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 174, Loss: 0.4682, Train: 0.9746, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 175, Loss: 0.4608, Train: 0.9879, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 176, Loss: 0.4461, Train: 0.9867, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 177, Loss: 0.4648, Train: 0.9800, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 178, Loss: 0.4562, Train: 0.9804, Val: 0.9867, Test: 0.9667\n",
      "Epoch: 179, Loss: 0.4608, Train: 0.9821, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 180, Loss: 0.4609, Train: 0.9842, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 181, Loss: 0.4643, Train: 0.9879, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 182, Loss: 0.4603, Train: 0.9829, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 183, Loss: 0.4614, Train: 0.9842, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 184, Loss: 0.4463, Train: 0.9838, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 185, Loss: 0.4523, Train: 0.9871, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 186, Loss: 0.4444, Train: 0.9858, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 187, Loss: 0.4568, Train: 0.9846, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 188, Loss: 0.4720, Train: 0.9821, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 189, Loss: 0.4555, Train: 0.9879, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 190, Loss: 0.4481, Train: 0.9862, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 191, Loss: 0.4668, Train: 0.9833, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 192, Loss: 0.4567, Train: 0.9858, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 193, Loss: 0.4490, Train: 0.9717, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 194, Loss: 0.4446, Train: 0.9846, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 195, Loss: 0.4552, Train: 0.9792, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 196, Loss: 0.4440, Train: 0.9875, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 197, Loss: 0.4439, Train: 0.9858, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 198, Loss: 0.4397, Train: 0.9846, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 199, Loss: 0.4454, Train: 0.9833, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 200, Loss: 0.4487, Train: 0.9833, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 201, Loss: 0.4437, Train: 0.9883, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 202, Loss: 0.4517, Train: 0.9846, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 203, Loss: 0.4485, Train: 0.9879, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 204, Loss: 0.4434, Train: 0.9875, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 205, Loss: 0.4439, Train: 0.9842, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 206, Loss: 0.4544, Train: 0.9846, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 207, Loss: 0.4511, Train: 0.9796, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 208, Loss: 0.4529, Train: 0.9883, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 209, Loss: 0.4710, Train: 0.9862, Val: 1.0000, Test: 0.9800\n",
      "Epoch: 210, Loss: 0.4484, Train: 0.9896, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 211, Loss: 0.4429, Train: 0.9871, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 212, Loss: 0.4631, Train: 0.9858, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 213, Loss: 0.4548, Train: 0.9796, Val: 0.9833, Test: 0.9667\n",
      "Epoch: 214, Loss: 0.4429, Train: 0.9879, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 215, Loss: 0.4458, Train: 0.9862, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 216, Loss: 0.4582, Train: 0.9629, Val: 0.9833, Test: 0.9600\n",
      "Epoch: 217, Loss: 0.4516, Train: 0.9842, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 218, Loss: 0.4469, Train: 0.9800, Val: 0.9900, Test: 0.9733\n",
      "Epoch: 219, Loss: 0.4414, Train: 0.9867, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 220, Loss: 0.4593, Train: 0.9788, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 221, Loss: 0.4506, Train: 0.9804, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 222, Loss: 0.4458, Train: 0.9867, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 223, Loss: 0.4374, Train: 0.9892, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 224, Loss: 0.4599, Train: 0.9858, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 225, Loss: 0.4439, Train: 0.9796, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 226, Loss: 0.4452, Train: 0.9804, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 227, Loss: 0.4436, Train: 0.9829, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 228, Loss: 0.4423, Train: 0.9854, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 229, Loss: 0.4533, Train: 0.9862, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 230, Loss: 0.4429, Train: 0.9879, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 231, Loss: 0.4460, Train: 0.9900, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 232, Loss: 0.4394, Train: 0.9871, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 233, Loss: 0.4376, Train: 0.9854, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 234, Loss: 0.4459, Train: 0.9692, Val: 0.9833, Test: 0.9600\n",
      "Epoch: 235, Loss: 0.4542, Train: 0.9896, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 236, Loss: 0.4429, Train: 0.9879, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 237, Loss: 0.4464, Train: 0.9892, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 238, Loss: 0.4385, Train: 0.9883, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 239, Loss: 0.4372, Train: 0.9871, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 240, Loss: 0.4478, Train: 0.9267, Val: 0.9433, Test: 0.9067\n",
      "Epoch: 241, Loss: 0.4489, Train: 0.9817, Val: 0.9967, Test: 0.9700\n",
      "Epoch: 242, Loss: 0.4467, Train: 0.9779, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 243, Loss: 0.4386, Train: 0.9888, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 244, Loss: 0.4378, Train: 0.9875, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 245, Loss: 0.4496, Train: 0.9796, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 246, Loss: 0.4328, Train: 0.9883, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 247, Loss: 0.4450, Train: 0.9842, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 248, Loss: 0.4374, Train: 0.9817, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 249, Loss: 0.4785, Train: 0.9808, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 250, Loss: 0.4498, Train: 0.9875, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 251, Loss: 0.4380, Train: 0.9850, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 252, Loss: 0.4388, Train: 0.9888, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 253, Loss: 0.4358, Train: 0.9892, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 254, Loss: 0.4403, Train: 0.9871, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 255, Loss: 0.4367, Train: 0.9867, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 256, Loss: 0.4478, Train: 0.9896, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 257, Loss: 0.4419, Train: 0.9888, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 258, Loss: 0.4351, Train: 0.9896, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 259, Loss: 0.4384, Train: 0.9892, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 260, Loss: 0.4369, Train: 0.9888, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 261, Loss: 0.4427, Train: 0.9879, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 262, Loss: 0.4417, Train: 0.9792, Val: 0.9933, Test: 0.9700\n",
      "Epoch: 263, Loss: 0.4334, Train: 0.9875, Val: 1.0000, Test: 0.9833\n",
      "Epoch: 264, Loss: 0.4351, Train: 0.9788, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 265, Loss: 0.4361, Train: 0.9888, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 266, Loss: 0.4337, Train: 0.9900, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 267, Loss: 0.4408, Train: 0.9646, Val: 0.9800, Test: 0.9500\n",
      "Epoch: 268, Loss: 0.4469, Train: 0.9879, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 269, Loss: 0.4413, Train: 0.9892, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 270, Loss: 0.4338, Train: 0.9829, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 271, Loss: 0.4545, Train: 0.9858, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 272, Loss: 0.4356, Train: 0.9829, Val: 0.9900, Test: 0.9767\n",
      "Epoch: 273, Loss: 0.4375, Train: 0.9883, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 274, Loss: 0.4464, Train: 0.9900, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 275, Loss: 0.4313, Train: 0.9812, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 276, Loss: 0.4300, Train: 0.9879, Val: 1.0000, Test: 0.9800\n",
      "Epoch: 277, Loss: 0.4386, Train: 0.9896, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 278, Loss: 0.4431, Train: 0.9900, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 279, Loss: 0.4360, Train: 0.9871, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 280, Loss: 0.4434, Train: 0.9896, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 281, Loss: 0.4395, Train: 0.9821, Val: 0.9933, Test: 0.9633\n",
      "Epoch: 282, Loss: 0.4405, Train: 0.9858, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 283, Loss: 0.4344, Train: 0.9888, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 284, Loss: 0.4466, Train: 0.9896, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 285, Loss: 0.4297, Train: 0.9896, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 286, Loss: 0.4354, Train: 0.9900, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 287, Loss: 0.4436, Train: 0.9850, Val: 1.0000, Test: 0.9733\n",
      "Epoch: 288, Loss: 0.4389, Train: 0.9879, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 289, Loss: 0.4358, Train: 0.9862, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 290, Loss: 0.4339, Train: 0.9867, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 291, Loss: 0.4444, Train: 0.9858, Val: 0.9933, Test: 0.9733\n",
      "Epoch: 292, Loss: 0.4354, Train: 0.9838, Val: 0.9967, Test: 0.9700\n",
      "Epoch: 293, Loss: 0.4492, Train: 0.9883, Val: 1.0000, Test: 0.9767\n",
      "Epoch: 294, Loss: 0.4344, Train: 0.9862, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 295, Loss: 0.4538, Train: 0.9754, Val: 0.9900, Test: 0.9667\n",
      "Epoch: 296, Loss: 0.4423, Train: 0.9892, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 297, Loss: 0.4299, Train: 0.9821, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 298, Loss: 0.4361, Train: 0.9850, Val: 0.9933, Test: 0.9700\n",
      "Epoch: 299, Loss: 0.4443, Train: 0.9892, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 300, Loss: 0.4313, Train: 0.9892, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 301, Loss: 0.4362, Train: 0.9879, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 302, Loss: 0.4560, Train: 0.9904, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 303, Loss: 0.4395, Train: 0.9783, Val: 0.9933, Test: 0.9733\n",
      "Epoch: 304, Loss: 0.4589, Train: 0.9875, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 305, Loss: 0.4506, Train: 0.9892, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 306, Loss: 0.4380, Train: 0.9900, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 307, Loss: 0.4303, Train: 0.9900, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 308, Loss: 0.4282, Train: 0.9888, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 309, Loss: 0.4369, Train: 0.9842, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 310, Loss: 0.4455, Train: 0.9800, Val: 0.9967, Test: 0.9667\n",
      "Epoch: 311, Loss: 0.4257, Train: 0.9825, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 312, Loss: 0.4473, Train: 0.9858, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 313, Loss: 0.4485, Train: 0.9846, Val: 0.9933, Test: 0.9633\n",
      "Epoch: 314, Loss: 0.4322, Train: 0.9829, Val: 0.9933, Test: 0.9667\n",
      "Epoch: 315, Loss: 0.4266, Train: 0.9871, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 316, Loss: 0.4463, Train: 0.9900, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 317, Loss: 0.4336, Train: 0.9867, Val: 0.9933, Test: 0.9733\n",
      "Epoch: 318, Loss: 0.4410, Train: 0.9862, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 319, Loss: 0.4314, Train: 0.9888, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 320, Loss: 0.4449, Train: 0.9808, Val: 0.9967, Test: 0.9700\n",
      "Epoch: 321, Loss: 0.4360, Train: 0.9838, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 322, Loss: 0.4375, Train: 0.9912, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 323, Loss: 0.4399, Train: 0.9879, Val: 1.0000, Test: 0.9800\n",
      "Epoch: 324, Loss: 0.4348, Train: 0.9904, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 325, Loss: 0.4247, Train: 0.9912, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 326, Loss: 0.4377, Train: 0.9812, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 327, Loss: 0.4303, Train: 0.9883, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 328, Loss: 0.4402, Train: 0.9575, Val: 0.9500, Test: 0.9733\n",
      "Epoch: 329, Loss: 0.4454, Train: 0.9733, Val: 0.9767, Test: 0.9900\n",
      "Epoch: 330, Loss: 0.4345, Train: 0.9854, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 331, Loss: 0.4345, Train: 0.9917, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 332, Loss: 0.4328, Train: 0.9904, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 333, Loss: 0.4260, Train: 0.9788, Val: 0.9833, Test: 0.9600\n",
      "Epoch: 334, Loss: 0.4422, Train: 0.9900, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 335, Loss: 0.4486, Train: 0.9871, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 336, Loss: 0.4323, Train: 0.9912, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 337, Loss: 0.4423, Train: 0.9867, Val: 0.9833, Test: 1.0000\n",
      "Epoch: 338, Loss: 0.4343, Train: 0.9908, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 339, Loss: 0.4342, Train: 0.9917, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 340, Loss: 0.4289, Train: 0.9908, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 341, Loss: 0.4270, Train: 0.9883, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 342, Loss: 0.4405, Train: 0.9896, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 343, Loss: 0.4304, Train: 0.9900, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 344, Loss: 0.4374, Train: 0.9904, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 345, Loss: 0.4380, Train: 0.9904, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 346, Loss: 0.4413, Train: 0.9788, Val: 0.9933, Test: 0.9667\n",
      "Epoch: 347, Loss: 0.4439, Train: 0.9900, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 348, Loss: 0.4418, Train: 0.9908, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 349, Loss: 0.4328, Train: 0.9904, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 350, Loss: 0.4251, Train: 0.9862, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 351, Loss: 0.4294, Train: 0.9912, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 352, Loss: 0.4331, Train: 0.9908, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 353, Loss: 0.4337, Train: 0.9858, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 354, Loss: 0.4305, Train: 0.9900, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 355, Loss: 0.4349, Train: 0.9896, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 356, Loss: 0.4259, Train: 0.9808, Val: 0.9867, Test: 0.9767\n",
      "Epoch: 357, Loss: 0.4365, Train: 0.9908, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 358, Loss: 0.4300, Train: 0.9892, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 359, Loss: 0.4343, Train: 0.9908, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 360, Loss: 0.4277, Train: 0.9921, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 361, Loss: 0.4368, Train: 0.9875, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 362, Loss: 0.4365, Train: 0.9896, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 363, Loss: 0.4383, Train: 0.9900, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 364, Loss: 0.4477, Train: 0.9896, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 365, Loss: 0.4293, Train: 0.9871, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 366, Loss: 0.4269, Train: 0.9912, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 367, Loss: 0.4277, Train: 0.9908, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 368, Loss: 0.4305, Train: 0.9912, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 369, Loss: 0.4302, Train: 0.9912, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 370, Loss: 0.4260, Train: 0.9904, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 371, Loss: 0.4330, Train: 0.9871, Val: 0.9933, Test: 0.9700\n",
      "Epoch: 372, Loss: 0.4256, Train: 0.9896, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 373, Loss: 0.4301, Train: 0.9892, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 374, Loss: 0.4226, Train: 0.9838, Val: 0.9900, Test: 0.9700\n",
      "Epoch: 375, Loss: 0.4298, Train: 0.9933, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 376, Loss: 0.4306, Train: 0.9904, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 377, Loss: 0.4192, Train: 0.9904, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 378, Loss: 0.4314, Train: 0.9925, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 379, Loss: 0.4339, Train: 0.9912, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 380, Loss: 0.4412, Train: 0.9925, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 381, Loss: 0.4254, Train: 0.9821, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 382, Loss: 0.4226, Train: 0.9904, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 383, Loss: 0.4287, Train: 0.9867, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 384, Loss: 0.4392, Train: 0.9337, Val: 0.9633, Test: 0.9033\n",
      "Epoch: 385, Loss: 0.4490, Train: 0.9896, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 386, Loss: 0.4254, Train: 0.9838, Val: 0.9967, Test: 0.9600\n",
      "Epoch: 387, Loss: 0.4222, Train: 0.9917, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 388, Loss: 0.4234, Train: 0.9904, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 389, Loss: 0.4219, Train: 0.9908, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 390, Loss: 0.4331, Train: 0.9921, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 391, Loss: 0.4231, Train: 0.9908, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 392, Loss: 0.4213, Train: 0.9888, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 393, Loss: 0.4218, Train: 0.9904, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 394, Loss: 0.4403, Train: 0.9833, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 395, Loss: 0.4293, Train: 0.9900, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 396, Loss: 0.4465, Train: 0.9888, Val: 1.0000, Test: 0.9833\n",
      "Epoch: 397, Loss: 0.4236, Train: 0.9792, Val: 0.9967, Test: 0.9700\n",
      "Epoch: 398, Loss: 0.4377, Train: 0.9854, Val: 0.9967, Test: 0.9700\n",
      "Epoch: 399, Loss: 0.4258, Train: 0.9883, Val: 0.9967, Test: 0.9700\n",
      "Epoch: 400, Loss: 0.4309, Train: 0.9842, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 401, Loss: 0.4338, Train: 0.9917, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 402, Loss: 0.4243, Train: 0.9888, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 403, Loss: 0.4223, Train: 0.9912, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 404, Loss: 0.4178, Train: 0.9917, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 405, Loss: 0.4243, Train: 0.9829, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 406, Loss: 0.4265, Train: 0.9892, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 407, Loss: 0.4287, Train: 0.9912, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 408, Loss: 0.4290, Train: 0.9929, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 409, Loss: 0.4189, Train: 0.9904, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 410, Loss: 0.4322, Train: 0.9888, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 411, Loss: 0.4372, Train: 0.9929, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 412, Loss: 0.4219, Train: 0.9900, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 413, Loss: 0.4216, Train: 0.9888, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 414, Loss: 0.4253, Train: 0.9908, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 415, Loss: 0.4252, Train: 0.9933, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 416, Loss: 0.4177, Train: 0.9921, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 417, Loss: 0.4277, Train: 0.9921, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 418, Loss: 0.4247, Train: 0.9917, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 419, Loss: 0.4225, Train: 0.9904, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 420, Loss: 0.4237, Train: 0.9838, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 421, Loss: 0.4393, Train: 0.9917, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 422, Loss: 0.4200, Train: 0.9850, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 423, Loss: 0.4267, Train: 0.9838, Val: 0.9967, Test: 0.9700\n",
      "Epoch: 424, Loss: 0.4272, Train: 0.9925, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 425, Loss: 0.4179, Train: 0.9921, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 426, Loss: 0.4196, Train: 0.9912, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 427, Loss: 0.4272, Train: 0.9908, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 428, Loss: 0.4217, Train: 0.9921, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 429, Loss: 0.4281, Train: 0.9888, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 430, Loss: 0.4204, Train: 0.9900, Val: 1.0000, Test: 0.9800\n",
      "Epoch: 431, Loss: 0.4254, Train: 0.9917, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 432, Loss: 0.4281, Train: 0.9933, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 433, Loss: 0.4216, Train: 0.9933, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 434, Loss: 0.4168, Train: 0.9921, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 435, Loss: 0.4191, Train: 0.9862, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 436, Loss: 0.4404, Train: 0.9896, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 437, Loss: 0.4263, Train: 0.9671, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 438, Loss: 0.4248, Train: 0.9933, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 439, Loss: 0.4211, Train: 0.9883, Val: 0.9800, Test: 0.9967\n",
      "Epoch: 440, Loss: 0.4221, Train: 0.9933, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 441, Loss: 0.4234, Train: 0.9908, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 442, Loss: 0.4184, Train: 0.9929, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 443, Loss: 0.4195, Train: 0.9908, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 444, Loss: 0.4287, Train: 0.9725, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 445, Loss: 0.4291, Train: 0.9925, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 446, Loss: 0.4211, Train: 0.9925, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 447, Loss: 0.4160, Train: 0.9892, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 448, Loss: 0.4166, Train: 0.9900, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 449, Loss: 0.4263, Train: 0.9896, Val: 1.0000, Test: 0.9833\n",
      "Epoch: 450, Loss: 0.4200, Train: 0.9846, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 451, Loss: 0.4302, Train: 0.9908, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 452, Loss: 0.4409, Train: 0.9933, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 453, Loss: 0.4201, Train: 0.9812, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 454, Loss: 0.4154, Train: 0.9929, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 455, Loss: 0.4211, Train: 0.9921, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 456, Loss: 0.4178, Train: 0.9921, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 457, Loss: 0.4284, Train: 0.9871, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 458, Loss: 0.4428, Train: 0.9921, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 459, Loss: 0.4169, Train: 0.9925, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 460, Loss: 0.4110, Train: 0.9929, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 461, Loss: 0.4219, Train: 0.9921, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 462, Loss: 0.4138, Train: 0.9850, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 463, Loss: 0.4294, Train: 0.9908, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 464, Loss: 0.4284, Train: 0.9921, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 465, Loss: 0.4182, Train: 0.9842, Val: 0.9733, Test: 0.9900\n",
      "Epoch: 466, Loss: 0.4182, Train: 0.9929, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 467, Loss: 0.4172, Train: 0.9938, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 468, Loss: 0.4158, Train: 0.9929, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 469, Loss: 0.4205, Train: 0.9925, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 470, Loss: 0.4163, Train: 0.9933, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 471, Loss: 0.4167, Train: 0.9900, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 472, Loss: 0.4307, Train: 0.9900, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 473, Loss: 0.4168, Train: 0.9917, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 474, Loss: 0.4217, Train: 0.9946, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 475, Loss: 0.4141, Train: 0.9908, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 476, Loss: 0.4125, Train: 0.9938, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 477, Loss: 0.4253, Train: 0.9904, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 478, Loss: 0.4189, Train: 0.9917, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 479, Loss: 0.4309, Train: 0.9925, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 480, Loss: 0.4136, Train: 0.9921, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 481, Loss: 0.4151, Train: 0.9929, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 482, Loss: 0.4217, Train: 0.9883, Val: 0.9867, Test: 1.0000\n",
      "Epoch: 483, Loss: 0.4154, Train: 0.9875, Val: 0.9967, Test: 0.9767\n",
      "Epoch: 484, Loss: 0.4122, Train: 0.9842, Val: 0.9767, Test: 0.9900\n",
      "Epoch: 485, Loss: 0.4142, Train: 0.9946, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 486, Loss: 0.4284, Train: 0.9800, Val: 0.9933, Test: 0.9667\n",
      "Epoch: 487, Loss: 0.4264, Train: 0.9912, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 488, Loss: 0.4159, Train: 0.9925, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 489, Loss: 0.4137, Train: 0.9921, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 490, Loss: 0.4244, Train: 0.9929, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 491, Loss: 0.4143, Train: 0.9921, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 492, Loss: 0.4158, Train: 0.9912, Val: 1.0000, Test: 0.9867\n",
      "Epoch: 493, Loss: 0.4182, Train: 0.9929, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 494, Loss: 0.4204, Train: 0.9754, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 495, Loss: 0.4302, Train: 0.9942, Val: 1.0000, Test: 0.9900\n",
      "Epoch: 496, Loss: 0.4123, Train: 0.9917, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 497, Loss: 0.4241, Train: 0.9921, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 498, Loss: 0.4151, Train: 0.9871, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 499, Loss: 0.4181, Train: 0.9896, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 500, Loss: 0.4144, Train: 0.9933, Val: 1.0000, Test: 0.9900\n",
      "### Run 0 - val loss: 0.002, test acc: 0.987\n",
      "Accuracies in each run:  [0.9866666666666667]\n",
      "test acc - mean: 0.987, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'mincut'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmon\n",
      "Epoch: 001, Loss: 2.0819, Train: 0.6804, Val: 0.7167, Test: 0.6767\n",
      "Epoch: 002, Loss: 2.0178, Train: 0.7079, Val: 0.7000, Test: 0.6867\n",
      "Epoch: 003, Loss: 1.9866, Train: 0.5917, Val: 0.5933, Test: 0.5533\n",
      "Epoch: 004, Loss: 1.9342, Train: 0.7467, Val: 0.7033, Test: 0.7067\n",
      "Epoch: 005, Loss: 1.8908, Train: 0.7104, Val: 0.7167, Test: 0.7033\n",
      "Epoch: 006, Loss: 1.8614, Train: 0.8004, Val: 0.7967, Test: 0.7600\n",
      "Epoch: 007, Loss: 1.8344, Train: 0.6504, Val: 0.6633, Test: 0.6633\n",
      "Epoch: 008, Loss: 1.8108, Train: 0.7188, Val: 0.7033, Test: 0.6933\n",
      "Epoch: 009, Loss: 1.7972, Train: 0.7854, Val: 0.7800, Test: 0.7733\n",
      "Epoch: 010, Loss: 1.7777, Train: 0.7950, Val: 0.7867, Test: 0.7867\n",
      "Epoch: 011, Loss: 1.7669, Train: 0.8175, Val: 0.8200, Test: 0.8100\n",
      "Epoch: 012, Loss: 1.7795, Train: 0.7617, Val: 0.7533, Test: 0.7533\n",
      "Epoch: 013, Loss: 1.7434, Train: 0.7996, Val: 0.8033, Test: 0.7900\n",
      "Epoch: 014, Loss: 1.7549, Train: 0.8529, Val: 0.8567, Test: 0.8567\n",
      "Epoch: 015, Loss: 1.7523, Train: 0.8329, Val: 0.8267, Test: 0.8300\n",
      "Epoch: 016, Loss: 1.7306, Train: 0.8629, Val: 0.8500, Test: 0.8367\n",
      "Epoch: 017, Loss: 1.7362, Train: 0.8496, Val: 0.8367, Test: 0.8267\n",
      "Epoch: 018, Loss: 1.7177, Train: 0.8746, Val: 0.8767, Test: 0.8600\n",
      "Epoch: 019, Loss: 1.6993, Train: 0.8796, Val: 0.8833, Test: 0.8633\n",
      "Epoch: 020, Loss: 1.6879, Train: 0.8383, Val: 0.8267, Test: 0.8433\n",
      "Epoch: 021, Loss: 1.6721, Train: 0.8108, Val: 0.8100, Test: 0.8100\n",
      "Epoch: 022, Loss: 1.6891, Train: 0.8629, Val: 0.8767, Test: 0.8400\n",
      "Epoch: 023, Loss: 1.6791, Train: 0.8600, Val: 0.8733, Test: 0.8367\n",
      "Epoch: 024, Loss: 1.7232, Train: 0.8196, Val: 0.8000, Test: 0.8133\n",
      "Epoch: 025, Loss: 1.6938, Train: 0.8363, Val: 0.8433, Test: 0.8133\n",
      "Epoch: 026, Loss: 1.6780, Train: 0.8025, Val: 0.7767, Test: 0.7900\n",
      "Epoch: 027, Loss: 1.6591, Train: 0.8962, Val: 0.9033, Test: 0.8667\n",
      "Epoch: 028, Loss: 1.6565, Train: 0.7883, Val: 0.7700, Test: 0.7833\n",
      "Epoch: 029, Loss: 1.6692, Train: 0.8708, Val: 0.8600, Test: 0.8733\n",
      "Epoch: 030, Loss: 1.6355, Train: 0.8342, Val: 0.8167, Test: 0.8233\n",
      "Epoch: 031, Loss: 1.6555, Train: 0.8013, Val: 0.8333, Test: 0.7767\n",
      "Epoch: 032, Loss: 1.6471, Train: 0.8471, Val: 0.8467, Test: 0.8267\n",
      "Epoch: 033, Loss: 1.6605, Train: 0.8746, Val: 0.8900, Test: 0.8333\n",
      "Epoch: 034, Loss: 1.6534, Train: 0.8517, Val: 0.8233, Test: 0.8467\n",
      "Epoch: 035, Loss: 1.6486, Train: 0.8792, Val: 0.8700, Test: 0.8767\n",
      "Epoch: 036, Loss: 1.6339, Train: 0.8946, Val: 0.9067, Test: 0.8767\n",
      "Epoch: 037, Loss: 1.6216, Train: 0.8825, Val: 0.8767, Test: 0.8733\n",
      "Epoch: 038, Loss: 1.6127, Train: 0.8350, Val: 0.8167, Test: 0.8567\n",
      "Epoch: 039, Loss: 1.6334, Train: 0.8150, Val: 0.8033, Test: 0.8100\n",
      "Epoch: 040, Loss: 1.6190, Train: 0.8696, Val: 0.8433, Test: 0.8733\n",
      "Epoch: 041, Loss: 1.6306, Train: 0.8121, Val: 0.8200, Test: 0.8233\n",
      "Epoch: 042, Loss: 1.6414, Train: 0.8912, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 043, Loss: 1.6087, Train: 0.8458, Val: 0.8800, Test: 0.8200\n",
      "Epoch: 044, Loss: 1.6236, Train: 0.9087, Val: 0.9167, Test: 0.8900\n",
      "Epoch: 045, Loss: 1.6082, Train: 0.9121, Val: 0.9233, Test: 0.8967\n",
      "Epoch: 046, Loss: 1.5970, Train: 0.8200, Val: 0.8600, Test: 0.7933\n",
      "Epoch: 047, Loss: 1.6225, Train: 0.9083, Val: 0.9133, Test: 0.8700\n",
      "Epoch: 048, Loss: 1.6404, Train: 0.8363, Val: 0.8100, Test: 0.8400\n",
      "Epoch: 049, Loss: 1.6038, Train: 0.8488, Val: 0.8233, Test: 0.8667\n",
      "Epoch: 050, Loss: 1.5915, Train: 0.9038, Val: 0.9033, Test: 0.8867\n",
      "Epoch: 051, Loss: 1.5897, Train: 0.8137, Val: 0.8033, Test: 0.8100\n",
      "Epoch: 052, Loss: 1.6097, Train: 0.8942, Val: 0.8900, Test: 0.8800\n",
      "Epoch: 053, Loss: 1.5815, Train: 0.8354, Val: 0.8100, Test: 0.8467\n",
      "Epoch: 054, Loss: 1.5741, Train: 0.8912, Val: 0.9067, Test: 0.8667\n",
      "Epoch: 055, Loss: 1.6071, Train: 0.8833, Val: 0.9000, Test: 0.8533\n",
      "Epoch: 056, Loss: 1.5786, Train: 0.8633, Val: 0.8700, Test: 0.8267\n",
      "Epoch: 057, Loss: 1.5824, Train: 0.9187, Val: 0.9300, Test: 0.9100\n",
      "Epoch: 058, Loss: 1.5841, Train: 0.8821, Val: 0.8833, Test: 0.8400\n",
      "Epoch: 059, Loss: 1.6014, Train: 0.9071, Val: 0.9333, Test: 0.8667\n",
      "Epoch: 060, Loss: 1.5854, Train: 0.8083, Val: 0.8200, Test: 0.7800\n",
      "Epoch: 061, Loss: 1.5866, Train: 0.9175, Val: 0.9233, Test: 0.8900\n",
      "Epoch: 062, Loss: 1.5622, Train: 0.8875, Val: 0.8800, Test: 0.9033\n",
      "Epoch: 063, Loss: 1.5770, Train: 0.9254, Val: 0.9433, Test: 0.8733\n",
      "Epoch: 064, Loss: 1.5373, Train: 0.9146, Val: 0.9067, Test: 0.9067\n",
      "Epoch: 065, Loss: 1.5914, Train: 0.9275, Val: 0.9400, Test: 0.8967\n",
      "Epoch: 066, Loss: 1.5414, Train: 0.8996, Val: 0.8800, Test: 0.9100\n",
      "Epoch: 067, Loss: 1.5417, Train: 0.9167, Val: 0.9133, Test: 0.9167\n",
      "Epoch: 068, Loss: 1.5561, Train: 0.9313, Val: 0.9500, Test: 0.9067\n",
      "Epoch: 069, Loss: 1.5555, Train: 0.8521, Val: 0.8400, Test: 0.8600\n",
      "Epoch: 070, Loss: 1.5530, Train: 0.9250, Val: 0.9433, Test: 0.8967\n",
      "Epoch: 071, Loss: 1.5519, Train: 0.8742, Val: 0.8833, Test: 0.8533\n",
      "Epoch: 072, Loss: 1.5396, Train: 0.9017, Val: 0.9333, Test: 0.8567\n",
      "Epoch: 073, Loss: 1.5138, Train: 0.9287, Val: 0.9167, Test: 0.9300\n",
      "Epoch: 074, Loss: 1.5238, Train: 0.9367, Val: 0.9433, Test: 0.9233\n",
      "Epoch: 075, Loss: 1.5181, Train: 0.9333, Val: 0.9300, Test: 0.9167\n",
      "Epoch: 076, Loss: 1.5042, Train: 0.9233, Val: 0.9333, Test: 0.8933\n",
      "Epoch: 077, Loss: 1.5126, Train: 0.9333, Val: 0.9133, Test: 0.9467\n",
      "Epoch: 078, Loss: 1.5098, Train: 0.9192, Val: 0.9200, Test: 0.9000\n",
      "Epoch: 079, Loss: 1.5192, Train: 0.9471, Val: 0.9500, Test: 0.9267\n",
      "Epoch: 080, Loss: 1.5040, Train: 0.9471, Val: 0.9500, Test: 0.9067\n",
      "Epoch: 081, Loss: 1.4956, Train: 0.9442, Val: 0.9267, Test: 0.9367\n",
      "Epoch: 082, Loss: 1.4846, Train: 0.8300, Val: 0.8267, Test: 0.8367\n",
      "Epoch: 083, Loss: 1.4713, Train: 0.9546, Val: 0.9533, Test: 0.9167\n",
      "Epoch: 084, Loss: 1.4757, Train: 0.9417, Val: 0.9500, Test: 0.9367\n",
      "Epoch: 085, Loss: 1.4873, Train: 0.9329, Val: 0.9533, Test: 0.9100\n",
      "Epoch: 086, Loss: 1.4652, Train: 0.9458, Val: 0.9500, Test: 0.9133\n",
      "Epoch: 087, Loss: 1.4741, Train: 0.9471, Val: 0.9633, Test: 0.9333\n",
      "Epoch: 088, Loss: 1.4769, Train: 0.9583, Val: 0.9600, Test: 0.9367\n",
      "Epoch: 089, Loss: 1.4688, Train: 0.9542, Val: 0.9600, Test: 0.9267\n",
      "Epoch: 090, Loss: 1.4481, Train: 0.9117, Val: 0.9167, Test: 0.9267\n",
      "Epoch: 091, Loss: 1.4553, Train: 0.9113, Val: 0.8833, Test: 0.9367\n",
      "Epoch: 092, Loss: 1.4375, Train: 0.9429, Val: 0.9500, Test: 0.9133\n",
      "Epoch: 093, Loss: 1.4435, Train: 0.9646, Val: 0.9700, Test: 0.9500\n",
      "Epoch: 094, Loss: 1.4551, Train: 0.9642, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 095, Loss: 1.4324, Train: 0.8712, Val: 0.8600, Test: 0.9000\n",
      "Epoch: 096, Loss: 1.4526, Train: 0.9629, Val: 0.9600, Test: 0.9433\n",
      "Epoch: 097, Loss: 1.4571, Train: 0.9587, Val: 0.9433, Test: 0.9600\n",
      "Epoch: 098, Loss: 1.4230, Train: 0.9692, Val: 0.9633, Test: 0.9533\n",
      "Epoch: 099, Loss: 1.4482, Train: 0.9492, Val: 0.9433, Test: 0.9533\n",
      "Epoch: 100, Loss: 1.4225, Train: 0.9296, Val: 0.9267, Test: 0.9033\n",
      "Epoch: 101, Loss: 1.4311, Train: 0.9696, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 102, Loss: 1.4307, Train: 0.9708, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 103, Loss: 1.4332, Train: 0.9663, Val: 0.9633, Test: 0.9567\n",
      "Epoch: 104, Loss: 1.4145, Train: 0.9613, Val: 0.9600, Test: 0.9433\n",
      "Epoch: 105, Loss: 1.4361, Train: 0.9587, Val: 0.9467, Test: 0.9367\n",
      "Epoch: 106, Loss: 1.4235, Train: 0.9700, Val: 0.9700, Test: 0.9500\n",
      "Epoch: 107, Loss: 1.4397, Train: 0.9650, Val: 0.9567, Test: 0.9333\n",
      "Epoch: 108, Loss: 1.4183, Train: 0.9742, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 109, Loss: 1.4533, Train: 0.9637, Val: 0.9567, Test: 0.9467\n",
      "Epoch: 110, Loss: 1.4370, Train: 0.9717, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 111, Loss: 1.3965, Train: 0.9746, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 112, Loss: 1.4118, Train: 0.9600, Val: 0.9600, Test: 0.9267\n",
      "Epoch: 113, Loss: 1.4094, Train: 0.9663, Val: 0.9633, Test: 0.9367\n",
      "Epoch: 114, Loss: 1.3941, Train: 0.9563, Val: 0.9533, Test: 0.9533\n",
      "Epoch: 115, Loss: 1.4170, Train: 0.9775, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 116, Loss: 1.4144, Train: 0.9717, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 117, Loss: 1.4057, Train: 0.9404, Val: 0.9267, Test: 0.9133\n",
      "Epoch: 118, Loss: 1.4017, Train: 0.9621, Val: 0.9533, Test: 0.9400\n",
      "Epoch: 119, Loss: 1.4089, Train: 0.9029, Val: 0.8967, Test: 0.9233\n",
      "Epoch: 120, Loss: 1.4217, Train: 0.9804, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 121, Loss: 1.4882, Train: 0.9738, Val: 0.9633, Test: 0.9700\n",
      "Epoch: 122, Loss: 1.4105, Train: 0.9788, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 123, Loss: 1.3961, Train: 0.9413, Val: 0.9233, Test: 0.9067\n",
      "Epoch: 124, Loss: 1.4019, Train: 0.9550, Val: 0.9367, Test: 0.9567\n",
      "Epoch: 125, Loss: 1.4119, Train: 0.9812, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 126, Loss: 1.3900, Train: 0.9725, Val: 0.9667, Test: 0.9567\n",
      "Epoch: 127, Loss: 1.3843, Train: 0.9692, Val: 0.9533, Test: 0.9667\n",
      "Epoch: 128, Loss: 1.4097, Train: 0.9796, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 129, Loss: 1.3940, Train: 0.9817, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 130, Loss: 1.3791, Train: 0.9721, Val: 0.9667, Test: 0.9467\n",
      "Epoch: 131, Loss: 1.3955, Train: 0.9700, Val: 0.9567, Test: 0.9500\n",
      "Epoch: 132, Loss: 1.4228, Train: 0.9613, Val: 0.9433, Test: 0.9600\n",
      "Epoch: 133, Loss: 1.3767, Train: 0.9796, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 134, Loss: 1.4027, Train: 0.9150, Val: 0.9067, Test: 0.9300\n",
      "Epoch: 135, Loss: 1.3963, Train: 0.9692, Val: 0.9467, Test: 0.9633\n",
      "Epoch: 136, Loss: 1.3880, Train: 0.9850, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 137, Loss: 1.3924, Train: 0.9779, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 138, Loss: 1.3875, Train: 0.9817, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 139, Loss: 1.3876, Train: 0.9475, Val: 0.9400, Test: 0.9400\n",
      "Epoch: 140, Loss: 1.3954, Train: 0.9858, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 141, Loss: 1.3817, Train: 0.9783, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 142, Loss: 1.3842, Train: 0.9850, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 143, Loss: 1.3857, Train: 0.9796, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 144, Loss: 1.4000, Train: 0.9788, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 145, Loss: 1.3906, Train: 0.9762, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 146, Loss: 1.3844, Train: 0.9163, Val: 0.9133, Test: 0.9200\n",
      "Epoch: 147, Loss: 1.3813, Train: 0.9725, Val: 0.9667, Test: 0.9567\n",
      "Epoch: 148, Loss: 1.3695, Train: 0.9629, Val: 0.9433, Test: 0.9567\n",
      "Epoch: 149, Loss: 1.3811, Train: 0.9842, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 150, Loss: 1.3787, Train: 0.9663, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 151, Loss: 1.3660, Train: 0.9850, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 152, Loss: 1.3790, Train: 0.9850, Val: 0.9667, Test: 0.9800\n",
      "Epoch: 153, Loss: 1.3690, Train: 0.9746, Val: 0.9567, Test: 0.9667\n",
      "Epoch: 154, Loss: 1.3729, Train: 0.9463, Val: 0.9267, Test: 0.9433\n",
      "Epoch: 155, Loss: 1.4007, Train: 0.9408, Val: 0.9267, Test: 0.9100\n",
      "Epoch: 156, Loss: 1.3610, Train: 0.9679, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 157, Loss: 1.3552, Train: 0.8938, Val: 0.8867, Test: 0.9200\n",
      "Epoch: 158, Loss: 1.3709, Train: 0.9758, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 159, Loss: 1.3944, Train: 0.9825, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 160, Loss: 1.3888, Train: 0.9796, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 161, Loss: 1.3751, Train: 0.9862, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 162, Loss: 1.3541, Train: 0.9879, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 163, Loss: 1.3669, Train: 0.9846, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 164, Loss: 1.3758, Train: 0.9904, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 165, Loss: 1.3815, Train: 0.9862, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 166, Loss: 1.3626, Train: 0.9904, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 167, Loss: 1.3550, Train: 0.9608, Val: 0.9533, Test: 0.9433\n",
      "Epoch: 168, Loss: 1.3566, Train: 0.9733, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 169, Loss: 1.3541, Train: 0.9679, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 170, Loss: 1.3525, Train: 0.9854, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 171, Loss: 1.3587, Train: 0.9683, Val: 0.9600, Test: 0.9633\n",
      "Epoch: 172, Loss: 1.3696, Train: 0.9900, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 173, Loss: 1.3561, Train: 0.9475, Val: 0.9533, Test: 0.9400\n",
      "Epoch: 174, Loss: 1.3552, Train: 0.9908, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 175, Loss: 1.3508, Train: 0.9896, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 176, Loss: 1.3703, Train: 0.9875, Val: 0.9667, Test: 0.9833\n",
      "Epoch: 177, Loss: 1.3735, Train: 0.9800, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 178, Loss: 1.3698, Train: 0.9888, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 179, Loss: 1.3540, Train: 0.9883, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 180, Loss: 1.3413, Train: 0.9833, Val: 0.9667, Test: 0.9767\n",
      "Epoch: 181, Loss: 1.3673, Train: 0.9875, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 182, Loss: 1.3485, Train: 0.9471, Val: 0.9367, Test: 0.9267\n",
      "Epoch: 183, Loss: 1.3533, Train: 0.9883, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 184, Loss: 1.3723, Train: 0.9879, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 185, Loss: 1.3426, Train: 0.9917, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 186, Loss: 1.3564, Train: 0.9804, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 187, Loss: 1.3335, Train: 0.9904, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 188, Loss: 1.3470, Train: 0.9912, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 189, Loss: 1.3655, Train: 0.9838, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 190, Loss: 1.3468, Train: 0.9712, Val: 0.9500, Test: 0.9533\n",
      "Epoch: 191, Loss: 1.3489, Train: 0.9658, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 192, Loss: 1.3491, Train: 0.9812, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 193, Loss: 1.3797, Train: 0.9858, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 194, Loss: 1.3562, Train: 0.9912, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 195, Loss: 1.3506, Train: 0.9879, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 196, Loss: 1.3410, Train: 0.9858, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 197, Loss: 1.3353, Train: 0.9900, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 198, Loss: 1.3351, Train: 0.9912, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 199, Loss: 1.3449, Train: 0.9846, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 200, Loss: 1.3390, Train: 0.9875, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 201, Loss: 1.3402, Train: 0.9867, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 202, Loss: 1.3356, Train: 0.9896, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 203, Loss: 1.3520, Train: 0.9908, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 204, Loss: 1.3453, Train: 0.9533, Val: 0.9400, Test: 0.9567\n",
      "Epoch: 205, Loss: 1.3566, Train: 0.9883, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 206, Loss: 1.3579, Train: 0.9896, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 207, Loss: 1.3496, Train: 0.9900, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 208, Loss: 1.3388, Train: 0.9829, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 209, Loss: 1.3317, Train: 0.9904, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 210, Loss: 1.3243, Train: 0.9917, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 211, Loss: 1.3436, Train: 0.9904, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 212, Loss: 1.3435, Train: 0.9904, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 213, Loss: 1.3594, Train: 0.9917, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 214, Loss: 1.3273, Train: 0.9933, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 215, Loss: 1.3440, Train: 0.9917, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 216, Loss: 1.3696, Train: 0.9912, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 217, Loss: 1.3283, Train: 0.9800, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 218, Loss: 1.3386, Train: 0.9921, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 219, Loss: 1.3745, Train: 0.9867, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 220, Loss: 1.3460, Train: 0.9879, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 221, Loss: 1.3352, Train: 0.9900, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 222, Loss: 1.3255, Train: 0.9888, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 223, Loss: 1.3364, Train: 0.9871, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 224, Loss: 1.3312, Train: 0.9808, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 225, Loss: 1.3367, Train: 0.9933, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 226, Loss: 1.3670, Train: 0.9658, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 227, Loss: 1.3305, Train: 0.9925, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 228, Loss: 1.3284, Train: 0.9921, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 229, Loss: 1.3344, Train: 0.9929, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 230, Loss: 1.3355, Train: 0.9871, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 231, Loss: 1.3294, Train: 0.9904, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 232, Loss: 1.3188, Train: 0.9950, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 233, Loss: 1.3267, Train: 0.9896, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 234, Loss: 1.3440, Train: 0.9929, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 235, Loss: 1.3374, Train: 0.9367, Val: 0.9233, Test: 0.9433\n",
      "Epoch: 236, Loss: 1.3409, Train: 0.9938, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 237, Loss: 1.3251, Train: 0.9746, Val: 0.9600, Test: 0.9700\n",
      "Epoch: 238, Loss: 1.3418, Train: 0.9904, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 239, Loss: 1.3265, Train: 0.9883, Val: 0.9800, Test: 0.9700\n",
      "Epoch: 240, Loss: 1.3279, Train: 0.9883, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 241, Loss: 1.3321, Train: 0.9667, Val: 0.9600, Test: 0.9633\n",
      "Epoch: 242, Loss: 1.3235, Train: 0.9929, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 243, Loss: 1.3213, Train: 0.9817, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 244, Loss: 1.3235, Train: 0.9938, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 245, Loss: 1.3225, Train: 0.9929, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 246, Loss: 1.3221, Train: 0.9700, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 247, Loss: 1.3325, Train: 0.9829, Val: 0.9767, Test: 0.9667\n",
      "Epoch: 248, Loss: 1.3342, Train: 0.9779, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 249, Loss: 1.3348, Train: 0.9942, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 250, Loss: 1.3317, Train: 0.9921, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 251, Loss: 1.3200, Train: 0.9929, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 252, Loss: 1.3134, Train: 0.9858, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 253, Loss: 1.3263, Train: 0.9921, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 254, Loss: 1.3211, Train: 0.9942, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 255, Loss: 1.3199, Train: 0.9879, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 256, Loss: 1.3175, Train: 0.9904, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 257, Loss: 1.3334, Train: 0.9912, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 258, Loss: 1.3327, Train: 0.9900, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 259, Loss: 1.3313, Train: 0.9721, Val: 0.9700, Test: 0.9500\n",
      "Epoch: 260, Loss: 1.3216, Train: 0.9821, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 261, Loss: 1.3171, Train: 0.9950, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 262, Loss: 1.3150, Train: 0.9942, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 263, Loss: 1.3176, Train: 0.9925, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 264, Loss: 1.3213, Train: 0.9929, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 265, Loss: 1.3099, Train: 0.9933, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 266, Loss: 1.3160, Train: 0.9921, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 267, Loss: 1.3201, Train: 0.9921, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 268, Loss: 1.3198, Train: 0.9938, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 269, Loss: 1.3183, Train: 0.9908, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 270, Loss: 1.3204, Train: 0.9925, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 271, Loss: 1.3259, Train: 0.9829, Val: 0.9733, Test: 0.9567\n",
      "Epoch: 272, Loss: 1.3263, Train: 0.9921, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 273, Loss: 1.3148, Train: 0.9838, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 274, Loss: 1.3216, Train: 0.9900, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 275, Loss: 1.3428, Train: 0.9921, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 276, Loss: 1.3112, Train: 0.9854, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 277, Loss: 1.3131, Train: 0.9904, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 278, Loss: 1.3445, Train: 0.9938, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 279, Loss: 1.3188, Train: 0.9925, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 280, Loss: 1.3091, Train: 0.9921, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 281, Loss: 1.3309, Train: 0.9825, Val: 0.9767, Test: 0.9633\n",
      "Epoch: 282, Loss: 1.3252, Train: 0.9762, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 283, Loss: 1.3214, Train: 0.9929, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 284, Loss: 1.3186, Train: 0.9946, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 285, Loss: 1.3092, Train: 0.9912, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 286, Loss: 1.3053, Train: 0.9721, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 287, Loss: 1.3056, Train: 0.9929, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 288, Loss: 1.3124, Train: 0.9938, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 289, Loss: 1.3098, Train: 0.9958, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 290, Loss: 1.3156, Train: 0.9938, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 291, Loss: 1.2982, Train: 0.9938, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 292, Loss: 1.3067, Train: 0.9804, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 293, Loss: 1.3234, Train: 0.9875, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 294, Loss: 1.3042, Train: 0.9900, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 295, Loss: 1.3053, Train: 0.9929, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 296, Loss: 1.3147, Train: 0.9958, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 297, Loss: 1.3091, Train: 0.9904, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 298, Loss: 1.3150, Train: 0.9938, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 299, Loss: 1.3150, Train: 0.9933, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 300, Loss: 1.2997, Train: 0.9958, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 301, Loss: 1.3030, Train: 0.9950, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 302, Loss: 1.3061, Train: 0.9950, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 303, Loss: 1.3083, Train: 0.9938, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 304, Loss: 1.2999, Train: 0.9946, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 305, Loss: 1.3016, Train: 0.9929, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 306, Loss: 1.2965, Train: 0.9938, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 307, Loss: 1.3046, Train: 0.9954, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 308, Loss: 1.3067, Train: 0.9971, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 309, Loss: 1.3048, Train: 0.9946, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 310, Loss: 1.2969, Train: 0.9896, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 311, Loss: 1.2970, Train: 0.9971, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 312, Loss: 1.3104, Train: 0.9921, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 313, Loss: 1.2953, Train: 0.9929, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 314, Loss: 1.3059, Train: 0.9950, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 315, Loss: 1.3076, Train: 0.9925, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 316, Loss: 1.2985, Train: 0.9946, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 317, Loss: 1.3241, Train: 0.9971, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 318, Loss: 1.2933, Train: 0.9971, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 319, Loss: 1.2994, Train: 0.9925, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 320, Loss: 1.3020, Train: 0.9954, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 321, Loss: 1.2971, Train: 0.9946, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 322, Loss: 1.3006, Train: 0.9962, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 323, Loss: 1.2945, Train: 0.9971, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 324, Loss: 1.3010, Train: 0.9938, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 325, Loss: 1.2908, Train: 0.9862, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 326, Loss: 1.2934, Train: 0.9929, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 327, Loss: 1.2983, Train: 0.9933, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 328, Loss: 1.2931, Train: 0.9950, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 329, Loss: 1.3074, Train: 0.9962, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 330, Loss: 1.3279, Train: 0.9821, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 331, Loss: 1.2986, Train: 0.9942, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 332, Loss: 1.2987, Train: 0.9950, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 333, Loss: 1.2927, Train: 0.9938, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 334, Loss: 1.2935, Train: 0.9971, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 335, Loss: 1.2930, Train: 0.9954, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 336, Loss: 1.3028, Train: 0.9950, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 337, Loss: 1.2925, Train: 0.9954, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 338, Loss: 1.2943, Train: 0.9950, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 339, Loss: 1.2834, Train: 0.9962, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 340, Loss: 1.2882, Train: 0.9904, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 341, Loss: 1.2887, Train: 0.9975, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 342, Loss: 1.2818, Train: 0.9938, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 343, Loss: 1.2963, Train: 0.9946, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 344, Loss: 1.3131, Train: 0.9946, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 345, Loss: 1.2991, Train: 0.9929, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 346, Loss: 1.2916, Train: 0.9888, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 347, Loss: 1.2987, Train: 0.9767, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 348, Loss: 1.3030, Train: 0.9896, Val: 0.9900, Test: 0.9733\n",
      "Epoch: 349, Loss: 1.2925, Train: 0.9971, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 350, Loss: 1.2887, Train: 0.9954, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 351, Loss: 1.2866, Train: 0.9975, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 352, Loss: 1.2920, Train: 0.9938, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 353, Loss: 1.2870, Train: 0.9946, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 354, Loss: 1.2956, Train: 0.9971, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 355, Loss: 1.2914, Train: 0.9958, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 356, Loss: 1.2964, Train: 0.9971, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 357, Loss: 1.2887, Train: 0.9962, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 358, Loss: 1.2865, Train: 0.9971, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 359, Loss: 1.2931, Train: 0.9979, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 360, Loss: 1.2844, Train: 0.9971, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 361, Loss: 1.2957, Train: 0.9833, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 362, Loss: 1.2910, Train: 0.9967, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 363, Loss: 1.2834, Train: 0.9942, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 364, Loss: 1.2876, Train: 0.9954, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 365, Loss: 1.2912, Train: 0.9954, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 366, Loss: 1.2869, Train: 0.9942, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 367, Loss: 1.2998, Train: 0.9950, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 368, Loss: 1.2866, Train: 0.9979, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 369, Loss: 1.2907, Train: 0.9971, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 370, Loss: 1.2802, Train: 0.9971, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 371, Loss: 1.2829, Train: 0.9971, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 372, Loss: 1.2927, Train: 0.9967, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 373, Loss: 1.2970, Train: 0.9938, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 374, Loss: 1.2943, Train: 0.9904, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 375, Loss: 1.2870, Train: 0.9962, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 376, Loss: 1.2955, Train: 0.9954, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 377, Loss: 1.2807, Train: 0.9958, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 378, Loss: 1.2916, Train: 0.9979, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 379, Loss: 1.2856, Train: 0.9971, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 380, Loss: 1.2799, Train: 0.9971, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 381, Loss: 1.2830, Train: 0.9967, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 382, Loss: 1.2926, Train: 0.9954, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 383, Loss: 1.2857, Train: 0.9971, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 384, Loss: 1.2928, Train: 0.9896, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 385, Loss: 1.3011, Train: 0.9967, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 386, Loss: 1.2818, Train: 0.9971, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 387, Loss: 1.2829, Train: 0.9938, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 388, Loss: 1.2987, Train: 0.9925, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 389, Loss: 1.2912, Train: 0.9958, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 390, Loss: 1.2813, Train: 0.9971, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 391, Loss: 1.2779, Train: 0.9929, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 392, Loss: 1.2849, Train: 0.9975, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 393, Loss: 1.2889, Train: 0.9983, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 394, Loss: 1.2829, Train: 0.9983, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 395, Loss: 1.2754, Train: 0.9975, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 396, Loss: 1.2863, Train: 0.9917, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 397, Loss: 1.2859, Train: 0.9967, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 398, Loss: 1.2859, Train: 0.9950, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 399, Loss: 1.2962, Train: 0.9942, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 400, Loss: 1.2949, Train: 0.9975, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 401, Loss: 1.2773, Train: 0.9983, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 402, Loss: 1.2904, Train: 0.9950, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 403, Loss: 1.2781, Train: 0.9992, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 404, Loss: 1.2908, Train: 0.9971, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 405, Loss: 1.2745, Train: 0.9983, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 406, Loss: 1.3150, Train: 0.9967, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 407, Loss: 1.2788, Train: 0.9971, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 408, Loss: 1.2828, Train: 0.9983, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 409, Loss: 1.2813, Train: 0.9975, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 410, Loss: 1.2801, Train: 0.9888, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 411, Loss: 1.2921, Train: 0.9979, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 412, Loss: 1.2788, Train: 0.9950, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 413, Loss: 1.2802, Train: 0.9975, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 414, Loss: 1.2856, Train: 0.9954, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 415, Loss: 1.2810, Train: 0.9992, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 416, Loss: 1.2855, Train: 0.9971, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 417, Loss: 1.2831, Train: 0.9954, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 418, Loss: 1.2818, Train: 0.9983, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 419, Loss: 1.2822, Train: 0.9954, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 420, Loss: 1.2813, Train: 0.9938, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 421, Loss: 1.2806, Train: 0.9942, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 422, Loss: 1.2797, Train: 0.9983, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 423, Loss: 1.2724, Train: 0.9942, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 424, Loss: 1.2974, Train: 0.9954, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 425, Loss: 1.2793, Train: 0.9988, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 426, Loss: 1.2804, Train: 0.9975, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 427, Loss: 1.2739, Train: 0.9954, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 428, Loss: 1.2901, Train: 0.9954, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 429, Loss: 1.2920, Train: 0.9946, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 430, Loss: 1.2866, Train: 0.9988, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 431, Loss: 1.2794, Train: 0.9992, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 432, Loss: 1.2754, Train: 0.9988, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 433, Loss: 1.2771, Train: 0.9983, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 434, Loss: 1.2789, Train: 0.9954, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 435, Loss: 1.2790, Train: 0.9983, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 436, Loss: 1.2684, Train: 0.9979, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 437, Loss: 1.2809, Train: 0.9971, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 438, Loss: 1.2766, Train: 0.9988, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 439, Loss: 1.2799, Train: 0.9988, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 440, Loss: 1.2833, Train: 0.9983, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 441, Loss: 1.2830, Train: 0.9975, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 442, Loss: 1.2811, Train: 0.9962, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 443, Loss: 1.2729, Train: 0.9967, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 444, Loss: 1.2754, Train: 0.9988, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 445, Loss: 1.2774, Train: 0.9983, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 446, Loss: 1.2726, Train: 0.9988, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 447, Loss: 1.2826, Train: 0.9983, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 448, Loss: 1.2958, Train: 0.9962, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 449, Loss: 1.2782, Train: 0.9992, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 450, Loss: 1.2790, Train: 0.9979, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 451, Loss: 1.2815, Train: 0.9988, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 452, Loss: 1.2717, Train: 0.9996, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 453, Loss: 1.2759, Train: 0.9988, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 454, Loss: 1.2762, Train: 0.9942, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 455, Loss: 1.2740, Train: 0.9979, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 456, Loss: 1.2725, Train: 0.9992, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 457, Loss: 1.2885, Train: 0.9967, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 458, Loss: 1.2793, Train: 0.9983, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 459, Loss: 1.2779, Train: 0.9967, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 460, Loss: 1.2791, Train: 0.9988, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 461, Loss: 1.2676, Train: 0.9992, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 462, Loss: 1.2858, Train: 0.9983, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 463, Loss: 1.2733, Train: 0.9975, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 464, Loss: 1.2760, Train: 0.9879, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 465, Loss: 1.2776, Train: 0.9996, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 466, Loss: 1.2804, Train: 0.9983, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 467, Loss: 1.2753, Train: 0.9992, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 468, Loss: 1.2730, Train: 0.9971, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 469, Loss: 1.2773, Train: 0.9992, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 470, Loss: 1.2813, Train: 0.9979, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 471, Loss: 1.2713, Train: 0.9992, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 472, Loss: 1.2750, Train: 0.9988, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 473, Loss: 1.2908, Train: 0.9946, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 474, Loss: 1.2743, Train: 0.9983, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 475, Loss: 1.2727, Train: 0.9996, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 476, Loss: 1.2755, Train: 0.9908, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 477, Loss: 1.2723, Train: 0.9958, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 478, Loss: 1.2712, Train: 0.9917, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 479, Loss: 1.2752, Train: 0.9983, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 480, Loss: 1.2765, Train: 0.9967, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 481, Loss: 1.2944, Train: 0.9967, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 482, Loss: 1.2705, Train: 0.9992, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 483, Loss: 1.2624, Train: 0.9996, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 484, Loss: 1.2779, Train: 0.9983, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 485, Loss: 1.2736, Train: 0.9996, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 486, Loss: 1.2830, Train: 0.9979, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 487, Loss: 1.2728, Train: 0.9979, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 488, Loss: 1.2751, Train: 0.9979, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 489, Loss: 1.2784, Train: 0.9946, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 490, Loss: 1.2811, Train: 0.9988, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 491, Loss: 1.2788, Train: 0.9983, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 492, Loss: 1.2665, Train: 0.9975, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 493, Loss: 1.2694, Train: 0.9996, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 494, Loss: 1.2713, Train: 0.9979, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 495, Loss: 1.2785, Train: 0.9996, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 496, Loss: 1.2705, Train: 0.9992, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 497, Loss: 1.2786, Train: 0.9958, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 498, Loss: 1.2816, Train: 0.9933, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 499, Loss: 1.2721, Train: 0.9983, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 500, Loss: 1.2829, Train: 0.9875, Val: 0.9733, Test: 0.9733\n",
      "### Run 0 - val loss: 0.023, test acc: 0.983\n",
      "Accuracies in each run:  [0.9833333333333333]\n",
      "test acc - mean: 0.983, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'dmon'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edgepool\n",
      "Epoch: 001, Loss: 0.7675, Train: 0.7150, Val: 0.7033, Test: 0.7167\n",
      "Epoch: 002, Loss: 0.5985, Train: 0.8029, Val: 0.7567, Test: 0.7667\n",
      "Epoch: 003, Loss: 0.4961, Train: 0.8525, Val: 0.8133, Test: 0.8233\n",
      "Epoch: 004, Loss: 0.4324, Train: 0.8488, Val: 0.8333, Test: 0.8767\n",
      "Epoch: 005, Loss: 0.3683, Train: 0.8400, Val: 0.8367, Test: 0.8367\n",
      "Epoch: 006, Loss: 0.3411, Train: 0.8683, Val: 0.8467, Test: 0.8867\n",
      "Epoch: 007, Loss: 0.3242, Train: 0.9125, Val: 0.8767, Test: 0.8800\n",
      "Epoch: 008, Loss: 0.2928, Train: 0.9042, Val: 0.8967, Test: 0.9033\n",
      "Epoch: 009, Loss: 0.2644, Train: 0.9125, Val: 0.8967, Test: 0.9233\n",
      "Epoch: 010, Loss: 0.2529, Train: 0.9263, Val: 0.8867, Test: 0.8933\n",
      "Epoch: 011, Loss: 0.2446, Train: 0.9300, Val: 0.9000, Test: 0.9100\n",
      "Epoch: 012, Loss: 0.2594, Train: 0.9342, Val: 0.9167, Test: 0.9267\n",
      "Epoch: 013, Loss: 0.2156, Train: 0.8975, Val: 0.8900, Test: 0.9333\n",
      "Epoch: 014, Loss: 0.2129, Train: 0.8458, Val: 0.8933, Test: 0.8700\n",
      "Epoch: 015, Loss: 0.2023, Train: 0.9267, Val: 0.9133, Test: 0.9367\n",
      "Epoch: 016, Loss: 0.2127, Train: 0.9254, Val: 0.8767, Test: 0.9000\n",
      "Epoch: 017, Loss: 0.1835, Train: 0.9171, Val: 0.9300, Test: 0.9267\n",
      "Epoch: 018, Loss: 0.1985, Train: 0.8483, Val: 0.8300, Test: 0.8067\n",
      "Epoch: 019, Loss: 0.1584, Train: 0.9567, Val: 0.9367, Test: 0.9567\n",
      "Epoch: 020, Loss: 0.1522, Train: 0.9563, Val: 0.9367, Test: 0.9400\n",
      "Epoch: 021, Loss: 0.1622, Train: 0.9363, Val: 0.9200, Test: 0.9433\n",
      "Epoch: 022, Loss: 0.1607, Train: 0.9637, Val: 0.9433, Test: 0.9567\n",
      "Epoch: 023, Loss: 0.1379, Train: 0.9625, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 024, Loss: 0.1511, Train: 0.9596, Val: 0.9400, Test: 0.9433\n",
      "Epoch: 025, Loss: 0.1420, Train: 0.9617, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 026, Loss: 0.1189, Train: 0.9554, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 027, Loss: 0.1314, Train: 0.9646, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 028, Loss: 0.1280, Train: 0.9671, Val: 0.9633, Test: 0.9533\n",
      "Epoch: 029, Loss: 0.1071, Train: 0.9529, Val: 0.9400, Test: 0.9233\n",
      "Epoch: 030, Loss: 0.1227, Train: 0.9446, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 031, Loss: 0.1503, Train: 0.9642, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 032, Loss: 0.1161, Train: 0.9692, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 033, Loss: 0.1061, Train: 0.9771, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 034, Loss: 0.1127, Train: 0.9542, Val: 0.9367, Test: 0.9367\n",
      "Epoch: 035, Loss: 0.1168, Train: 0.9788, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 036, Loss: 0.0928, Train: 0.9654, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 037, Loss: 0.1003, Train: 0.9804, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 038, Loss: 0.1072, Train: 0.9400, Val: 0.9600, Test: 0.9533\n",
      "Epoch: 039, Loss: 0.0743, Train: 0.9871, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 040, Loss: 0.0785, Train: 0.9692, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 041, Loss: 0.1046, Train: 0.9821, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 042, Loss: 0.1006, Train: 0.9817, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 043, Loss: 0.0732, Train: 0.9625, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 044, Loss: 0.0834, Train: 0.9767, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 045, Loss: 0.0934, Train: 0.9721, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 046, Loss: 0.0968, Train: 0.9738, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 047, Loss: 0.0706, Train: 0.9721, Val: 0.9633, Test: 0.9500\n",
      "Epoch: 048, Loss: 0.0770, Train: 0.9675, Val: 0.9833, Test: 0.9700\n",
      "Epoch: 049, Loss: 0.0584, Train: 0.9683, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 050, Loss: 0.0982, Train: 0.9846, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 051, Loss: 0.0889, Train: 0.9883, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 052, Loss: 0.0580, Train: 0.9679, Val: 0.9767, Test: 0.9667\n",
      "Epoch: 053, Loss: 0.0507, Train: 0.9817, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 054, Loss: 0.0537, Train: 0.9754, Val: 0.9833, Test: 0.9667\n",
      "Epoch: 055, Loss: 0.0592, Train: 0.9667, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 056, Loss: 0.0599, Train: 0.9871, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 057, Loss: 0.0663, Train: 0.9838, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 058, Loss: 0.0516, Train: 0.9900, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 059, Loss: 0.0816, Train: 0.9658, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 060, Loss: 0.0620, Train: 0.9846, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 061, Loss: 0.0689, Train: 0.9904, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 062, Loss: 0.0671, Train: 0.9833, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 063, Loss: 0.0513, Train: 0.9912, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 064, Loss: 0.0525, Train: 0.9896, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 065, Loss: 0.0611, Train: 0.9871, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 066, Loss: 0.0479, Train: 0.9858, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 067, Loss: 0.0418, Train: 0.9925, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 068, Loss: 0.0371, Train: 0.9717, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 069, Loss: 0.0523, Train: 0.9817, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 070, Loss: 0.0398, Train: 0.9896, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 071, Loss: 0.0437, Train: 0.9925, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 072, Loss: 0.0528, Train: 0.9858, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 073, Loss: 0.0434, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 074, Loss: 0.0442, Train: 0.9917, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 075, Loss: 0.0453, Train: 0.9754, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 076, Loss: 0.0547, Train: 0.9904, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 077, Loss: 0.0567, Train: 0.9879, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 078, Loss: 0.0608, Train: 0.9921, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 079, Loss: 0.0389, Train: 0.9942, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 080, Loss: 0.0363, Train: 0.9938, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 081, Loss: 0.0321, Train: 0.9933, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 082, Loss: 0.0396, Train: 0.9921, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 083, Loss: 0.0323, Train: 0.9875, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 084, Loss: 0.0405, Train: 0.9883, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 085, Loss: 0.0333, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 086, Loss: 0.0630, Train: 0.9888, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 087, Loss: 0.0366, Train: 0.9879, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 088, Loss: 0.0382, Train: 0.9929, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 089, Loss: 0.0347, Train: 0.9942, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 090, Loss: 0.0321, Train: 0.9938, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 091, Loss: 0.0377, Train: 0.9871, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 092, Loss: 0.0371, Train: 0.9912, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 093, Loss: 0.0482, Train: 0.9871, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 094, Loss: 0.0351, Train: 0.9904, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 095, Loss: 0.0402, Train: 0.9817, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 096, Loss: 0.0330, Train: 0.9829, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 097, Loss: 0.0265, Train: 0.9892, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 098, Loss: 0.0344, Train: 0.9525, Val: 0.9533, Test: 0.9367\n",
      "Epoch: 099, Loss: 0.0371, Train: 0.9608, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 100, Loss: 0.0264, Train: 0.9942, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 101, Loss: 0.0247, Train: 0.9946, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 102, Loss: 0.0297, Train: 0.9925, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 103, Loss: 0.0304, Train: 0.9958, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 104, Loss: 0.0378, Train: 0.9817, Val: 0.9867, Test: 0.9667\n",
      "Epoch: 105, Loss: 0.0392, Train: 0.9829, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 106, Loss: 0.0393, Train: 0.9867, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 107, Loss: 0.0271, Train: 0.9908, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 108, Loss: 0.0276, Train: 0.9725, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 109, Loss: 0.0283, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 110, Loss: 0.0172, Train: 0.9950, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 111, Loss: 0.0376, Train: 0.9912, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 112, Loss: 0.0410, Train: 0.9942, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 113, Loss: 0.0364, Train: 0.9950, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 114, Loss: 0.0414, Train: 0.9958, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 115, Loss: 0.0282, Train: 0.9971, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 116, Loss: 0.0287, Train: 0.9921, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 117, Loss: 0.0283, Train: 0.9946, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 118, Loss: 0.0256, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 119, Loss: 0.0312, Train: 0.9962, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 120, Loss: 0.0339, Train: 0.9917, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 121, Loss: 0.0329, Train: 0.9900, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 122, Loss: 0.0337, Train: 0.9950, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 123, Loss: 0.0253, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 124, Loss: 0.0196, Train: 0.9958, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 125, Loss: 0.0264, Train: 0.9975, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 126, Loss: 0.0347, Train: 0.9675, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 127, Loss: 0.0221, Train: 0.9954, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 128, Loss: 0.0598, Train: 0.9904, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 129, Loss: 0.0197, Train: 0.9917, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 130, Loss: 0.0251, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 131, Loss: 0.0202, Train: 0.9942, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 132, Loss: 0.0183, Train: 0.9933, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 133, Loss: 0.0264, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 134, Loss: 0.0234, Train: 0.9942, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 135, Loss: 0.0243, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 136, Loss: 0.0282, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 137, Loss: 0.0179, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 138, Loss: 0.0238, Train: 0.9962, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 139, Loss: 0.0195, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 140, Loss: 0.0430, Train: 0.9925, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 141, Loss: 0.0215, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 142, Loss: 0.0185, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 143, Loss: 0.0210, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 144, Loss: 0.0219, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 145, Loss: 0.0264, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 146, Loss: 0.0502, Train: 0.9742, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 147, Loss: 0.0280, Train: 0.9904, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 148, Loss: 0.0229, Train: 0.9942, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 149, Loss: 0.0211, Train: 0.9979, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 150, Loss: 0.0170, Train: 0.9883, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 151, Loss: 0.0270, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 152, Loss: 0.0167, Train: 0.9938, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 153, Loss: 0.0270, Train: 0.9950, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 154, Loss: 0.0169, Train: 0.9979, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 155, Loss: 0.0191, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 156, Loss: 0.0228, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 157, Loss: 0.0181, Train: 0.9946, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 158, Loss: 0.0220, Train: 0.9938, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 159, Loss: 0.0182, Train: 0.9958, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 160, Loss: 0.0217, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 161, Loss: 0.0172, Train: 0.9971, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 162, Loss: 0.0172, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 163, Loss: 0.0303, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 164, Loss: 0.0187, Train: 0.9967, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 165, Loss: 0.0169, Train: 0.9958, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 166, Loss: 0.0149, Train: 0.9946, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 167, Loss: 0.0229, Train: 0.9942, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 168, Loss: 0.0222, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 169, Loss: 0.0184, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 170, Loss: 0.0260, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 171, Loss: 0.0304, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 172, Loss: 0.0146, Train: 0.9800, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 173, Loss: 0.0225, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 174, Loss: 0.0217, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 175, Loss: 0.0240, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 176, Loss: 0.0170, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 177, Loss: 0.0167, Train: 0.9925, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 178, Loss: 0.0140, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 179, Loss: 0.0130, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 180, Loss: 0.0223, Train: 0.9942, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 181, Loss: 0.0140, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 182, Loss: 0.0225, Train: 0.9958, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 183, Loss: 0.0199, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 184, Loss: 0.0170, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 185, Loss: 0.0199, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 186, Loss: 0.0174, Train: 0.9908, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 187, Loss: 0.0232, Train: 0.9892, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 188, Loss: 0.0145, Train: 0.9933, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 189, Loss: 0.0192, Train: 0.9979, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 190, Loss: 0.0177, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 191, Loss: 0.0149, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 192, Loss: 0.0219, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 193, Loss: 0.0184, Train: 0.9950, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 194, Loss: 0.0183, Train: 0.9904, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 195, Loss: 0.0164, Train: 0.9983, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 196, Loss: 0.0095, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 197, Loss: 0.0206, Train: 0.9550, Val: 0.9433, Test: 0.9333\n",
      "Epoch: 198, Loss: 0.0422, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 199, Loss: 0.0183, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 200, Loss: 0.0113, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 201, Loss: 0.0171, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 202, Loss: 0.0158, Train: 0.9954, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 203, Loss: 0.0294, Train: 0.9975, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 204, Loss: 0.0115, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 205, Loss: 0.0087, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 206, Loss: 0.0113, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 207, Loss: 0.0150, Train: 0.9962, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 208, Loss: 0.0147, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 209, Loss: 0.0186, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 210, Loss: 0.0155, Train: 0.9967, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 211, Loss: 0.0212, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 212, Loss: 0.0250, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 213, Loss: 0.0206, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 214, Loss: 0.0126, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 215, Loss: 0.0155, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 216, Loss: 0.0162, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 217, Loss: 0.0137, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 218, Loss: 0.0100, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 219, Loss: 0.0129, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 220, Loss: 0.0115, Train: 0.9925, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 221, Loss: 0.0212, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 222, Loss: 0.0071, Train: 0.9946, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 223, Loss: 0.0114, Train: 0.9958, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 224, Loss: 0.0144, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 225, Loss: 0.0201, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 226, Loss: 0.0247, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 227, Loss: 0.0137, Train: 0.9971, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 228, Loss: 0.0119, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 229, Loss: 0.0088, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 230, Loss: 0.0238, Train: 0.9879, Val: 0.9900, Test: 0.9700\n",
      "Epoch: 231, Loss: 0.0175, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 232, Loss: 0.0085, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 233, Loss: 0.0280, Train: 0.9900, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 234, Loss: 0.0310, Train: 0.9929, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 235, Loss: 0.0180, Train: 0.9933, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 236, Loss: 0.0147, Train: 0.9954, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 237, Loss: 0.0107, Train: 0.9950, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 238, Loss: 0.0166, Train: 0.9958, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 239, Loss: 0.0087, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 240, Loss: 0.0130, Train: 0.9988, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 241, Loss: 0.0112, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 242, Loss: 0.0130, Train: 0.9929, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 243, Loss: 0.0219, Train: 0.9921, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 244, Loss: 0.0129, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 245, Loss: 0.0093, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 246, Loss: 0.0071, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 247, Loss: 0.0128, Train: 0.9992, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 248, Loss: 0.0106, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 249, Loss: 0.0075, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 250, Loss: 0.0126, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 251, Loss: 0.0161, Train: 0.9921, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 252, Loss: 0.0139, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 253, Loss: 0.0097, Train: 0.9925, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 254, Loss: 0.0100, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 255, Loss: 0.0100, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 256, Loss: 0.0083, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 257, Loss: 0.0198, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 258, Loss: 0.0106, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 259, Loss: 0.0158, Train: 0.9958, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 260, Loss: 0.0088, Train: 0.9971, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 261, Loss: 0.0220, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 262, Loss: 0.0205, Train: 0.9883, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 263, Loss: 0.0124, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 264, Loss: 0.0070, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 265, Loss: 0.0135, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 266, Loss: 0.0145, Train: 0.9954, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 267, Loss: 0.0169, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 268, Loss: 0.0101, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 269, Loss: 0.0126, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 270, Loss: 0.0121, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 271, Loss: 0.0084, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 272, Loss: 0.0053, Train: 0.9962, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 273, Loss: 0.0083, Train: 0.9988, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 274, Loss: 0.0097, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 275, Loss: 0.0115, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 276, Loss: 0.0112, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 277, Loss: 0.0083, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 278, Loss: 0.0089, Train: 0.9962, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 279, Loss: 0.0145, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 280, Loss: 0.0180, Train: 0.9979, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 281, Loss: 0.0194, Train: 0.9954, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 282, Loss: 0.0104, Train: 0.9938, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 283, Loss: 0.0092, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 284, Loss: 0.0122, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 285, Loss: 0.0060, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 286, Loss: 0.0105, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 287, Loss: 0.0083, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 288, Loss: 0.0072, Train: 0.9929, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 289, Loss: 0.0237, Train: 0.9750, Val: 0.9767, Test: 0.9667\n",
      "Epoch: 290, Loss: 0.0240, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 291, Loss: 0.0083, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 292, Loss: 0.0155, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 293, Loss: 0.0073, Train: 0.9950, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 294, Loss: 0.0109, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 295, Loss: 0.0112, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 296, Loss: 0.0119, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 297, Loss: 0.0169, Train: 0.9950, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 298, Loss: 0.0140, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 299, Loss: 0.0183, Train: 0.9708, Val: 0.9667, Test: 0.9767\n",
      "Epoch: 300, Loss: 0.0288, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 301, Loss: 0.0128, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 302, Loss: 0.0079, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 303, Loss: 0.0078, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 304, Loss: 0.0091, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 305, Loss: 0.0050, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 306, Loss: 0.0087, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 307, Loss: 0.0100, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 308, Loss: 0.0070, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 309, Loss: 0.0106, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 310, Loss: 0.0083, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 311, Loss: 0.0067, Train: 0.9783, Val: 0.9867, Test: 0.9700\n",
      "Epoch: 312, Loss: 0.0215, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 313, Loss: 0.0132, Train: 0.9971, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 314, Loss: 0.0079, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 315, Loss: 0.0131, Train: 0.9921, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 316, Loss: 0.0102, Train: 0.9992, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 317, Loss: 0.0104, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 318, Loss: 0.0074, Train: 0.9979, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 319, Loss: 0.0068, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 320, Loss: 0.0073, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 321, Loss: 0.0142, Train: 0.9958, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 322, Loss: 0.0108, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 323, Loss: 0.0082, Train: 0.9862, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 324, Loss: 0.0089, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 325, Loss: 0.0055, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 326, Loss: 0.0086, Train: 0.9975, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 327, Loss: 0.0044, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 328, Loss: 0.0188, Train: 0.9954, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 329, Loss: 0.0076, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 330, Loss: 0.0089, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 331, Loss: 0.0058, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 332, Loss: 0.0113, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 333, Loss: 0.0071, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 334, Loss: 0.0098, Train: 0.9996, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 335, Loss: 0.0162, Train: 0.9967, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 336, Loss: 0.0212, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 337, Loss: 0.0085, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 338, Loss: 0.0082, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 339, Loss: 0.0066, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 340, Loss: 0.0092, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 341, Loss: 0.0171, Train: 0.9904, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 342, Loss: 0.0151, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 343, Loss: 0.0040, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 344, Loss: 0.0081, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 345, Loss: 0.0062, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 346, Loss: 0.0054, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 347, Loss: 0.0056, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 348, Loss: 0.0046, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 349, Loss: 0.0122, Train: 0.9912, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 350, Loss: 0.0076, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 351, Loss: 0.0046, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 352, Loss: 0.0123, Train: 0.9962, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 353, Loss: 0.0120, Train: 0.9967, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 354, Loss: 0.0062, Train: 0.9983, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 355, Loss: 0.0092, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 356, Loss: 0.0124, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 357, Loss: 0.0111, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 358, Loss: 0.0085, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 359, Loss: 0.0095, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 360, Loss: 0.0069, Train: 0.9983, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 361, Loss: 0.0073, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 362, Loss: 0.0070, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 363, Loss: 0.0083, Train: 0.9979, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 364, Loss: 0.0083, Train: 0.9996, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 365, Loss: 0.0070, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 366, Loss: 0.0245, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 367, Loss: 0.0105, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 368, Loss: 0.0070, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 369, Loss: 0.0057, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 370, Loss: 0.0125, Train: 0.9975, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 371, Loss: 0.0092, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 372, Loss: 0.0073, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 373, Loss: 0.0049, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 374, Loss: 0.0139, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 375, Loss: 0.0093, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 376, Loss: 0.0066, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 377, Loss: 0.0039, Train: 0.9992, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 378, Loss: 0.0036, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 379, Loss: 0.0032, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 380, Loss: 0.0083, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 381, Loss: 0.0092, Train: 0.9983, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 382, Loss: 0.0077, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 383, Loss: 0.0097, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 384, Loss: 0.0184, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 385, Loss: 0.0081, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 386, Loss: 0.0056, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 387, Loss: 0.0045, Train: 0.9983, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 388, Loss: 0.0064, Train: 0.9975, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 389, Loss: 0.0037, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 390, Loss: 0.0056, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 391, Loss: 0.0118, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 392, Loss: 0.0107, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 393, Loss: 0.0065, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 394, Loss: 0.0103, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 395, Loss: 0.0072, Train: 0.9958, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 396, Loss: 0.0113, Train: 0.9983, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 397, Loss: 0.0096, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 398, Loss: 0.0160, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 399, Loss: 0.0063, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 400, Loss: 0.0058, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 401, Loss: 0.0115, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 402, Loss: 0.0122, Train: 0.9992, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 403, Loss: 0.0088, Train: 0.9983, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 404, Loss: 0.0046, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 405, Loss: 0.0108, Train: 0.9962, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 406, Loss: 0.0098, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 407, Loss: 0.0091, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 408, Loss: 0.0048, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 409, Loss: 0.0055, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 410, Loss: 0.0072, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 411, Loss: 0.0047, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 412, Loss: 0.0054, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 413, Loss: 0.0069, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 414, Loss: 0.0189, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 415, Loss: 0.0100, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 416, Loss: 0.0060, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 417, Loss: 0.0063, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 418, Loss: 0.0074, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 419, Loss: 0.0084, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 420, Loss: 0.0097, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 421, Loss: 0.0064, Train: 0.9946, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 422, Loss: 0.0182, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 423, Loss: 0.0066, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 424, Loss: 0.0056, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 425, Loss: 0.0065, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 426, Loss: 0.0067, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 427, Loss: 0.0087, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 428, Loss: 0.0064, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 429, Loss: 0.0048, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 430, Loss: 0.0095, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 431, Loss: 0.0157, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 432, Loss: 0.0120, Train: 0.9971, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 433, Loss: 0.0055, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 434, Loss: 0.0051, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 435, Loss: 0.0061, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 436, Loss: 0.0036, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 437, Loss: 0.0058, Train: 0.9962, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 438, Loss: 0.0056, Train: 0.9967, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 439, Loss: 0.0041, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 440, Loss: 0.0061, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 441, Loss: 0.0126, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 442, Loss: 0.0075, Train: 0.9979, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 443, Loss: 0.0078, Train: 0.9983, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 444, Loss: 0.0040, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 445, Loss: 0.0109, Train: 0.9992, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 446, Loss: 0.0069, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 447, Loss: 0.0054, Train: 0.9992, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 448, Loss: 0.0046, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 449, Loss: 0.0096, Train: 0.9983, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 450, Loss: 0.0136, Train: 0.9983, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 451, Loss: 0.0076, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 452, Loss: 0.0056, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 453, Loss: 0.0044, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 454, Loss: 0.0091, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 455, Loss: 0.0166, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 456, Loss: 0.0056, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 457, Loss: 0.0085, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 458, Loss: 0.0066, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 459, Loss: 0.0041, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 460, Loss: 0.0057, Train: 0.9988, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 461, Loss: 0.0110, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 462, Loss: 0.0066, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 463, Loss: 0.0133, Train: 0.9942, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 464, Loss: 0.0097, Train: 0.9704, Val: 0.9800, Test: 0.9633\n",
      "Epoch: 465, Loss: 0.0123, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 466, Loss: 0.0038, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 467, Loss: 0.0085, Train: 0.9975, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 468, Loss: 0.0055, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 469, Loss: 0.0056, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 470, Loss: 0.0036, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 471, Loss: 0.0046, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 472, Loss: 0.0093, Train: 0.9988, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 473, Loss: 0.0053, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 474, Loss: 0.0043, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 475, Loss: 0.0068, Train: 0.9988, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 476, Loss: 0.0073, Train: 0.9983, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 477, Loss: 0.0076, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 478, Loss: 0.0100, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 479, Loss: 0.0065, Train: 0.9946, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 480, Loss: 0.0153, Train: 0.9979, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 481, Loss: 0.0066, Train: 0.9954, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 482, Loss: 0.0116, Train: 0.9988, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 483, Loss: 0.0113, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 484, Loss: 0.0061, Train: 0.9988, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 485, Loss: 0.0064, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 486, Loss: 0.0062, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 487, Loss: 0.0061, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 488, Loss: 0.0071, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 489, Loss: 0.0031, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 490, Loss: 0.0047, Train: 0.9992, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 491, Loss: 0.0040, Train: 0.9992, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 492, Loss: 0.0079, Train: 0.9779, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 493, Loss: 0.0189, Train: 0.9917, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 494, Loss: 0.0094, Train: 0.9979, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 495, Loss: 0.0065, Train: 0.9996, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 496, Loss: 0.0043, Train: 0.9988, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 497, Loss: 0.0053, Train: 0.9996, Val: 0.9900, Test: 1.0000\n",
      "Epoch: 498, Loss: 0.0034, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 499, Loss: 0.0026, Train: 0.9996, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 500, Loss: 0.0032, Train: 0.9992, Val: 0.9900, Test: 0.9967\n",
      "### Run 0 - val loss: 0.021, test acc: 0.993\n",
      "Accuracies in each run:  [0.9933333333333333]\n",
      "test acc - mean: 0.993, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'edgepool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graclus\n",
      "Epoch: 001, Loss: 0.8587, Train: 0.5725, Val: 0.5167, Test: 0.5267\n",
      "Epoch: 002, Loss: 0.6111, Train: 0.7217, Val: 0.7267, Test: 0.7600\n",
      "Epoch: 003, Loss: 0.5578, Train: 0.7650, Val: 0.7267, Test: 0.7400\n",
      "Epoch: 004, Loss: 0.4936, Train: 0.6833, Val: 0.6200, Test: 0.6467\n",
      "Epoch: 005, Loss: 0.4261, Train: 0.8567, Val: 0.8800, Test: 0.8733\n",
      "Epoch: 006, Loss: 0.3772, Train: 0.7646, Val: 0.7133, Test: 0.7467\n",
      "Epoch: 007, Loss: 0.3534, Train: 0.7913, Val: 0.8100, Test: 0.8233\n",
      "Epoch: 008, Loss: 0.3314, Train: 0.8771, Val: 0.8133, Test: 0.8833\n",
      "Epoch: 009, Loss: 0.3215, Train: 0.8983, Val: 0.9133, Test: 0.9233\n",
      "Epoch: 010, Loss: 0.3043, Train: 0.8954, Val: 0.8233, Test: 0.8833\n",
      "Epoch: 011, Loss: 0.2973, Train: 0.8825, Val: 0.8933, Test: 0.9100\n",
      "Epoch: 012, Loss: 0.2881, Train: 0.9171, Val: 0.9100, Test: 0.9367\n",
      "Epoch: 013, Loss: 0.2472, Train: 0.9292, Val: 0.9100, Test: 0.9267\n",
      "Epoch: 014, Loss: 0.2363, Train: 0.8883, Val: 0.8333, Test: 0.8967\n",
      "Epoch: 015, Loss: 0.2263, Train: 0.9279, Val: 0.9400, Test: 0.9433\n",
      "Epoch: 016, Loss: 0.2360, Train: 0.9012, Val: 0.9133, Test: 0.9267\n",
      "Epoch: 017, Loss: 0.1937, Train: 0.9263, Val: 0.8800, Test: 0.9167\n",
      "Epoch: 018, Loss: 0.1999, Train: 0.9458, Val: 0.9233, Test: 0.9467\n",
      "Epoch: 019, Loss: 0.2224, Train: 0.9208, Val: 0.8933, Test: 0.9267\n",
      "Epoch: 020, Loss: 0.1884, Train: 0.9400, Val: 0.9233, Test: 0.9533\n",
      "Epoch: 021, Loss: 0.1678, Train: 0.9213, Val: 0.9300, Test: 0.9233\n",
      "Epoch: 022, Loss: 0.1813, Train: 0.9554, Val: 0.9467, Test: 0.9700\n",
      "Epoch: 023, Loss: 0.1704, Train: 0.9179, Val: 0.9333, Test: 0.9300\n",
      "Epoch: 024, Loss: 0.1680, Train: 0.9200, Val: 0.8667, Test: 0.9367\n",
      "Epoch: 025, Loss: 0.1739, Train: 0.9542, Val: 0.9667, Test: 0.9400\n",
      "Epoch: 026, Loss: 0.1644, Train: 0.9504, Val: 0.9633, Test: 0.9700\n",
      "Epoch: 027, Loss: 0.1376, Train: 0.9642, Val: 0.9567, Test: 0.9667\n",
      "Epoch: 028, Loss: 0.1341, Train: 0.9654, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 029, Loss: 0.1407, Train: 0.9121, Val: 0.8767, Test: 0.9133\n",
      "Epoch: 030, Loss: 0.1351, Train: 0.9292, Val: 0.8700, Test: 0.9167\n",
      "Epoch: 031, Loss: 0.1206, Train: 0.9525, Val: 0.9600, Test: 0.9733\n",
      "Epoch: 032, Loss: 0.1268, Train: 0.9533, Val: 0.9100, Test: 0.9633\n",
      "Epoch: 033, Loss: 0.1161, Train: 0.9667, Val: 0.9400, Test: 0.9633\n",
      "Epoch: 034, Loss: 0.1236, Train: 0.9617, Val: 0.9433, Test: 0.9567\n",
      "Epoch: 035, Loss: 0.1312, Train: 0.9721, Val: 0.9600, Test: 0.9733\n",
      "Epoch: 036, Loss: 0.1232, Train: 0.9650, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 037, Loss: 0.1158, Train: 0.9613, Val: 0.9567, Test: 0.9667\n",
      "Epoch: 038, Loss: 0.1143, Train: 0.9450, Val: 0.9200, Test: 0.9367\n",
      "Epoch: 039, Loss: 0.1255, Train: 0.9487, Val: 0.9267, Test: 0.9533\n",
      "Epoch: 040, Loss: 0.1206, Train: 0.9529, Val: 0.9333, Test: 0.9367\n",
      "Epoch: 041, Loss: 0.1093, Train: 0.9717, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 042, Loss: 0.1198, Train: 0.9750, Val: 0.9367, Test: 0.9667\n",
      "Epoch: 043, Loss: 0.1074, Train: 0.9708, Val: 0.9500, Test: 0.9700\n",
      "Epoch: 044, Loss: 0.1155, Train: 0.9558, Val: 0.9600, Test: 0.9733\n",
      "Epoch: 045, Loss: 0.0922, Train: 0.9742, Val: 0.9500, Test: 0.9767\n",
      "Epoch: 046, Loss: 0.1081, Train: 0.9571, Val: 0.9600, Test: 0.9467\n",
      "Epoch: 047, Loss: 0.1100, Train: 0.9621, Val: 0.9367, Test: 0.9600\n",
      "Epoch: 048, Loss: 0.1036, Train: 0.9762, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 049, Loss: 0.0933, Train: 0.9688, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 050, Loss: 0.0819, Train: 0.9525, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 051, Loss: 0.0965, Train: 0.8954, Val: 0.9333, Test: 0.9000\n",
      "Epoch: 052, Loss: 0.0866, Train: 0.9608, Val: 0.9367, Test: 0.9433\n",
      "Epoch: 053, Loss: 0.0799, Train: 0.9729, Val: 0.9500, Test: 0.9700\n",
      "Epoch: 054, Loss: 0.0779, Train: 0.9471, Val: 0.9200, Test: 0.9467\n",
      "Epoch: 055, Loss: 0.0856, Train: 0.9592, Val: 0.9167, Test: 0.9500\n",
      "Epoch: 056, Loss: 0.0713, Train: 0.9721, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 057, Loss: 0.0708, Train: 0.9708, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 058, Loss: 0.0814, Train: 0.9696, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 059, Loss: 0.0859, Train: 0.9758, Val: 0.9500, Test: 0.9933\n",
      "Epoch: 060, Loss: 0.0975, Train: 0.9637, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 061, Loss: 0.1000, Train: 0.9642, Val: 0.9267, Test: 0.9667\n",
      "Epoch: 062, Loss: 0.0769, Train: 0.9758, Val: 0.9800, Test: 0.9733\n",
      "Epoch: 063, Loss: 0.0915, Train: 0.9783, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 064, Loss: 0.0756, Train: 0.9550, Val: 0.9333, Test: 0.9467\n",
      "Epoch: 065, Loss: 0.0749, Train: 0.9792, Val: 0.9667, Test: 0.9767\n",
      "Epoch: 066, Loss: 0.0811, Train: 0.9392, Val: 0.9400, Test: 0.9500\n",
      "Epoch: 067, Loss: 0.0760, Train: 0.9796, Val: 0.9567, Test: 0.9667\n",
      "Epoch: 068, Loss: 0.0818, Train: 0.9804, Val: 0.9667, Test: 0.9800\n",
      "Epoch: 069, Loss: 0.0611, Train: 0.9637, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 070, Loss: 0.0646, Train: 0.9842, Val: 0.9667, Test: 0.9833\n",
      "Epoch: 071, Loss: 0.0569, Train: 0.9871, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 072, Loss: 0.0969, Train: 0.9496, Val: 0.9667, Test: 0.9533\n",
      "Epoch: 073, Loss: 0.0830, Train: 0.9667, Val: 0.9300, Test: 0.9633\n",
      "Epoch: 074, Loss: 0.0617, Train: 0.9829, Val: 0.9733, Test: 0.9867\n",
      "Epoch: 075, Loss: 0.0524, Train: 0.9854, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 076, Loss: 0.0578, Train: 0.9796, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 077, Loss: 0.0615, Train: 0.9833, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 078, Loss: 0.0498, Train: 0.9842, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 079, Loss: 0.0591, Train: 0.9867, Val: 0.9633, Test: 0.9833\n",
      "Epoch: 080, Loss: 0.0568, Train: 0.9896, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 081, Loss: 0.0582, Train: 0.9796, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 082, Loss: 0.0623, Train: 0.9479, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 083, Loss: 0.0772, Train: 0.9817, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 084, Loss: 0.0556, Train: 0.9875, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 085, Loss: 0.0522, Train: 0.9900, Val: 0.9700, Test: 0.9900\n",
      "Epoch: 086, Loss: 0.0511, Train: 0.9829, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 087, Loss: 0.0859, Train: 0.9846, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 088, Loss: 0.0564, Train: 0.9883, Val: 0.9833, Test: 0.9800\n",
      "Epoch: 089, Loss: 0.0533, Train: 0.9754, Val: 0.9433, Test: 0.9633\n",
      "Epoch: 090, Loss: 0.0529, Train: 0.9858, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 091, Loss: 0.0534, Train: 0.9821, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 092, Loss: 0.0503, Train: 0.9633, Val: 0.9733, Test: 0.9567\n",
      "Epoch: 093, Loss: 0.0587, Train: 0.9883, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 094, Loss: 0.0496, Train: 0.9808, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 095, Loss: 0.0527, Train: 0.9862, Val: 0.9633, Test: 0.9767\n",
      "Epoch: 096, Loss: 0.0435, Train: 0.9879, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 097, Loss: 0.0551, Train: 0.9729, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 098, Loss: 0.0814, Train: 0.9867, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 099, Loss: 0.0463, Train: 0.9921, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 100, Loss: 0.0344, Train: 0.9942, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 101, Loss: 0.0461, Train: 0.9896, Val: 0.9733, Test: 0.9867\n",
      "Epoch: 102, Loss: 0.0671, Train: 0.9875, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 103, Loss: 0.0466, Train: 0.9883, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 104, Loss: 0.0535, Train: 0.9904, Val: 0.9733, Test: 0.9867\n",
      "Epoch: 105, Loss: 0.0544, Train: 0.9629, Val: 0.9333, Test: 0.9500\n",
      "Epoch: 106, Loss: 0.0360, Train: 0.9754, Val: 0.9567, Test: 0.9733\n",
      "Epoch: 107, Loss: 0.0560, Train: 0.9879, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 108, Loss: 0.0538, Train: 0.9904, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 109, Loss: 0.0424, Train: 0.9842, Val: 0.9600, Test: 0.9800\n",
      "Epoch: 110, Loss: 0.0403, Train: 0.9867, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 111, Loss: 0.0423, Train: 0.9896, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 112, Loss: 0.0462, Train: 0.9683, Val: 0.9467, Test: 0.9533\n",
      "Epoch: 113, Loss: 0.0566, Train: 0.9938, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 114, Loss: 0.0389, Train: 0.9929, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 115, Loss: 0.0333, Train: 0.9858, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 116, Loss: 0.0428, Train: 0.9917, Val: 0.9800, Test: 0.9967\n",
      "Epoch: 117, Loss: 0.0431, Train: 0.9938, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 118, Loss: 0.0471, Train: 0.9842, Val: 0.9633, Test: 0.9800\n",
      "Epoch: 119, Loss: 0.0433, Train: 0.9821, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 120, Loss: 0.0587, Train: 0.9883, Val: 0.9700, Test: 0.9900\n",
      "Epoch: 121, Loss: 0.0387, Train: 0.9804, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 122, Loss: 0.0442, Train: 0.9846, Val: 0.9600, Test: 0.9733\n",
      "Epoch: 123, Loss: 0.0492, Train: 0.9754, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 124, Loss: 0.0413, Train: 0.9921, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 125, Loss: 0.0337, Train: 0.9921, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 126, Loss: 0.0424, Train: 0.9892, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 127, Loss: 0.0434, Train: 0.9929, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 128, Loss: 0.0361, Train: 0.9929, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 129, Loss: 0.0464, Train: 0.9846, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 130, Loss: 0.0444, Train: 0.9933, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 131, Loss: 0.0422, Train: 0.9904, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 132, Loss: 0.0368, Train: 0.9892, Val: 0.9667, Test: 0.9833\n",
      "Epoch: 133, Loss: 0.0357, Train: 0.9929, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 134, Loss: 0.0287, Train: 0.9896, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 135, Loss: 0.0289, Train: 0.9929, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 136, Loss: 0.0467, Train: 0.9929, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 137, Loss: 0.0285, Train: 0.9921, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 138, Loss: 0.0360, Train: 0.9862, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 139, Loss: 0.0385, Train: 0.9933, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 140, Loss: 0.0398, Train: 0.9800, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 141, Loss: 0.0309, Train: 0.9883, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 142, Loss: 0.0324, Train: 0.9929, Val: 0.9733, Test: 0.9867\n",
      "Epoch: 143, Loss: 0.0354, Train: 0.9800, Val: 0.9800, Test: 0.9833\n",
      "Epoch: 144, Loss: 0.0321, Train: 0.9912, Val: 0.9767, Test: 0.9800\n",
      "Epoch: 145, Loss: 0.0333, Train: 0.9912, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 146, Loss: 0.0370, Train: 0.9938, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 147, Loss: 0.0408, Train: 0.9938, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 148, Loss: 0.0348, Train: 0.9921, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 149, Loss: 0.0338, Train: 0.9921, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 150, Loss: 0.0275, Train: 0.9929, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 151, Loss: 0.0293, Train: 0.9912, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 152, Loss: 0.0368, Train: 0.9929, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 153, Loss: 0.0294, Train: 0.9838, Val: 0.9767, Test: 0.9900\n",
      "Epoch: 154, Loss: 0.0324, Train: 0.9933, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 155, Loss: 0.0307, Train: 0.9954, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 156, Loss: 0.0460, Train: 0.9862, Val: 0.9733, Test: 0.9933\n",
      "Epoch: 157, Loss: 0.0182, Train: 0.9929, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 158, Loss: 0.0244, Train: 0.9954, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 159, Loss: 0.0319, Train: 0.9921, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 160, Loss: 0.0303, Train: 0.9917, Val: 0.9767, Test: 0.9933\n",
      "Epoch: 161, Loss: 0.0334, Train: 0.9925, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 162, Loss: 0.0272, Train: 0.9933, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 163, Loss: 0.0291, Train: 0.9933, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 164, Loss: 0.0271, Train: 0.9904, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 165, Loss: 0.0216, Train: 0.9929, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 166, Loss: 0.0419, Train: 0.9933, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 167, Loss: 0.0370, Train: 0.9829, Val: 0.9667, Test: 0.9800\n",
      "Epoch: 168, Loss: 0.0397, Train: 0.9908, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 169, Loss: 0.0310, Train: 0.9812, Val: 0.9667, Test: 0.9833\n",
      "Epoch: 170, Loss: 0.0511, Train: 0.9896, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 171, Loss: 0.0215, Train: 0.9925, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 172, Loss: 0.0324, Train: 0.9883, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 173, Loss: 0.0195, Train: 0.9938, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 174, Loss: 0.0311, Train: 0.9938, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 175, Loss: 0.0351, Train: 0.9958, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 176, Loss: 0.0237, Train: 0.9954, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 177, Loss: 0.0230, Train: 0.9938, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 178, Loss: 0.0271, Train: 0.9925, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 179, Loss: 0.0319, Train: 0.9912, Val: 0.9767, Test: 0.9900\n",
      "Epoch: 180, Loss: 0.0347, Train: 0.9967, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 181, Loss: 0.0267, Train: 0.9933, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 182, Loss: 0.0308, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 183, Loss: 0.0202, Train: 0.9929, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 184, Loss: 0.0236, Train: 0.9958, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 185, Loss: 0.0312, Train: 0.9929, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 186, Loss: 0.0195, Train: 0.9933, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 187, Loss: 0.0200, Train: 0.9954, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 188, Loss: 0.0244, Train: 0.9888, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 189, Loss: 0.0260, Train: 0.9950, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 190, Loss: 0.0265, Train: 0.9933, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 191, Loss: 0.0279, Train: 0.9950, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 192, Loss: 0.0203, Train: 0.9946, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 193, Loss: 0.0306, Train: 0.9912, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 194, Loss: 0.0205, Train: 0.9933, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 195, Loss: 0.0205, Train: 0.9958, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 196, Loss: 0.0314, Train: 0.9950, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 197, Loss: 0.0392, Train: 0.9958, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 198, Loss: 0.0228, Train: 0.9958, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 199, Loss: 0.0228, Train: 0.9933, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 200, Loss: 0.0278, Train: 0.9925, Val: 0.9767, Test: 0.9900\n",
      "Epoch: 201, Loss: 0.0278, Train: 0.9912, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 202, Loss: 0.0252, Train: 0.9904, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 203, Loss: 0.0250, Train: 0.9954, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 204, Loss: 0.0206, Train: 0.9962, Val: 0.9767, Test: 0.9933\n",
      "Epoch: 205, Loss: 0.0235, Train: 0.9900, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 206, Loss: 0.0275, Train: 0.9942, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 207, Loss: 0.0318, Train: 0.9921, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 208, Loss: 0.0286, Train: 0.9954, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 209, Loss: 0.0190, Train: 0.9888, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 210, Loss: 0.0268, Train: 0.9938, Val: 0.9733, Test: 0.9933\n",
      "Epoch: 211, Loss: 0.0212, Train: 0.9962, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 212, Loss: 0.0250, Train: 0.9925, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 213, Loss: 0.0188, Train: 0.9921, Val: 0.9767, Test: 0.9933\n",
      "Epoch: 214, Loss: 0.0342, Train: 0.9925, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 215, Loss: 0.0141, Train: 0.9954, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 216, Loss: 0.0194, Train: 0.9942, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 217, Loss: 0.0236, Train: 0.9950, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 218, Loss: 0.0191, Train: 0.9946, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 219, Loss: 0.0234, Train: 0.9925, Val: 0.9767, Test: 0.9900\n",
      "Epoch: 220, Loss: 0.0278, Train: 0.9896, Val: 0.9767, Test: 0.9933\n",
      "Epoch: 221, Loss: 0.0205, Train: 0.9942, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 222, Loss: 0.0194, Train: 0.9954, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 223, Loss: 0.0167, Train: 0.9979, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 224, Loss: 0.0251, Train: 0.9942, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 225, Loss: 0.0303, Train: 0.9758, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 226, Loss: 0.0229, Train: 0.9954, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 227, Loss: 0.0208, Train: 0.9929, Val: 0.9800, Test: 0.9900\n",
      "Epoch: 228, Loss: 0.0168, Train: 0.9958, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 229, Loss: 0.0207, Train: 0.9933, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 230, Loss: 0.0147, Train: 0.9938, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 231, Loss: 0.0195, Train: 0.9946, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 232, Loss: 0.0226, Train: 0.9946, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 233, Loss: 0.0214, Train: 0.9954, Val: 0.9800, Test: 0.9967\n",
      "Epoch: 234, Loss: 0.0188, Train: 0.9933, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 235, Loss: 0.0232, Train: 0.9967, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 236, Loss: 0.0249, Train: 0.9925, Val: 0.9767, Test: 0.9933\n",
      "Epoch: 237, Loss: 0.0194, Train: 0.9967, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 238, Loss: 0.0199, Train: 0.9921, Val: 0.9733, Test: 0.9867\n",
      "Epoch: 239, Loss: 0.0223, Train: 0.9967, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 240, Loss: 0.0197, Train: 0.9958, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 241, Loss: 0.0181, Train: 0.9971, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 242, Loss: 0.0143, Train: 0.9950, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 243, Loss: 0.0146, Train: 0.9975, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 244, Loss: 0.0255, Train: 0.9950, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 245, Loss: 0.0190, Train: 0.9967, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 246, Loss: 0.0181, Train: 0.9958, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 247, Loss: 0.0190, Train: 0.9962, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 248, Loss: 0.0166, Train: 0.9967, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 249, Loss: 0.0194, Train: 0.9946, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 250, Loss: 0.0283, Train: 0.9900, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 251, Loss: 0.0233, Train: 0.9929, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 252, Loss: 0.0157, Train: 0.9962, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 253, Loss: 0.0274, Train: 0.9958, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 254, Loss: 0.0167, Train: 0.9975, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 255, Loss: 0.0240, Train: 0.9962, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 256, Loss: 0.0208, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 257, Loss: 0.0147, Train: 0.9892, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 258, Loss: 0.0173, Train: 0.9950, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 259, Loss: 0.0163, Train: 0.9958, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 260, Loss: 0.0190, Train: 0.9933, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 261, Loss: 0.0149, Train: 0.9958, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 262, Loss: 0.0258, Train: 0.9958, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 263, Loss: 0.0197, Train: 0.9958, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 264, Loss: 0.0154, Train: 0.9983, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 265, Loss: 0.0183, Train: 0.9850, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 266, Loss: 0.0170, Train: 0.9938, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 267, Loss: 0.0229, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 268, Loss: 0.0177, Train: 0.9929, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 269, Loss: 0.0220, Train: 0.9962, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 270, Loss: 0.0155, Train: 0.9975, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 271, Loss: 0.0135, Train: 0.9967, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 272, Loss: 0.0201, Train: 0.9962, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 273, Loss: 0.0182, Train: 0.9958, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 274, Loss: 0.0181, Train: 0.9954, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 275, Loss: 0.0205, Train: 0.9967, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 276, Loss: 0.0174, Train: 0.9954, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 277, Loss: 0.0254, Train: 0.9958, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 278, Loss: 0.0200, Train: 0.9971, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 279, Loss: 0.0115, Train: 0.9950, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 280, Loss: 0.0131, Train: 0.9958, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 281, Loss: 0.0154, Train: 0.9983, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 282, Loss: 0.0212, Train: 0.9962, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 283, Loss: 0.0160, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 284, Loss: 0.0186, Train: 0.9979, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 285, Loss: 0.0103, Train: 0.9988, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 286, Loss: 0.0158, Train: 0.9954, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 287, Loss: 0.0221, Train: 0.9971, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 288, Loss: 0.0259, Train: 0.9904, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 289, Loss: 0.0227, Train: 0.9942, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 290, Loss: 0.0180, Train: 0.9962, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 291, Loss: 0.0135, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 292, Loss: 0.0226, Train: 0.9962, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 293, Loss: 0.0135, Train: 0.9958, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 294, Loss: 0.0199, Train: 0.9979, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 295, Loss: 0.0206, Train: 0.9967, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 296, Loss: 0.0268, Train: 0.9967, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 297, Loss: 0.0228, Train: 0.9962, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 298, Loss: 0.0180, Train: 0.9971, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 299, Loss: 0.0140, Train: 0.9958, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 300, Loss: 0.0146, Train: 0.9967, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 301, Loss: 0.0162, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 302, Loss: 0.0139, Train: 0.9958, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 303, Loss: 0.0185, Train: 0.9962, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 304, Loss: 0.0152, Train: 0.9967, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 305, Loss: 0.0463, Train: 0.9942, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 306, Loss: 0.0190, Train: 0.9942, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 307, Loss: 0.0204, Train: 0.9975, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 308, Loss: 0.0232, Train: 0.9979, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 309, Loss: 0.0161, Train: 0.9983, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 310, Loss: 0.0147, Train: 0.9958, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 311, Loss: 0.0122, Train: 0.9975, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 312, Loss: 0.0179, Train: 0.9967, Val: 0.9867, Test: 0.9900\n",
      "Epoch: 313, Loss: 0.0201, Train: 0.9988, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 314, Loss: 0.0137, Train: 0.9983, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 315, Loss: 0.0213, Train: 0.9950, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 316, Loss: 0.0173, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 317, Loss: 0.0112, Train: 0.9962, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 318, Loss: 0.0150, Train: 0.9962, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 319, Loss: 0.0123, Train: 0.9958, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 320, Loss: 0.0147, Train: 0.9946, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 321, Loss: 0.0212, Train: 0.9967, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 322, Loss: 0.0160, Train: 0.9942, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 323, Loss: 0.0238, Train: 0.9962, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 324, Loss: 0.0172, Train: 0.9954, Val: 0.9800, Test: 0.9967\n",
      "Epoch: 325, Loss: 0.0153, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 326, Loss: 0.0174, Train: 0.9967, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 327, Loss: 0.0205, Train: 0.9979, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 328, Loss: 0.0210, Train: 0.9938, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 329, Loss: 0.0144, Train: 0.9983, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 330, Loss: 0.0152, Train: 0.9967, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 331, Loss: 0.0156, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 332, Loss: 0.0234, Train: 0.9925, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 333, Loss: 0.0200, Train: 0.9967, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 334, Loss: 0.0136, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 335, Loss: 0.0159, Train: 0.9983, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 336, Loss: 0.0109, Train: 0.9967, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 337, Loss: 0.0158, Train: 0.9962, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 338, Loss: 0.0567, Train: 0.9946, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 339, Loss: 0.0238, Train: 0.9962, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 340, Loss: 0.0138, Train: 0.9942, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 341, Loss: 0.0128, Train: 0.9950, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 342, Loss: 0.0119, Train: 0.9975, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 343, Loss: 0.0165, Train: 0.9938, Val: 0.9700, Test: 0.9933\n",
      "Epoch: 344, Loss: 0.0128, Train: 0.9954, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 345, Loss: 0.0137, Train: 0.9975, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 346, Loss: 0.0123, Train: 0.9983, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 347, Loss: 0.0096, Train: 0.9975, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 348, Loss: 0.0127, Train: 0.9988, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 349, Loss: 0.0171, Train: 0.9979, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 350, Loss: 0.0148, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 351, Loss: 0.0116, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 352, Loss: 0.0129, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 353, Loss: 0.0262, Train: 0.9954, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 354, Loss: 0.0156, Train: 0.9979, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 355, Loss: 0.0136, Train: 0.9862, Val: 0.9733, Test: 0.9833\n",
      "Epoch: 356, Loss: 0.0173, Train: 0.9962, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 357, Loss: 0.0156, Train: 0.9962, Val: 0.9800, Test: 0.9967\n",
      "Epoch: 358, Loss: 0.0141, Train: 0.9950, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 359, Loss: 0.0151, Train: 0.9954, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 360, Loss: 0.0247, Train: 0.9888, Val: 0.9667, Test: 0.9900\n",
      "Epoch: 361, Loss: 0.0128, Train: 0.9962, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 362, Loss: 0.0117, Train: 0.9979, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 363, Loss: 0.0112, Train: 0.9979, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 364, Loss: 0.0139, Train: 0.9983, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 365, Loss: 0.0103, Train: 0.9950, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 366, Loss: 0.0145, Train: 0.9950, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 367, Loss: 0.0160, Train: 0.9950, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 368, Loss: 0.0152, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 369, Loss: 0.0116, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 370, Loss: 0.0171, Train: 0.9988, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 371, Loss: 0.0119, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 372, Loss: 0.0141, Train: 0.9912, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 373, Loss: 0.0176, Train: 0.9971, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 374, Loss: 0.0132, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 375, Loss: 0.0151, Train: 0.9988, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 376, Loss: 0.0132, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 377, Loss: 0.0180, Train: 0.9983, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 378, Loss: 0.0153, Train: 0.9983, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 379, Loss: 0.0162, Train: 0.9979, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 380, Loss: 0.0195, Train: 0.9904, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 381, Loss: 0.0164, Train: 0.9983, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 382, Loss: 0.0216, Train: 0.9971, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 383, Loss: 0.0163, Train: 0.9975, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 384, Loss: 0.0157, Train: 0.9967, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 385, Loss: 0.0136, Train: 0.9979, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 386, Loss: 0.0145, Train: 0.9958, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 387, Loss: 0.0171, Train: 0.9888, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 388, Loss: 0.0132, Train: 0.9979, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 389, Loss: 0.0143, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 390, Loss: 0.0108, Train: 0.9946, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 391, Loss: 0.0217, Train: 0.9975, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 392, Loss: 0.0143, Train: 0.9958, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 393, Loss: 0.0128, Train: 0.9954, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 394, Loss: 0.0166, Train: 0.9975, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 395, Loss: 0.0210, Train: 0.9975, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 396, Loss: 0.0183, Train: 0.9950, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 397, Loss: 0.0168, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 398, Loss: 0.0114, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 399, Loss: 0.0206, Train: 0.9942, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 400, Loss: 0.0146, Train: 0.9942, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 401, Loss: 0.0120, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 402, Loss: 0.0113, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 403, Loss: 0.0138, Train: 0.9958, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 404, Loss: 0.0085, Train: 0.9975, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 405, Loss: 0.0131, Train: 0.9975, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 406, Loss: 0.0149, Train: 0.9975, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 407, Loss: 0.0100, Train: 0.9975, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 408, Loss: 0.0125, Train: 0.9979, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 409, Loss: 0.0108, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 410, Loss: 0.0112, Train: 0.9975, Val: 0.9800, Test: 0.9967\n",
      "Epoch: 411, Loss: 0.0091, Train: 0.9950, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 412, Loss: 0.0214, Train: 0.9962, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 413, Loss: 0.0181, Train: 0.9942, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 414, Loss: 0.0114, Train: 0.9962, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 415, Loss: 0.0274, Train: 0.9979, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 416, Loss: 0.0143, Train: 0.9979, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 417, Loss: 0.0086, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 418, Loss: 0.0146, Train: 0.9975, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 419, Loss: 0.0075, Train: 0.9975, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 420, Loss: 0.0249, Train: 0.9950, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 421, Loss: 0.0115, Train: 0.9967, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 422, Loss: 0.0129, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 423, Loss: 0.0110, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 424, Loss: 0.0122, Train: 0.9967, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 425, Loss: 0.0125, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 426, Loss: 0.0130, Train: 0.9971, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 427, Loss: 0.0088, Train: 0.9950, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 428, Loss: 0.0172, Train: 0.9804, Val: 0.9567, Test: 0.9700\n",
      "Epoch: 429, Loss: 0.0207, Train: 0.9975, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 430, Loss: 0.0124, Train: 0.9983, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 431, Loss: 0.0138, Train: 0.9967, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 432, Loss: 0.0141, Train: 0.9988, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 433, Loss: 0.0146, Train: 0.9983, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 434, Loss: 0.0206, Train: 0.9962, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 435, Loss: 0.0131, Train: 0.9975, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 436, Loss: 0.0123, Train: 0.9967, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 437, Loss: 0.0157, Train: 0.9954, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 438, Loss: 0.0165, Train: 0.9975, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 439, Loss: 0.0139, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 440, Loss: 0.0237, Train: 0.9975, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 441, Loss: 0.0117, Train: 0.9983, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 442, Loss: 0.0146, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 443, Loss: 0.0120, Train: 0.9979, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 444, Loss: 0.0176, Train: 0.9971, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 445, Loss: 0.0126, Train: 0.9979, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 446, Loss: 0.0071, Train: 0.9942, Val: 0.9833, Test: 0.9900\n",
      "Epoch: 447, Loss: 0.0125, Train: 0.9979, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 448, Loss: 0.0112, Train: 0.9979, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 449, Loss: 0.0110, Train: 0.9975, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 450, Loss: 0.0107, Train: 0.9967, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 451, Loss: 0.0184, Train: 0.9975, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 452, Loss: 0.0102, Train: 0.9975, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 453, Loss: 0.0108, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 454, Loss: 0.0107, Train: 0.9954, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 455, Loss: 0.0123, Train: 0.9983, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 456, Loss: 0.0138, Train: 0.9983, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 457, Loss: 0.0200, Train: 0.9967, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 458, Loss: 0.0098, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 459, Loss: 0.0217, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 460, Loss: 0.0157, Train: 0.9971, Val: 0.9800, Test: 0.9933\n",
      "Epoch: 461, Loss: 0.0130, Train: 0.9979, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 462, Loss: 0.0123, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 463, Loss: 0.0158, Train: 0.9975, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 464, Loss: 0.0195, Train: 0.9983, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 465, Loss: 0.0099, Train: 0.9933, Val: 0.9833, Test: 0.9933\n",
      "Epoch: 466, Loss: 0.0109, Train: 0.9983, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 467, Loss: 0.0116, Train: 0.9962, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 468, Loss: 0.0101, Train: 0.9988, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 469, Loss: 0.0145, Train: 0.9975, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 470, Loss: 0.0197, Train: 0.9946, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 471, Loss: 0.0143, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 472, Loss: 0.0132, Train: 0.9983, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 473, Loss: 0.0123, Train: 0.9967, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 474, Loss: 0.0085, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 475, Loss: 0.0066, Train: 0.9979, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 476, Loss: 0.0128, Train: 0.9979, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 477, Loss: 0.0161, Train: 0.9962, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 478, Loss: 0.0096, Train: 0.9975, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 479, Loss: 0.0129, Train: 0.9983, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 480, Loss: 0.0150, Train: 0.9979, Val: 0.9933, Test: 0.9933\n",
      "Epoch: 481, Loss: 0.0088, Train: 0.9975, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 482, Loss: 0.0102, Train: 0.9983, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 483, Loss: 0.0122, Train: 0.9992, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 484, Loss: 0.0121, Train: 0.9971, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 485, Loss: 0.0130, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 486, Loss: 0.0104, Train: 0.9967, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 487, Loss: 0.0095, Train: 0.9692, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 488, Loss: 0.0187, Train: 0.9967, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 489, Loss: 0.0089, Train: 0.9946, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 490, Loss: 0.0238, Train: 0.9967, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 491, Loss: 0.0114, Train: 0.9983, Val: 0.9833, Test: 0.9967\n",
      "Epoch: 492, Loss: 0.0111, Train: 0.9962, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 493, Loss: 0.0165, Train: 0.9975, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 494, Loss: 0.0121, Train: 0.9979, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 495, Loss: 0.0078, Train: 0.9967, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 496, Loss: 0.0122, Train: 0.9975, Val: 0.9933, Test: 0.9967\n",
      "Epoch: 497, Loss: 0.0117, Train: 0.9979, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 498, Loss: 0.0102, Train: 0.9971, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 499, Loss: 0.0122, Train: 0.9958, Val: 0.9867, Test: 0.9933\n",
      "Epoch: 500, Loss: 0.0179, Train: 0.9896, Val: 0.9733, Test: 0.9867\n",
      "### Run 0 - val loss: 0.018, test acc: 0.997\n",
      "Accuracies in each run:  [0.9966666666666667]\n",
      "test acc - mean: 0.997, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'graclus'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmis\n",
      "Epoch: 001, Loss: 0.6769, Train: 0.7987, Val: 0.7700, Test: 0.7800\n",
      "Epoch: 002, Loss: 0.4728, Train: 0.8263, Val: 0.8167, Test: 0.8067\n",
      "Epoch: 003, Loss: 0.3597, Train: 0.7996, Val: 0.7600, Test: 0.7867\n",
      "Epoch: 004, Loss: 0.2463, Train: 0.9446, Val: 0.9200, Test: 0.9300\n",
      "Epoch: 005, Loss: 0.1901, Train: 0.9492, Val: 0.9467, Test: 0.9433\n",
      "Epoch: 006, Loss: 0.1283, Train: 0.9717, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 007, Loss: 0.1067, Train: 0.9792, Val: 0.9767, Test: 0.9700\n",
      "Epoch: 008, Loss: 0.0863, Train: 0.9775, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 009, Loss: 0.0778, Train: 0.9829, Val: 0.9700, Test: 0.9833\n",
      "Epoch: 010, Loss: 0.0715, Train: 0.9892, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 011, Loss: 0.0577, Train: 0.9900, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 012, Loss: 0.0617, Train: 0.9838, Val: 0.9767, Test: 0.9933\n",
      "Epoch: 013, Loss: 0.0448, Train: 0.9875, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 014, Loss: 0.0450, Train: 0.9896, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 015, Loss: 0.0334, Train: 0.9879, Val: 0.9867, Test: 0.9967\n",
      "Epoch: 016, Loss: 0.0325, Train: 0.9792, Val: 0.9567, Test: 0.9767\n",
      "Epoch: 017, Loss: 0.0382, Train: 0.9746, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 018, Loss: 0.0309, Train: 0.9896, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 019, Loss: 0.0351, Train: 0.9904, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 020, Loss: 0.0383, Train: 0.9950, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 021, Loss: 0.0245, Train: 0.9925, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 022, Loss: 0.0323, Train: 0.9958, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 023, Loss: 0.0247, Train: 0.9971, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 024, Loss: 0.0232, Train: 0.9938, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 025, Loss: 0.0212, Train: 0.9933, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 026, Loss: 0.0213, Train: 0.9946, Val: 0.9900, Test: 0.9967\n",
      "Epoch: 027, Loss: 0.0218, Train: 0.9933, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 028, Loss: 0.0157, Train: 0.9962, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 029, Loss: 0.0183, Train: 0.9967, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 030, Loss: 0.0205, Train: 0.9962, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 031, Loss: 0.0196, Train: 0.9967, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 032, Loss: 0.0087, Train: 0.9975, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 033, Loss: 0.0110, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 034, Loss: 0.0160, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 035, Loss: 0.0109, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 036, Loss: 0.0063, Train: 0.9983, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 037, Loss: 0.0159, Train: 0.9975, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 038, Loss: 0.0280, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 039, Loss: 0.0240, Train: 0.9975, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 040, Loss: 0.0121, Train: 0.9979, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 041, Loss: 0.0088, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 042, Loss: 0.0103, Train: 0.9983, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 043, Loss: 0.0342, Train: 0.9967, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 044, Loss: 0.0137, Train: 0.9967, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 045, Loss: 0.0120, Train: 0.9979, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 046, Loss: 0.0076, Train: 0.9979, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 047, Loss: 0.0053, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 048, Loss: 0.0099, Train: 0.9967, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 049, Loss: 0.0184, Train: 0.9950, Val: 0.9933, Test: 1.0000\n",
      "Epoch: 050, Loss: 0.0099, Train: 0.9988, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 051, Loss: 0.0083, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 052, Loss: 0.0118, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 053, Loss: 0.0256, Train: 0.9938, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 054, Loss: 0.0148, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 055, Loss: 0.0053, Train: 0.9983, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 056, Loss: 0.0065, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 057, Loss: 0.0063, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 058, Loss: 0.0127, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 059, Loss: 0.0102, Train: 0.9971, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 060, Loss: 0.0061, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 061, Loss: 0.0037, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 062, Loss: 0.0025, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 063, Loss: 0.0045, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 064, Loss: 0.0104, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 065, Loss: 0.0080, Train: 0.9971, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 066, Loss: 0.0144, Train: 0.9996, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 067, Loss: 0.0281, Train: 0.9975, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 068, Loss: 0.0088, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 069, Loss: 0.0040, Train: 0.9992, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 070, Loss: 0.0069, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 071, Loss: 0.0031, Train: 0.9988, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 072, Loss: 0.0071, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 073, Loss: 0.0064, Train: 0.9992, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 074, Loss: 0.0027, Train: 0.9988, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 075, Loss: 0.0045, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 076, Loss: 0.0119, Train: 0.9900, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 077, Loss: 0.0076, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 078, Loss: 0.0009, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 079, Loss: 0.0065, Train: 0.9979, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 080, Loss: 0.0042, Train: 0.9979, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 081, Loss: 0.0044, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 082, Loss: 0.0123, Train: 0.9975, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 083, Loss: 0.0026, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 084, Loss: 0.0088, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 085, Loss: 0.0051, Train: 0.9983, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 086, Loss: 0.0030, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 087, Loss: 0.0067, Train: 0.9988, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 088, Loss: 0.0024, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 089, Loss: 0.0017, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 090, Loss: 0.0013, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 091, Loss: 0.0047, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 092, Loss: 0.0010, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 093, Loss: 0.0036, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 094, Loss: 0.0034, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 095, Loss: 0.0071, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 096, Loss: 0.0012, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 097, Loss: 0.0006, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 098, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 099, Loss: 0.0031, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 100, Loss: 0.0003, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 101, Loss: 0.0033, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 102, Loss: 0.0074, Train: 0.9938, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 103, Loss: 0.0049, Train: 0.9983, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 104, Loss: 0.0067, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 105, Loss: 0.0007, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 106, Loss: 0.0049, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 107, Loss: 0.0050, Train: 0.9992, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 108, Loss: 0.0107, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 109, Loss: 0.0079, Train: 0.9954, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 110, Loss: 0.0173, Train: 0.9983, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 111, Loss: 0.0111, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 112, Loss: 0.0031, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 113, Loss: 0.0036, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 114, Loss: 0.0105, Train: 0.9983, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 115, Loss: 0.0054, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 116, Loss: 0.0019, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 117, Loss: 0.0021, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 118, Loss: 0.0020, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 119, Loss: 0.0051, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 120, Loss: 0.0047, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 121, Loss: 0.0245, Train: 0.9971, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 122, Loss: 0.0041, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 123, Loss: 0.0068, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 124, Loss: 0.0011, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 125, Loss: 0.0051, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 126, Loss: 0.0081, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 127, Loss: 0.0010, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 128, Loss: 0.0036, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 129, Loss: 0.0009, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 130, Loss: 0.0040, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 131, Loss: 0.0031, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 132, Loss: 0.0014, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 133, Loss: 0.0019, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 134, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 135, Loss: 0.0012, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 136, Loss: 0.0033, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 137, Loss: 0.0015, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 138, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 139, Loss: 0.0009, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 140, Loss: 0.0002, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 141, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 142, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 143, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 144, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 145, Loss: 0.0064, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 146, Loss: 0.0045, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 147, Loss: 0.0101, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 148, Loss: 0.0044, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 149, Loss: 0.0013, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 150, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 151, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 152, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 153, Loss: 0.0007, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 154, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 155, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 156, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 157, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 158, Loss: 0.0007, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 159, Loss: 0.0477, Train: 0.9850, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 160, Loss: 0.0140, Train: 0.9992, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 161, Loss: 0.0033, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 162, Loss: 0.0027, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 163, Loss: 0.0015, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 164, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 165, Loss: 0.0020, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 166, Loss: 0.0013, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 167, Loss: 0.0021, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 168, Loss: 0.0010, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 169, Loss: 0.0057, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 170, Loss: 0.0024, Train: 0.9979, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 171, Loss: 0.0053, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 172, Loss: 0.0006, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 173, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 174, Loss: 0.0006, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 175, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 176, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 177, Loss: 0.0007, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 178, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 179, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 180, Loss: 0.0014, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 181, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 182, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 183, Loss: 0.0064, Train: 0.9971, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 184, Loss: 0.0100, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 185, Loss: 0.0019, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 186, Loss: 0.0010, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 187, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 188, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 189, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 190, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 191, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 192, Loss: 0.0009, Train: 0.9996, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 193, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 194, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 195, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 196, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 197, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 198, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 199, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 200, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 201, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 202, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 203, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 204, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 205, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 206, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 207, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 208, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 209, Loss: 0.0002, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 210, Loss: 0.0326, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 211, Loss: 0.0042, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 212, Loss: 0.0009, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 213, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 214, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 215, Loss: 0.0006, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 216, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 217, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 218, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 219, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 220, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 221, Loss: 0.0011, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 222, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 223, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 224, Loss: 0.0037, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 225, Loss: 0.0010, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 226, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 227, Loss: 0.0103, Train: 0.9879, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 228, Loss: 0.0230, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 229, Loss: 0.0050, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 230, Loss: 0.0008, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 231, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 232, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 233, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 234, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 235, Loss: 0.0007, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 236, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 237, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 238, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 239, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 240, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 241, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 242, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 243, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 244, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 245, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 246, Loss: 0.0327, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 247, Loss: 0.0077, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 248, Loss: 0.0011, Train: 1.0000, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 249, Loss: 0.0027, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 250, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 251, Loss: 0.0012, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 252, Loss: 0.0018, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 253, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 254, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 255, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 256, Loss: 0.0009, Train: 0.9975, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 257, Loss: 0.0010, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 258, Loss: 0.0004, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 259, Loss: 0.0010, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 260, Loss: 0.0018, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 261, Loss: 0.0005, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 262, Loss: 0.0019, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 263, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 264, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 265, Loss: 0.0008, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 266, Loss: 0.0088, Train: 0.9983, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 267, Loss: 0.0054, Train: 0.9988, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 268, Loss: 0.0006, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 269, Loss: 0.0006, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 270, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 271, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 272, Loss: 0.0009, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 273, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 274, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 275, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 276, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 277, Loss: 0.0026, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 278, Loss: 0.0082, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 279, Loss: 0.0006, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 280, Loss: 0.0012, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 281, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 282, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 283, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 284, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 285, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 286, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 287, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 288, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 289, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 290, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 291, Loss: 0.0092, Train: 0.9988, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 292, Loss: 0.0009, Train: 0.9975, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 293, Loss: 0.0034, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 294, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 295, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 296, Loss: 0.0006, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 297, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 298, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 299, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 300, Loss: 0.0035, Train: 0.9975, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 301, Loss: 0.0112, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 302, Loss: 0.0018, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 303, Loss: 0.0008, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 304, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 305, Loss: 0.0006, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 306, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 307, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 308, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 309, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 310, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 311, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 312, Loss: 0.0054, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 313, Loss: 0.0072, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 314, Loss: 0.0026, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 315, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 316, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 317, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 318, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 319, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 320, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 321, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 322, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 323, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 324, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 325, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 326, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 327, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 328, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 329, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 330, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 331, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 332, Loss: 0.0003, Train: 0.9979, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 333, Loss: 0.0043, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 334, Loss: 0.0257, Train: 0.9954, Val: 0.9967, Test: 1.0000\n",
      "Epoch: 335, Loss: 0.0085, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 336, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 337, Loss: 0.0029, Train: 0.9983, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 338, Loss: 0.0074, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 339, Loss: 0.0010, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 340, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 341, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 342, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 343, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 344, Loss: 0.0016, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 345, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 346, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 347, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 348, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 349, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 350, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 351, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 352, Loss: 0.0009, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 353, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 354, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 355, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 356, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 357, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 358, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 359, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 360, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 361, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 362, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 363, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 364, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 365, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 366, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 367, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 368, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 369, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 370, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 371, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 372, Loss: 0.0040, Train: 0.9958, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 373, Loss: 0.0117, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 374, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 375, Loss: 0.0008, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 376, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 377, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 378, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 379, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 380, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 381, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 382, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 383, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 384, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 385, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 386, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 387, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 388, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 389, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 390, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 391, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 392, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 393, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 394, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 395, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 396, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 397, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 398, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 399, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 400, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 401, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 402, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 403, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 404, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 405, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 406, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 407, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 408, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 409, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 410, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 411, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 412, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 413, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 414, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 415, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 416, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 417, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 418, Loss: 0.0110, Train: 0.9825, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 419, Loss: 0.0436, Train: 0.9988, Val: 1.0000, Test: 0.9933\n",
      "Epoch: 420, Loss: 0.0056, Train: 0.9983, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 421, Loss: 0.0052, Train: 1.0000, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 422, Loss: 0.0048, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 423, Loss: 0.0015, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 424, Loss: 0.0007, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 425, Loss: 0.0017, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 426, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 427, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 428, Loss: 0.0012, Train: 1.0000, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 429, Loss: 0.0004, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 430, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 431, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 432, Loss: 0.0003, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 433, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 0.9967\n",
      "Epoch: 434, Loss: 0.0017, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 435, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 436, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 437, Loss: 0.0033, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 438, Loss: 0.0016, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 439, Loss: 0.0005, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 440, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 441, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 442, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 443, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 444, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 445, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 446, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 447, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 448, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 449, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 450, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 451, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 452, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 453, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 454, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 455, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 456, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 457, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 458, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 459, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 460, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 461, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 462, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 463, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 464, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 465, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 466, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 467, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 468, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 469, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 470, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 471, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 472, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 473, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 474, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 475, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 476, Loss: 0.0203, Train: 0.9938, Val: 0.9967, Test: 0.9967\n",
      "Epoch: 477, Loss: 0.0135, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 478, Loss: 0.0052, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 479, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 480, Loss: 0.0007, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 481, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 482, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 483, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 484, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 485, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 486, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 487, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 488, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 489, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 490, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 491, Loss: 0.0017, Train: 0.9992, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 492, Loss: 0.0027, Train: 0.9996, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 493, Loss: 0.0011, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 494, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 495, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 496, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 497, Loss: 0.0001, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 498, Loss: 0.0000, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 499, Loss: 0.0002, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "Epoch: 500, Loss: 0.0028, Train: 1.0000, Val: 1.0000, Test: 1.0000\n",
      "### Run 0 - val loss: 0.000, test acc: 1.000\n",
      "Accuracies in each run:  [1.0]\n",
      "test acc - mean: 1.000, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'kmis'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk\n",
      "Epoch: 001, Loss: 0.7051, Train: 0.5088, Val: 0.5000, Test: 0.4500\n",
      "Epoch: 002, Loss: 0.6996, Train: 0.5333, Val: 0.5433, Test: 0.5133\n",
      "Epoch: 003, Loss: 0.6905, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 004, Loss: 0.6674, Train: 0.7662, Val: 0.7433, Test: 0.7500\n",
      "Epoch: 005, Loss: 0.6909, Train: 0.5396, Val: 0.5633, Test: 0.5067\n",
      "Epoch: 006, Loss: 0.6463, Train: 0.5088, Val: 0.5067, Test: 0.4567\n",
      "Epoch: 007, Loss: 0.6540, Train: 0.7812, Val: 0.7400, Test: 0.7767\n",
      "Epoch: 008, Loss: 0.6745, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 009, Loss: 0.6891, Train: 0.7300, Val: 0.7233, Test: 0.7233\n",
      "Epoch: 010, Loss: 0.6437, Train: 0.7804, Val: 0.7500, Test: 0.7700\n",
      "Epoch: 011, Loss: 0.7038, Train: 0.5092, Val: 0.5133, Test: 0.4633\n",
      "Epoch: 012, Loss: 0.7033, Train: 0.4813, Val: 0.5067, Test: 0.5167\n",
      "Epoch: 013, Loss: 0.6990, Train: 0.4988, Val: 0.5067, Test: 0.5367\n",
      "Epoch: 014, Loss: 0.6940, Train: 0.5587, Val: 0.5700, Test: 0.5333\n",
      "Epoch: 015, Loss: 0.6947, Train: 0.5200, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 016, Loss: 0.6865, Train: 0.4950, Val: 0.4967, Test: 0.5467\n",
      "Epoch: 017, Loss: 0.6859, Train: 0.4954, Val: 0.4967, Test: 0.5433\n",
      "Epoch: 018, Loss: 0.7003, Train: 0.5996, Val: 0.5833, Test: 0.6233\n",
      "Epoch: 019, Loss: 0.6589, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 020, Loss: 0.6560, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 021, Loss: 0.6883, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 022, Loss: 0.7015, Train: 0.5092, Val: 0.5133, Test: 0.4500\n",
      "Epoch: 023, Loss: 0.7000, Train: 0.5054, Val: 0.4933, Test: 0.4400\n",
      "Epoch: 024, Loss: 0.6981, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 025, Loss: 0.6983, Train: 0.5117, Val: 0.4833, Test: 0.4900\n",
      "Epoch: 026, Loss: 0.6943, Train: 0.5092, Val: 0.4900, Test: 0.4833\n",
      "Epoch: 027, Loss: 0.6950, Train: 0.5042, Val: 0.4700, Test: 0.4933\n",
      "Epoch: 028, Loss: 0.6959, Train: 0.5000, Val: 0.5100, Test: 0.4467\n",
      "Epoch: 029, Loss: 0.6956, Train: 0.5042, Val: 0.4900, Test: 0.5567\n",
      "Epoch: 030, Loss: 0.6952, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 031, Loss: 0.6945, Train: 0.4913, Val: 0.4900, Test: 0.5433\n",
      "Epoch: 032, Loss: 0.6929, Train: 0.4900, Val: 0.4267, Test: 0.5067\n",
      "Epoch: 033, Loss: 0.6957, Train: 0.5033, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 034, Loss: 0.6954, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 035, Loss: 0.6961, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 036, Loss: 0.6916, Train: 0.5254, Val: 0.5200, Test: 0.5800\n",
      "Epoch: 037, Loss: 0.6372, Train: 0.7767, Val: 0.7500, Test: 0.7700\n",
      "Epoch: 038, Loss: 0.6739, Train: 0.7958, Val: 0.7733, Test: 0.8100\n",
      "Epoch: 039, Loss: 0.6438, Train: 0.5067, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 040, Loss: 0.7075, Train: 0.5188, Val: 0.5367, Test: 0.4867\n",
      "Epoch: 041, Loss: 0.6804, Train: 0.5121, Val: 0.5100, Test: 0.4633\n",
      "Epoch: 042, Loss: 0.6463, Train: 0.5067, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 043, Loss: 0.7035, Train: 0.5467, Val: 0.5533, Test: 0.5100\n",
      "Epoch: 044, Loss: 0.6535, Train: 0.7696, Val: 0.7433, Test: 0.7567\n",
      "Epoch: 045, Loss: 0.6570, Train: 0.7612, Val: 0.7467, Test: 0.7500\n",
      "Epoch: 046, Loss: 0.6522, Train: 0.7621, Val: 0.7300, Test: 0.7533\n",
      "Epoch: 047, Loss: 0.7007, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 048, Loss: 0.7021, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 049, Loss: 0.7011, Train: 0.5075, Val: 0.5067, Test: 0.4567\n",
      "Epoch: 050, Loss: 0.6948, Train: 0.5158, Val: 0.5167, Test: 0.4733\n",
      "Epoch: 051, Loss: 0.6966, Train: 0.5138, Val: 0.5133, Test: 0.4733\n",
      "Epoch: 052, Loss: 0.6953, Train: 0.5250, Val: 0.5333, Test: 0.4900\n",
      "Epoch: 053, Loss: 0.6932, Train: 0.5096, Val: 0.5100, Test: 0.4600\n",
      "Epoch: 054, Loss: 0.6944, Train: 0.5342, Val: 0.5400, Test: 0.4933\n",
      "Epoch: 055, Loss: 0.6911, Train: 0.5433, Val: 0.5600, Test: 0.5100\n",
      "Epoch: 056, Loss: 0.6901, Train: 0.6396, Val: 0.6300, Test: 0.6233\n",
      "Epoch: 057, Loss: 0.6854, Train: 0.5583, Val: 0.5633, Test: 0.5333\n",
      "Epoch: 058, Loss: 0.6855, Train: 0.5300, Val: 0.5367, Test: 0.4967\n",
      "Epoch: 059, Loss: 0.6922, Train: 0.5004, Val: 0.5033, Test: 0.5467\n",
      "Epoch: 060, Loss: 0.6912, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 061, Loss: 0.6754, Train: 0.5238, Val: 0.5267, Test: 0.4800\n",
      "Epoch: 062, Loss: 0.6474, Train: 0.5075, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 063, Loss: 0.6594, Train: 0.5104, Val: 0.5067, Test: 0.4600\n",
      "Epoch: 064, Loss: 0.6752, Train: 0.7296, Val: 0.7033, Test: 0.7300\n",
      "Epoch: 065, Loss: 0.6574, Train: 0.5154, Val: 0.5167, Test: 0.4700\n",
      "Epoch: 066, Loss: 0.6600, Train: 0.5129, Val: 0.5200, Test: 0.4633\n",
      "Epoch: 067, Loss: 0.6706, Train: 0.5171, Val: 0.5167, Test: 0.4667\n",
      "Epoch: 068, Loss: 0.6689, Train: 0.5054, Val: 0.5033, Test: 0.4533\n",
      "Epoch: 069, Loss: 0.6950, Train: 0.5654, Val: 0.5767, Test: 0.5333\n",
      "Epoch: 070, Loss: 0.6885, Train: 0.5842, Val: 0.6000, Test: 0.5533\n",
      "Epoch: 071, Loss: 0.6827, Train: 0.5787, Val: 0.5633, Test: 0.6200\n",
      "Epoch: 072, Loss: 0.6744, Train: 0.5138, Val: 0.5200, Test: 0.4633\n",
      "Epoch: 073, Loss: 0.6851, Train: 0.6012, Val: 0.5933, Test: 0.6400\n",
      "Epoch: 074, Loss: 0.6869, Train: 0.6067, Val: 0.5833, Test: 0.6033\n",
      "Epoch: 075, Loss: 0.6841, Train: 0.6104, Val: 0.5667, Test: 0.6400\n",
      "Epoch: 076, Loss: 0.6831, Train: 0.5942, Val: 0.6033, Test: 0.6167\n",
      "Epoch: 077, Loss: 0.6760, Train: 0.5854, Val: 0.6067, Test: 0.6167\n",
      "Epoch: 078, Loss: 0.6761, Train: 0.4950, Val: 0.4967, Test: 0.5500\n",
      "Epoch: 079, Loss: 0.6792, Train: 0.6179, Val: 0.6133, Test: 0.6000\n",
      "Epoch: 080, Loss: 0.6689, Train: 0.6038, Val: 0.6000, Test: 0.6133\n",
      "Epoch: 081, Loss: 0.7039, Train: 0.5679, Val: 0.5767, Test: 0.5367\n",
      "Epoch: 082, Loss: 0.6775, Train: 0.5475, Val: 0.5367, Test: 0.5200\n",
      "Epoch: 083, Loss: 0.6755, Train: 0.5800, Val: 0.5633, Test: 0.5533\n",
      "Epoch: 084, Loss: 0.6724, Train: 0.5883, Val: 0.5867, Test: 0.5867\n",
      "Epoch: 085, Loss: 0.6701, Train: 0.6204, Val: 0.5967, Test: 0.6233\n",
      "Epoch: 086, Loss: 0.6699, Train: 0.5717, Val: 0.5467, Test: 0.5600\n",
      "Epoch: 087, Loss: 0.6683, Train: 0.5971, Val: 0.5800, Test: 0.5767\n",
      "Epoch: 088, Loss: 0.6645, Train: 0.6021, Val: 0.6067, Test: 0.5767\n",
      "Epoch: 089, Loss: 0.6624, Train: 0.6217, Val: 0.6233, Test: 0.6433\n",
      "Epoch: 090, Loss: 0.6569, Train: 0.5871, Val: 0.5867, Test: 0.5567\n",
      "Epoch: 091, Loss: 0.7873, Train: 0.4950, Val: 0.4967, Test: 0.5433\n",
      "Epoch: 092, Loss: 0.6977, Train: 0.4975, Val: 0.5167, Test: 0.5567\n",
      "Epoch: 093, Loss: 0.6950, Train: 0.5396, Val: 0.5867, Test: 0.5233\n",
      "Epoch: 094, Loss: 0.6910, Train: 0.5062, Val: 0.5100, Test: 0.4567\n",
      "Epoch: 095, Loss: 0.6942, Train: 0.5483, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 096, Loss: 0.6899, Train: 0.5533, Val: 0.5500, Test: 0.5933\n",
      "Epoch: 097, Loss: 0.6740, Train: 0.5054, Val: 0.5100, Test: 0.4567\n",
      "Epoch: 098, Loss: 0.6720, Train: 0.7462, Val: 0.7333, Test: 0.7367\n",
      "Epoch: 099, Loss: 0.6882, Train: 0.5092, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 100, Loss: 0.6862, Train: 0.5079, Val: 0.5100, Test: 0.4633\n",
      "Epoch: 101, Loss: 0.6725, Train: 0.5062, Val: 0.5067, Test: 0.4567\n",
      "Epoch: 102, Loss: 0.6927, Train: 0.5446, Val: 0.5700, Test: 0.5067\n",
      "Epoch: 103, Loss: 0.6818, Train: 0.5546, Val: 0.5767, Test: 0.5233\n",
      "Epoch: 104, Loss: 0.6470, Train: 0.5179, Val: 0.5233, Test: 0.4700\n",
      "Epoch: 105, Loss: 0.6789, Train: 0.5150, Val: 0.5233, Test: 0.4733\n",
      "Epoch: 106, Loss: 0.6737, Train: 0.5821, Val: 0.6100, Test: 0.5767\n",
      "Epoch: 107, Loss: 0.6832, Train: 0.5913, Val: 0.5933, Test: 0.5767\n",
      "Epoch: 108, Loss: 0.6790, Train: 0.5921, Val: 0.5967, Test: 0.5600\n",
      "Epoch: 109, Loss: 0.6749, Train: 0.6029, Val: 0.6233, Test: 0.5833\n",
      "Epoch: 110, Loss: 0.6727, Train: 0.6129, Val: 0.6233, Test: 0.6067\n",
      "Epoch: 111, Loss: 0.6649, Train: 0.6179, Val: 0.6200, Test: 0.5733\n",
      "Epoch: 112, Loss: 0.6573, Train: 0.6225, Val: 0.6500, Test: 0.6233\n",
      "Epoch: 113, Loss: 0.7118, Train: 0.5408, Val: 0.5767, Test: 0.5667\n",
      "Epoch: 114, Loss: 0.6945, Train: 0.5521, Val: 0.5700, Test: 0.5067\n",
      "Epoch: 115, Loss: 0.6706, Train: 0.5067, Val: 0.5100, Test: 0.4567\n",
      "Epoch: 116, Loss: 0.6939, Train: 0.5221, Val: 0.5433, Test: 0.4633\n",
      "Epoch: 117, Loss: 0.6815, Train: 0.5083, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 118, Loss: 0.6768, Train: 0.6683, Val: 0.6500, Test: 0.6800\n",
      "Epoch: 119, Loss: 0.6609, Train: 0.5296, Val: 0.5533, Test: 0.5000\n",
      "Epoch: 120, Loss: 0.6572, Train: 0.6242, Val: 0.6033, Test: 0.6533\n",
      "Epoch: 121, Loss: 0.6499, Train: 0.5067, Val: 0.5067, Test: 0.4533\n",
      "Epoch: 122, Loss: 0.6832, Train: 0.5983, Val: 0.5867, Test: 0.5667\n",
      "Epoch: 123, Loss: 0.6762, Train: 0.6154, Val: 0.6233, Test: 0.5933\n",
      "Epoch: 124, Loss: 0.6676, Train: 0.6067, Val: 0.5967, Test: 0.5467\n",
      "Epoch: 125, Loss: 0.6665, Train: 0.6079, Val: 0.5967, Test: 0.6133\n",
      "Epoch: 126, Loss: 0.6555, Train: 0.6117, Val: 0.6100, Test: 0.6000\n",
      "Epoch: 127, Loss: 0.6623, Train: 0.6300, Val: 0.6233, Test: 0.6400\n",
      "Epoch: 128, Loss: 0.6815, Train: 0.6212, Val: 0.6433, Test: 0.5933\n",
      "Epoch: 129, Loss: 0.6665, Train: 0.6033, Val: 0.5900, Test: 0.5900\n",
      "Epoch: 130, Loss: 0.6580, Train: 0.6283, Val: 0.5967, Test: 0.6167\n",
      "Epoch: 131, Loss: 0.6530, Train: 0.6304, Val: 0.6233, Test: 0.6133\n",
      "Epoch: 132, Loss: 0.6753, Train: 0.4946, Val: 0.4967, Test: 0.5467\n",
      "Epoch: 133, Loss: 0.6421, Train: 0.5529, Val: 0.5700, Test: 0.5167\n",
      "Epoch: 134, Loss: 0.6487, Train: 0.5954, Val: 0.5633, Test: 0.6267\n",
      "Epoch: 135, Loss: 0.6559, Train: 0.5921, Val: 0.6233, Test: 0.5933\n",
      "Epoch: 136, Loss: 0.6466, Train: 0.6108, Val: 0.6133, Test: 0.6033\n",
      "Epoch: 137, Loss: 0.6519, Train: 0.6112, Val: 0.6333, Test: 0.5800\n",
      "Epoch: 138, Loss: 0.6577, Train: 0.6050, Val: 0.6267, Test: 0.5967\n",
      "Epoch: 139, Loss: 0.6590, Train: 0.6225, Val: 0.6367, Test: 0.6067\n",
      "Epoch: 140, Loss: 0.6470, Train: 0.5325, Val: 0.5500, Test: 0.4967\n",
      "Epoch: 141, Loss: 0.6704, Train: 0.6167, Val: 0.6300, Test: 0.6167\n",
      "Epoch: 142, Loss: 0.6660, Train: 0.4946, Val: 0.4967, Test: 0.5467\n",
      "Epoch: 143, Loss: 0.6370, Train: 0.5292, Val: 0.5467, Test: 0.5067\n",
      "Epoch: 144, Loss: 0.6571, Train: 0.5079, Val: 0.5100, Test: 0.4567\n",
      "Epoch: 145, Loss: 0.6841, Train: 0.5246, Val: 0.5467, Test: 0.4600\n",
      "Epoch: 146, Loss: 0.6785, Train: 0.5096, Val: 0.5267, Test: 0.4767\n",
      "Epoch: 147, Loss: 0.6586, Train: 0.5071, Val: 0.5100, Test: 0.4600\n",
      "Epoch: 148, Loss: 0.6911, Train: 0.5537, Val: 0.5767, Test: 0.5100\n",
      "Epoch: 149, Loss: 0.6794, Train: 0.6058, Val: 0.6100, Test: 0.6033\n",
      "Epoch: 150, Loss: 0.6727, Train: 0.6217, Val: 0.6300, Test: 0.6000\n",
      "Epoch: 151, Loss: 0.6772, Train: 0.4946, Val: 0.4967, Test: 0.5467\n",
      "Epoch: 152, Loss: 0.6291, Train: 0.8037, Val: 0.7633, Test: 0.7967\n",
      "Epoch: 153, Loss: 0.6789, Train: 0.5062, Val: 0.5100, Test: 0.4567\n",
      "Epoch: 154, Loss: 0.6751, Train: 0.5075, Val: 0.5233, Test: 0.4600\n",
      "Epoch: 155, Loss: 0.6864, Train: 0.5513, Val: 0.5867, Test: 0.5267\n",
      "Epoch: 156, Loss: 0.6771, Train: 0.5067, Val: 0.5200, Test: 0.4600\n",
      "Epoch: 157, Loss: 0.6795, Train: 0.5083, Val: 0.5167, Test: 0.4733\n",
      "Epoch: 158, Loss: 0.6787, Train: 0.5129, Val: 0.5267, Test: 0.4767\n",
      "Epoch: 159, Loss: 0.6757, Train: 0.5492, Val: 0.5700, Test: 0.5033\n",
      "Epoch: 160, Loss: 0.6893, Train: 0.5837, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 161, Loss: 0.6532, Train: 0.5062, Val: 0.5133, Test: 0.4600\n",
      "Epoch: 162, Loss: 0.6874, Train: 0.5504, Val: 0.5800, Test: 0.5300\n",
      "Epoch: 163, Loss: 0.6819, Train: 0.5837, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 164, Loss: 0.6898, Train: 0.5725, Val: 0.6067, Test: 0.5433\n",
      "Epoch: 165, Loss: 0.6744, Train: 0.4946, Val: 0.4967, Test: 0.5467\n",
      "Epoch: 166, Loss: 0.6725, Train: 0.5108, Val: 0.5267, Test: 0.4767\n",
      "Epoch: 167, Loss: 0.6946, Train: 0.5796, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 168, Loss: 0.6530, Train: 0.5775, Val: 0.5867, Test: 0.5467\n",
      "Epoch: 169, Loss: 0.6721, Train: 0.6142, Val: 0.6000, Test: 0.6133\n",
      "Epoch: 170, Loss: 0.6887, Train: 0.5500, Val: 0.5733, Test: 0.5133\n",
      "Epoch: 171, Loss: 0.6687, Train: 0.7758, Val: 0.7433, Test: 0.7800\n",
      "Epoch: 172, Loss: 0.6744, Train: 0.6067, Val: 0.6000, Test: 0.5667\n",
      "Epoch: 173, Loss: 0.6818, Train: 0.4946, Val: 0.4967, Test: 0.5467\n",
      "Epoch: 174, Loss: 0.6721, Train: 0.6658, Val: 0.6567, Test: 0.6833\n",
      "Epoch: 175, Loss: 0.6490, Train: 0.5996, Val: 0.6033, Test: 0.5567\n",
      "Epoch: 176, Loss: 0.6410, Train: 0.5221, Val: 0.5433, Test: 0.4900\n",
      "Epoch: 177, Loss: 0.6854, Train: 0.5833, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 178, Loss: 0.6779, Train: 0.5079, Val: 0.5000, Test: 0.5200\n",
      "Epoch: 179, Loss: 0.6546, Train: 0.5854, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 180, Loss: 0.6647, Train: 0.5813, Val: 0.5967, Test: 0.5467\n",
      "Epoch: 181, Loss: 0.6804, Train: 0.5846, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 182, Loss: 0.6415, Train: 0.5179, Val: 0.5600, Test: 0.4700\n",
      "Epoch: 183, Loss: 0.6884, Train: 0.5529, Val: 0.5700, Test: 0.4967\n",
      "Epoch: 184, Loss: 0.6493, Train: 0.7796, Val: 0.7667, Test: 0.7800\n",
      "Epoch: 185, Loss: 0.6541, Train: 0.5217, Val: 0.5500, Test: 0.4867\n",
      "Epoch: 186, Loss: 0.6616, Train: 0.5654, Val: 0.5833, Test: 0.5433\n",
      "Epoch: 187, Loss: 0.6609, Train: 0.5246, Val: 0.5700, Test: 0.4967\n",
      "Epoch: 188, Loss: 0.6915, Train: 0.5787, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 189, Loss: 0.6681, Train: 0.5837, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 190, Loss: 0.6889, Train: 0.5837, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 191, Loss: 0.6821, Train: 0.6071, Val: 0.5933, Test: 0.5867\n",
      "Epoch: 192, Loss: 0.6688, Train: 0.7758, Val: 0.7333, Test: 0.7733\n",
      "Epoch: 193, Loss: 0.6052, Train: 0.5196, Val: 0.5533, Test: 0.4667\n",
      "Epoch: 194, Loss: 0.6883, Train: 0.5758, Val: 0.5867, Test: 0.5433\n",
      "Epoch: 195, Loss: 0.6627, Train: 0.5850, Val: 0.6000, Test: 0.5433\n",
      "Epoch: 196, Loss: 0.6829, Train: 0.5858, Val: 0.6033, Test: 0.5367\n",
      "Epoch: 197, Loss: 0.6770, Train: 0.5858, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 198, Loss: 0.6638, Train: 0.5854, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 199, Loss: 0.6654, Train: 0.5846, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 200, Loss: 0.6843, Train: 0.5575, Val: 0.5700, Test: 0.5133\n",
      "Epoch: 201, Loss: 0.6448, Train: 0.5850, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 202, Loss: 0.6537, Train: 0.5563, Val: 0.5767, Test: 0.5100\n",
      "Epoch: 203, Loss: 0.6654, Train: 0.5858, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 204, Loss: 0.6172, Train: 0.5850, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 205, Loss: 0.6846, Train: 0.5863, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 206, Loss: 0.6259, Train: 0.5858, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 207, Loss: 0.6039, Train: 0.7992, Val: 0.7733, Test: 0.8000\n",
      "Epoch: 208, Loss: 0.6305, Train: 0.5808, Val: 0.5900, Test: 0.5367\n",
      "Epoch: 209, Loss: 0.6812, Train: 0.5850, Val: 0.6000, Test: 0.5433\n",
      "Epoch: 210, Loss: 0.6835, Train: 0.5850, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 211, Loss: 0.6545, Train: 0.7975, Val: 0.7633, Test: 0.7933\n",
      "Epoch: 212, Loss: 0.5889, Train: 0.5254, Val: 0.5500, Test: 0.4833\n",
      "Epoch: 213, Loss: 0.6712, Train: 0.5813, Val: 0.5900, Test: 0.5467\n",
      "Epoch: 214, Loss: 0.6828, Train: 0.5833, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 215, Loss: 0.6861, Train: 0.5837, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 216, Loss: 0.6853, Train: 0.5804, Val: 0.5933, Test: 0.5167\n",
      "Epoch: 217, Loss: 0.6805, Train: 0.5863, Val: 0.6000, Test: 0.5433\n",
      "Epoch: 218, Loss: 0.6086, Train: 0.5850, Val: 0.6000, Test: 0.5433\n",
      "Epoch: 219, Loss: 0.6772, Train: 0.5004, Val: 0.5067, Test: 0.5200\n",
      "Epoch: 220, Loss: 0.6866, Train: 0.5879, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 221, Loss: 0.6817, Train: 0.5533, Val: 0.5533, Test: 0.5267\n",
      "Epoch: 222, Loss: 0.6862, Train: 0.5875, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 223, Loss: 0.6745, Train: 0.5858, Val: 0.5933, Test: 0.5400\n",
      "Epoch: 224, Loss: 0.6387, Train: 0.5854, Val: 0.6000, Test: 0.5433\n",
      "Epoch: 225, Loss: 0.6772, Train: 0.5871, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 226, Loss: 0.6797, Train: 0.5563, Val: 0.5700, Test: 0.5133\n",
      "Epoch: 227, Loss: 0.6829, Train: 0.5883, Val: 0.5833, Test: 0.5233\n",
      "Epoch: 228, Loss: 0.6437, Train: 0.5867, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 229, Loss: 0.6826, Train: 0.5754, Val: 0.5733, Test: 0.5300\n",
      "Epoch: 230, Loss: 0.6181, Train: 0.8075, Val: 0.7700, Test: 0.7833\n",
      "Epoch: 231, Loss: 0.6146, Train: 0.5846, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 232, Loss: 0.6820, Train: 0.5854, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 233, Loss: 0.6467, Train: 0.5867, Val: 0.5933, Test: 0.5400\n",
      "Epoch: 234, Loss: 0.6782, Train: 0.5871, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 235, Loss: 0.6815, Train: 0.5871, Val: 0.5933, Test: 0.5400\n",
      "Epoch: 236, Loss: 0.6838, Train: 0.5867, Val: 0.6000, Test: 0.5433\n",
      "Epoch: 237, Loss: 0.6810, Train: 0.5875, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 238, Loss: 0.6846, Train: 0.5833, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 239, Loss: 0.6819, Train: 0.5871, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 240, Loss: 0.6789, Train: 0.5863, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 241, Loss: 0.6344, Train: 0.5871, Val: 0.6033, Test: 0.5367\n",
      "Epoch: 242, Loss: 0.6047, Train: 0.5858, Val: 0.6033, Test: 0.5367\n",
      "Epoch: 243, Loss: 0.6730, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 244, Loss: 0.6788, Train: 0.5892, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 245, Loss: 0.6793, Train: 0.5867, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 246, Loss: 0.6502, Train: 0.5613, Val: 0.5833, Test: 0.5300\n",
      "Epoch: 247, Loss: 0.6751, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 248, Loss: 0.6796, Train: 0.5808, Val: 0.5967, Test: 0.5200\n",
      "Epoch: 249, Loss: 0.6806, Train: 0.5879, Val: 0.5967, Test: 0.5067\n",
      "Epoch: 250, Loss: 0.6793, Train: 0.5879, Val: 0.5967, Test: 0.5167\n",
      "Epoch: 251, Loss: 0.6725, Train: 0.5721, Val: 0.5733, Test: 0.5100\n",
      "Epoch: 252, Loss: 0.5658, Train: 0.7987, Val: 0.7733, Test: 0.7967\n",
      "Epoch: 253, Loss: 0.5618, Train: 0.5842, Val: 0.6067, Test: 0.5367\n",
      "Epoch: 254, Loss: 0.6535, Train: 0.5867, Val: 0.6000, Test: 0.5300\n",
      "Epoch: 255, Loss: 0.6763, Train: 0.5729, Val: 0.5833, Test: 0.5167\n",
      "Epoch: 256, Loss: 0.6720, Train: 0.5879, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 257, Loss: 0.6799, Train: 0.5821, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 258, Loss: 0.6782, Train: 0.5829, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 259, Loss: 0.6841, Train: 0.5842, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 260, Loss: 0.6798, Train: 0.5887, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 261, Loss: 0.6794, Train: 0.5892, Val: 0.5833, Test: 0.5200\n",
      "Epoch: 262, Loss: 0.6788, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 263, Loss: 0.6732, Train: 0.5871, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 264, Loss: 0.6536, Train: 0.5875, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 265, Loss: 0.6777, Train: 0.5875, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 266, Loss: 0.6771, Train: 0.5875, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 267, Loss: 0.6768, Train: 0.5867, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 268, Loss: 0.6823, Train: 0.5817, Val: 0.5867, Test: 0.5033\n",
      "Epoch: 269, Loss: 0.6778, Train: 0.5854, Val: 0.5967, Test: 0.5267\n",
      "Epoch: 270, Loss: 0.6763, Train: 0.5663, Val: 0.5933, Test: 0.5267\n",
      "Epoch: 271, Loss: 0.6738, Train: 0.5875, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 272, Loss: 0.6758, Train: 0.5867, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 273, Loss: 0.6789, Train: 0.5879, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 274, Loss: 0.6781, Train: 0.5804, Val: 0.6000, Test: 0.4967\n",
      "Epoch: 275, Loss: 0.6780, Train: 0.5867, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 276, Loss: 0.5455, Train: 0.8071, Val: 0.7733, Test: 0.7967\n",
      "Epoch: 277, Loss: 0.5701, Train: 0.5871, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 278, Loss: 0.6402, Train: 0.5875, Val: 0.6000, Test: 0.5467\n",
      "Epoch: 279, Loss: 0.6775, Train: 0.5829, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 280, Loss: 0.6762, Train: 0.5467, Val: 0.5267, Test: 0.5067\n",
      "Epoch: 281, Loss: 0.6772, Train: 0.5842, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 282, Loss: 0.6762, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 283, Loss: 0.6674, Train: 0.5863, Val: 0.5933, Test: 0.5333\n",
      "Epoch: 284, Loss: 0.6767, Train: 0.5879, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 285, Loss: 0.6763, Train: 0.5879, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 286, Loss: 0.6771, Train: 0.5854, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 287, Loss: 0.6733, Train: 0.5850, Val: 0.5967, Test: 0.5167\n",
      "Epoch: 288, Loss: 0.6769, Train: 0.5875, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 289, Loss: 0.6786, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 290, Loss: 0.6763, Train: 0.5842, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 291, Loss: 0.6759, Train: 0.5875, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 292, Loss: 0.6736, Train: 0.5875, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 293, Loss: 0.6761, Train: 0.5679, Val: 0.5833, Test: 0.5133\n",
      "Epoch: 294, Loss: 0.6767, Train: 0.5608, Val: 0.5833, Test: 0.5367\n",
      "Epoch: 295, Loss: 0.6752, Train: 0.5867, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 296, Loss: 0.6731, Train: 0.5875, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 297, Loss: 0.6733, Train: 0.5867, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 298, Loss: 0.6473, Train: 0.5879, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 299, Loss: 0.6063, Train: 0.5875, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 300, Loss: 0.6324, Train: 0.5883, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 301, Loss: 0.5730, Train: 0.8092, Val: 0.7867, Test: 0.8233\n",
      "Epoch: 302, Loss: 0.6285, Train: 0.5921, Val: 0.5800, Test: 0.5267\n",
      "Epoch: 303, Loss: 0.6790, Train: 0.5604, Val: 0.5633, Test: 0.5167\n",
      "Epoch: 304, Loss: 0.6763, Train: 0.5787, Val: 0.5933, Test: 0.5167\n",
      "Epoch: 305, Loss: 0.6759, Train: 0.5846, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 306, Loss: 0.6811, Train: 0.5846, Val: 0.6033, Test: 0.5367\n",
      "Epoch: 307, Loss: 0.6772, Train: 0.5842, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 308, Loss: 0.6700, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 309, Loss: 0.6762, Train: 0.5846, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 310, Loss: 0.6745, Train: 0.5875, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 311, Loss: 0.6737, Train: 0.5842, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 312, Loss: 0.6776, Train: 0.5842, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 313, Loss: 0.6754, Train: 0.5887, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 314, Loss: 0.6793, Train: 0.5504, Val: 0.5700, Test: 0.5233\n",
      "Epoch: 315, Loss: 0.6741, Train: 0.5837, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 316, Loss: 0.6772, Train: 0.5854, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 317, Loss: 0.6624, Train: 0.5883, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 318, Loss: 0.6019, Train: 0.8100, Val: 0.7933, Test: 0.8167\n",
      "Epoch: 319, Loss: 0.5159, Train: 0.8142, Val: 0.7900, Test: 0.8167\n",
      "Epoch: 320, Loss: 0.6582, Train: 0.5871, Val: 0.5933, Test: 0.5400\n",
      "Epoch: 321, Loss: 0.6814, Train: 0.5817, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 322, Loss: 0.6766, Train: 0.5550, Val: 0.5833, Test: 0.5300\n",
      "Epoch: 323, Loss: 0.6726, Train: 0.5858, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 324, Loss: 0.6790, Train: 0.5754, Val: 0.6100, Test: 0.5200\n",
      "Epoch: 325, Loss: 0.6763, Train: 0.5842, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 326, Loss: 0.6751, Train: 0.5875, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 327, Loss: 0.6758, Train: 0.5867, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 328, Loss: 0.6751, Train: 0.5879, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 329, Loss: 0.6569, Train: 0.5871, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 330, Loss: 0.6616, Train: 0.5879, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 331, Loss: 0.6773, Train: 0.5867, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 332, Loss: 0.6796, Train: 0.5871, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 333, Loss: 0.6800, Train: 0.5842, Val: 0.6067, Test: 0.5400\n",
      "Epoch: 334, Loss: 0.6768, Train: 0.5837, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 335, Loss: 0.6757, Train: 0.5837, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 336, Loss: 0.6767, Train: 0.5846, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 337, Loss: 0.6740, Train: 0.5846, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 338, Loss: 0.6755, Train: 0.5871, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 339, Loss: 0.6731, Train: 0.5863, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 340, Loss: 0.6733, Train: 0.5896, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 341, Loss: 0.6743, Train: 0.5883, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 342, Loss: 0.6771, Train: 0.5854, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 343, Loss: 0.6763, Train: 0.5671, Val: 0.5467, Test: 0.5233\n",
      "Epoch: 344, Loss: 0.6769, Train: 0.5850, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 345, Loss: 0.6744, Train: 0.5867, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 346, Loss: 0.6742, Train: 0.5846, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 347, Loss: 0.6741, Train: 0.5829, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 348, Loss: 0.6714, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 349, Loss: 0.6713, Train: 0.5850, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 350, Loss: 0.6732, Train: 0.5879, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 351, Loss: 0.6729, Train: 0.5908, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 352, Loss: 0.6665, Train: 0.5875, Val: 0.6067, Test: 0.5367\n",
      "Epoch: 353, Loss: 0.6644, Train: 0.5867, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 354, Loss: 0.6722, Train: 0.5879, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 355, Loss: 0.6689, Train: 0.5887, Val: 0.6033, Test: 0.5367\n",
      "Epoch: 356, Loss: 0.6664, Train: 0.5883, Val: 0.6067, Test: 0.5367\n",
      "Epoch: 357, Loss: 0.6521, Train: 0.5238, Val: 0.4967, Test: 0.5233\n",
      "Epoch: 358, Loss: 0.6769, Train: 0.5825, Val: 0.6033, Test: 0.5367\n",
      "Epoch: 359, Loss: 0.6747, Train: 0.5837, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 360, Loss: 0.6738, Train: 0.5837, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 361, Loss: 0.6755, Train: 0.5879, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 362, Loss: 0.6703, Train: 0.5858, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 363, Loss: 0.6711, Train: 0.5858, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 364, Loss: 0.6754, Train: 0.5875, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 365, Loss: 0.6742, Train: 0.5879, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 366, Loss: 0.6724, Train: 0.5892, Val: 0.5867, Test: 0.5333\n",
      "Epoch: 367, Loss: 0.6732, Train: 0.5879, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 368, Loss: 0.6726, Train: 0.5887, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 369, Loss: 0.6778, Train: 0.5896, Val: 0.6033, Test: 0.5300\n",
      "Epoch: 370, Loss: 0.6762, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 371, Loss: 0.6748, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 372, Loss: 0.6742, Train: 0.5900, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 373, Loss: 0.6780, Train: 0.5867, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 374, Loss: 0.6630, Train: 0.5883, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 375, Loss: 0.6599, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 376, Loss: 0.6617, Train: 0.5879, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 377, Loss: 0.6344, Train: 0.5900, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 378, Loss: 0.6269, Train: 0.8096, Val: 0.7767, Test: 0.8100\n",
      "Epoch: 379, Loss: 0.6307, Train: 0.5887, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 380, Loss: 0.5687, Train: 0.8229, Val: 0.7700, Test: 0.7933\n",
      "Epoch: 381, Loss: 0.5099, Train: 0.8183, Val: 0.7767, Test: 0.7933\n",
      "Epoch: 382, Loss: 0.5273, Train: 0.8208, Val: 0.7900, Test: 0.8167\n",
      "Epoch: 383, Loss: 0.4683, Train: 0.8279, Val: 0.7900, Test: 0.8100\n",
      "Epoch: 384, Loss: 0.5283, Train: 0.5821, Val: 0.5833, Test: 0.5300\n",
      "Epoch: 385, Loss: 0.6774, Train: 0.5071, Val: 0.5100, Test: 0.5400\n",
      "Epoch: 386, Loss: 0.6767, Train: 0.5854, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 387, Loss: 0.6792, Train: 0.5854, Val: 0.5967, Test: 0.5367\n",
      "Epoch: 388, Loss: 0.6764, Train: 0.5829, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 389, Loss: 0.6774, Train: 0.5887, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 390, Loss: 0.6758, Train: 0.5854, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 391, Loss: 0.6748, Train: 0.5833, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 392, Loss: 0.6756, Train: 0.5867, Val: 0.6067, Test: 0.5333\n",
      "Epoch: 393, Loss: 0.6771, Train: 0.5875, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 394, Loss: 0.6778, Train: 0.5875, Val: 0.6067, Test: 0.5333\n",
      "Epoch: 395, Loss: 0.6745, Train: 0.5842, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 396, Loss: 0.6743, Train: 0.5833, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 397, Loss: 0.6739, Train: 0.5837, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 398, Loss: 0.6770, Train: 0.5837, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 399, Loss: 0.6735, Train: 0.5850, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 400, Loss: 0.6777, Train: 0.5833, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 401, Loss: 0.6748, Train: 0.5850, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 402, Loss: 0.6740, Train: 0.5821, Val: 0.5967, Test: 0.5433\n",
      "Epoch: 403, Loss: 0.6779, Train: 0.5883, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 404, Loss: 0.6731, Train: 0.5883, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 405, Loss: 0.6733, Train: 0.5846, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 406, Loss: 0.6739, Train: 0.5879, Val: 0.5967, Test: 0.5300\n",
      "Epoch: 407, Loss: 0.6749, Train: 0.5771, Val: 0.5700, Test: 0.5267\n",
      "Epoch: 408, Loss: 0.6739, Train: 0.5850, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 409, Loss: 0.6725, Train: 0.5871, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 410, Loss: 0.6737, Train: 0.5858, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 411, Loss: 0.6744, Train: 0.5875, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 412, Loss: 0.6727, Train: 0.5863, Val: 0.5933, Test: 0.5400\n",
      "Epoch: 413, Loss: 0.6754, Train: 0.5871, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 414, Loss: 0.6732, Train: 0.5879, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 415, Loss: 0.6784, Train: 0.5883, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 416, Loss: 0.6754, Train: 0.5892, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 417, Loss: 0.6738, Train: 0.5863, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 418, Loss: 0.6687, Train: 0.5871, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 419, Loss: 0.6725, Train: 0.5883, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 420, Loss: 0.6716, Train: 0.5896, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 421, Loss: 0.6726, Train: 0.5875, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 422, Loss: 0.6750, Train: 0.5879, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 423, Loss: 0.6721, Train: 0.5875, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 424, Loss: 0.6716, Train: 0.5892, Val: 0.6067, Test: 0.5367\n",
      "Epoch: 425, Loss: 0.6735, Train: 0.5896, Val: 0.5967, Test: 0.5300\n",
      "Epoch: 426, Loss: 0.6717, Train: 0.5867, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 427, Loss: 0.6727, Train: 0.5879, Val: 0.6067, Test: 0.5367\n",
      "Epoch: 428, Loss: 0.6745, Train: 0.5854, Val: 0.6067, Test: 0.5433\n",
      "Epoch: 429, Loss: 0.6740, Train: 0.5921, Val: 0.5867, Test: 0.5300\n",
      "Epoch: 430, Loss: 0.6740, Train: 0.5879, Val: 0.6067, Test: 0.5367\n",
      "Epoch: 431, Loss: 0.6717, Train: 0.5875, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 432, Loss: 0.6748, Train: 0.5871, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 433, Loss: 0.6708, Train: 0.5771, Val: 0.5900, Test: 0.5300\n",
      "Epoch: 434, Loss: 0.6712, Train: 0.5887, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 435, Loss: 0.6704, Train: 0.5883, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 436, Loss: 0.6740, Train: 0.5900, Val: 0.5933, Test: 0.5400\n",
      "Epoch: 437, Loss: 0.6675, Train: 0.5896, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 438, Loss: 0.6737, Train: 0.5900, Val: 0.5933, Test: 0.5400\n",
      "Epoch: 439, Loss: 0.6675, Train: 0.5883, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 440, Loss: 0.6693, Train: 0.5896, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 441, Loss: 0.6740, Train: 0.5908, Val: 0.5867, Test: 0.5367\n",
      "Epoch: 442, Loss: 0.6713, Train: 0.5879, Val: 0.5867, Test: 0.5300\n",
      "Epoch: 443, Loss: 0.6742, Train: 0.5887, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 444, Loss: 0.6723, Train: 0.5887, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 445, Loss: 0.6720, Train: 0.5908, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 446, Loss: 0.6745, Train: 0.5887, Val: 0.6033, Test: 0.5333\n",
      "Epoch: 447, Loss: 0.6721, Train: 0.5900, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 448, Loss: 0.6741, Train: 0.5896, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 449, Loss: 0.6709, Train: 0.5887, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 450, Loss: 0.6713, Train: 0.5887, Val: 0.5933, Test: 0.5400\n",
      "Epoch: 451, Loss: 0.6701, Train: 0.5883, Val: 0.5867, Test: 0.5400\n",
      "Epoch: 452, Loss: 0.6717, Train: 0.5896, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 453, Loss: 0.6672, Train: 0.5892, Val: 0.5867, Test: 0.5267\n",
      "Epoch: 454, Loss: 0.6718, Train: 0.5879, Val: 0.6100, Test: 0.5367\n",
      "Epoch: 455, Loss: 0.6670, Train: 0.5892, Val: 0.6000, Test: 0.5333\n",
      "Epoch: 456, Loss: 0.6706, Train: 0.5875, Val: 0.5933, Test: 0.5333\n",
      "Epoch: 457, Loss: 0.6712, Train: 0.5896, Val: 0.5833, Test: 0.5300\n",
      "Epoch: 458, Loss: 0.6741, Train: 0.5871, Val: 0.6033, Test: 0.5400\n",
      "Epoch: 459, Loss: 0.6726, Train: 0.5867, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 460, Loss: 0.6704, Train: 0.5887, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 461, Loss: 0.6728, Train: 0.5896, Val: 0.5933, Test: 0.5233\n",
      "Epoch: 462, Loss: 0.6737, Train: 0.5875, Val: 0.6000, Test: 0.5300\n",
      "Epoch: 463, Loss: 0.6724, Train: 0.5708, Val: 0.5733, Test: 0.5167\n",
      "Epoch: 464, Loss: 0.6722, Train: 0.5879, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 465, Loss: 0.6721, Train: 0.5892, Val: 0.5900, Test: 0.5400\n",
      "Epoch: 466, Loss: 0.6734, Train: 0.5913, Val: 0.5900, Test: 0.5367\n",
      "Epoch: 467, Loss: 0.6691, Train: 0.5883, Val: 0.6033, Test: 0.5367\n",
      "Epoch: 468, Loss: 0.6712, Train: 0.5879, Val: 0.5900, Test: 0.5367\n",
      "Epoch: 469, Loss: 0.6707, Train: 0.5883, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 470, Loss: 0.6726, Train: 0.5900, Val: 0.5900, Test: 0.5367\n",
      "Epoch: 471, Loss: 0.6702, Train: 0.5883, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 472, Loss: 0.6703, Train: 0.5833, Val: 0.5933, Test: 0.5433\n",
      "Epoch: 473, Loss: 0.6710, Train: 0.5896, Val: 0.5867, Test: 0.5400\n",
      "Epoch: 474, Loss: 0.6716, Train: 0.5846, Val: 0.6067, Test: 0.5433\n",
      "Epoch: 475, Loss: 0.6724, Train: 0.5887, Val: 0.5900, Test: 0.5200\n",
      "Epoch: 476, Loss: 0.6752, Train: 0.5871, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 477, Loss: 0.6706, Train: 0.5883, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 478, Loss: 0.6698, Train: 0.5896, Val: 0.5933, Test: 0.5333\n",
      "Epoch: 479, Loss: 0.6665, Train: 0.5871, Val: 0.6033, Test: 0.5433\n",
      "Epoch: 480, Loss: 0.6739, Train: 0.5867, Val: 0.6100, Test: 0.5433\n",
      "Epoch: 481, Loss: 0.6700, Train: 0.5808, Val: 0.5767, Test: 0.5433\n",
      "Epoch: 482, Loss: 0.6735, Train: 0.5883, Val: 0.6000, Test: 0.5300\n",
      "Epoch: 483, Loss: 0.6707, Train: 0.5900, Val: 0.5900, Test: 0.5367\n",
      "Epoch: 484, Loss: 0.6714, Train: 0.5896, Val: 0.5867, Test: 0.5267\n",
      "Epoch: 485, Loss: 0.6734, Train: 0.5879, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 486, Loss: 0.6731, Train: 0.5904, Val: 0.5867, Test: 0.5400\n",
      "Epoch: 487, Loss: 0.6700, Train: 0.5908, Val: 0.5800, Test: 0.5300\n",
      "Epoch: 488, Loss: 0.6693, Train: 0.5900, Val: 0.5967, Test: 0.5333\n",
      "Epoch: 489, Loss: 0.6699, Train: 0.5900, Val: 0.5933, Test: 0.5367\n",
      "Epoch: 490, Loss: 0.6698, Train: 0.5908, Val: 0.5900, Test: 0.5400\n",
      "Epoch: 491, Loss: 0.6724, Train: 0.5879, Val: 0.6033, Test: 0.5367\n",
      "Epoch: 492, Loss: 0.6738, Train: 0.5908, Val: 0.5933, Test: 0.5300\n",
      "Epoch: 493, Loss: 0.6683, Train: 0.5875, Val: 0.6067, Test: 0.5433\n",
      "Epoch: 494, Loss: 0.6716, Train: 0.5904, Val: 0.5967, Test: 0.5300\n",
      "Epoch: 495, Loss: 0.6711, Train: 0.5887, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 496, Loss: 0.6719, Train: 0.5883, Val: 0.6000, Test: 0.5400\n",
      "Epoch: 497, Loss: 0.6716, Train: 0.5892, Val: 0.5967, Test: 0.5400\n",
      "Epoch: 498, Loss: 0.6739, Train: 0.5896, Val: 0.6000, Test: 0.5367\n",
      "Epoch: 499, Loss: 0.6697, Train: 0.5846, Val: 0.6100, Test: 0.5400\n",
      "Epoch: 500, Loss: 0.6736, Train: 0.5908, Val: 0.5900, Test: 0.5333\n",
      "### Run 0 - val loss: 0.474, test acc: 0.810\n",
      "Accuracies in each run:  [0.81]\n",
      "test acc - mean: 0.810, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'topk'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panpool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\un\\lib\\site-packages\\torch_sparse\\matmul.py:97: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:56.)\n",
      "  C = torch.sparse.mm(A, B)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7304, Train: 0.4863, Val: 0.4967, Test: 0.4833\n",
      "Epoch: 002, Loss: 0.7047, Train: 0.5258, Val: 0.5400, Test: 0.5033\n",
      "Epoch: 003, Loss: 0.7032, Train: 0.5029, Val: 0.4767, Test: 0.5200\n",
      "Epoch: 004, Loss: 0.7008, Train: 0.5483, Val: 0.5367, Test: 0.5600\n",
      "Epoch: 005, Loss: 0.6961, Train: 0.5429, Val: 0.5533, Test: 0.5233\n",
      "Epoch: 006, Loss: 0.6952, Train: 0.5475, Val: 0.5433, Test: 0.5433\n",
      "Epoch: 007, Loss: 0.6918, Train: 0.5421, Val: 0.5400, Test: 0.5367\n",
      "Epoch: 008, Loss: 0.6954, Train: 0.5504, Val: 0.5367, Test: 0.5567\n",
      "Epoch: 009, Loss: 0.6965, Train: 0.5554, Val: 0.5200, Test: 0.5533\n",
      "Epoch: 010, Loss: 0.6923, Train: 0.5212, Val: 0.5233, Test: 0.5267\n",
      "Epoch: 011, Loss: 0.6965, Train: 0.5396, Val: 0.5333, Test: 0.5367\n",
      "Epoch: 012, Loss: 0.6940, Train: 0.5567, Val: 0.5300, Test: 0.5667\n",
      "Epoch: 013, Loss: 0.6937, Train: 0.5463, Val: 0.5167, Test: 0.5100\n",
      "Epoch: 014, Loss: 0.6955, Train: 0.5088, Val: 0.5000, Test: 0.5167\n",
      "Epoch: 015, Loss: 0.6962, Train: 0.5500, Val: 0.5400, Test: 0.5500\n",
      "Epoch: 016, Loss: 0.6937, Train: 0.5592, Val: 0.5167, Test: 0.5433\n",
      "Epoch: 017, Loss: 0.6919, Train: 0.5292, Val: 0.5367, Test: 0.5300\n",
      "Epoch: 018, Loss: 0.6926, Train: 0.5563, Val: 0.5300, Test: 0.5633\n",
      "Epoch: 019, Loss: 0.6959, Train: 0.5433, Val: 0.5233, Test: 0.5500\n",
      "Epoch: 020, Loss: 0.6958, Train: 0.5550, Val: 0.5167, Test: 0.5633\n",
      "Epoch: 021, Loss: 0.6935, Train: 0.5592, Val: 0.5300, Test: 0.5633\n",
      "Epoch: 022, Loss: 0.6926, Train: 0.5517, Val: 0.5200, Test: 0.5267\n",
      "Epoch: 023, Loss: 0.6904, Train: 0.5292, Val: 0.5400, Test: 0.5167\n",
      "Epoch: 024, Loss: 0.6922, Train: 0.5312, Val: 0.5333, Test: 0.5333\n",
      "Epoch: 025, Loss: 0.6931, Train: 0.5283, Val: 0.5367, Test: 0.5267\n",
      "Epoch: 026, Loss: 0.6901, Train: 0.5413, Val: 0.5400, Test: 0.5400\n",
      "Epoch: 027, Loss: 0.6928, Train: 0.5329, Val: 0.5333, Test: 0.5300\n",
      "Epoch: 028, Loss: 0.6915, Train: 0.5179, Val: 0.5300, Test: 0.5267\n",
      "Epoch: 029, Loss: 0.6906, Train: 0.5525, Val: 0.5233, Test: 0.5767\n",
      "Epoch: 030, Loss: 0.6912, Train: 0.5212, Val: 0.5333, Test: 0.5300\n",
      "Epoch: 031, Loss: 0.6932, Train: 0.5329, Val: 0.5400, Test: 0.5333\n",
      "Epoch: 032, Loss: 0.6897, Train: 0.5575, Val: 0.5233, Test: 0.5700\n",
      "Epoch: 033, Loss: 0.6913, Train: 0.5587, Val: 0.5233, Test: 0.5700\n",
      "Epoch: 034, Loss: 0.6934, Train: 0.5117, Val: 0.4933, Test: 0.5233\n",
      "Epoch: 035, Loss: 0.6946, Train: 0.5142, Val: 0.5200, Test: 0.5167\n",
      "Epoch: 036, Loss: 0.6908, Train: 0.5400, Val: 0.5367, Test: 0.5433\n",
      "Epoch: 037, Loss: 0.6915, Train: 0.5521, Val: 0.5267, Test: 0.5567\n",
      "Epoch: 038, Loss: 0.6892, Train: 0.5442, Val: 0.5267, Test: 0.5400\n",
      "Epoch: 039, Loss: 0.6931, Train: 0.5162, Val: 0.5167, Test: 0.5200\n",
      "Epoch: 040, Loss: 0.6941, Train: 0.5533, Val: 0.5333, Test: 0.5567\n",
      "Epoch: 041, Loss: 0.6925, Train: 0.5112, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 042, Loss: 0.6917, Train: 0.5471, Val: 0.5333, Test: 0.5533\n",
      "Epoch: 043, Loss: 0.6926, Train: 0.5300, Val: 0.5333, Test: 0.5267\n",
      "Epoch: 044, Loss: 0.6912, Train: 0.5579, Val: 0.5233, Test: 0.5567\n",
      "Epoch: 045, Loss: 0.6907, Train: 0.5358, Val: 0.5400, Test: 0.5367\n",
      "Epoch: 046, Loss: 0.6908, Train: 0.5521, Val: 0.5300, Test: 0.5300\n",
      "Epoch: 047, Loss: 0.6909, Train: 0.5508, Val: 0.5233, Test: 0.5567\n",
      "Epoch: 048, Loss: 0.6926, Train: 0.5629, Val: 0.5233, Test: 0.5700\n",
      "Epoch: 049, Loss: 0.6900, Train: 0.5604, Val: 0.5200, Test: 0.5667\n",
      "Epoch: 050, Loss: 0.6914, Train: 0.5138, Val: 0.5200, Test: 0.5167\n",
      "Epoch: 051, Loss: 0.6881, Train: 0.5546, Val: 0.5200, Test: 0.5467\n",
      "Epoch: 052, Loss: 0.6927, Train: 0.5371, Val: 0.5400, Test: 0.5367\n",
      "Epoch: 053, Loss: 0.6906, Train: 0.5683, Val: 0.5133, Test: 0.5500\n",
      "Epoch: 054, Loss: 0.6903, Train: 0.5463, Val: 0.5233, Test: 0.5500\n",
      "Epoch: 055, Loss: 0.6915, Train: 0.5537, Val: 0.5067, Test: 0.5600\n",
      "Epoch: 056, Loss: 0.6904, Train: 0.5496, Val: 0.5300, Test: 0.5567\n",
      "Epoch: 057, Loss: 0.6919, Train: 0.5504, Val: 0.5200, Test: 0.5533\n",
      "Epoch: 058, Loss: 0.6894, Train: 0.5312, Val: 0.5300, Test: 0.5367\n",
      "Epoch: 059, Loss: 0.6899, Train: 0.5350, Val: 0.5367, Test: 0.4967\n",
      "Epoch: 060, Loss: 0.6913, Train: 0.5429, Val: 0.5433, Test: 0.5200\n",
      "Epoch: 061, Loss: 0.6903, Train: 0.5408, Val: 0.5400, Test: 0.5267\n",
      "Epoch: 062, Loss: 0.6885, Train: 0.5663, Val: 0.5200, Test: 0.5700\n",
      "Epoch: 063, Loss: 0.6882, Train: 0.5404, Val: 0.5300, Test: 0.5467\n",
      "Epoch: 064, Loss: 0.6887, Train: 0.5658, Val: 0.5267, Test: 0.5733\n",
      "Epoch: 065, Loss: 0.6899, Train: 0.5525, Val: 0.5333, Test: 0.5567\n",
      "Epoch: 066, Loss: 0.6877, Train: 0.5571, Val: 0.5200, Test: 0.5767\n",
      "Epoch: 067, Loss: 0.6880, Train: 0.5558, Val: 0.5033, Test: 0.5567\n",
      "Epoch: 068, Loss: 0.6878, Train: 0.5379, Val: 0.5433, Test: 0.5433\n",
      "Epoch: 069, Loss: 0.6884, Train: 0.5517, Val: 0.5300, Test: 0.5333\n",
      "Epoch: 070, Loss: 0.6902, Train: 0.5692, Val: 0.5267, Test: 0.5667\n",
      "Epoch: 071, Loss: 0.6908, Train: 0.5400, Val: 0.5367, Test: 0.5367\n",
      "Epoch: 072, Loss: 0.6887, Train: 0.5396, Val: 0.5333, Test: 0.5367\n",
      "Epoch: 073, Loss: 0.6896, Train: 0.5475, Val: 0.5400, Test: 0.5300\n",
      "Epoch: 074, Loss: 0.6872, Train: 0.5629, Val: 0.5200, Test: 0.5500\n",
      "Epoch: 075, Loss: 0.6886, Train: 0.5563, Val: 0.5167, Test: 0.5733\n",
      "Epoch: 076, Loss: 0.6852, Train: 0.5642, Val: 0.5200, Test: 0.5400\n",
      "Epoch: 077, Loss: 0.6864, Train: 0.5513, Val: 0.5267, Test: 0.5300\n",
      "Epoch: 078, Loss: 0.6873, Train: 0.5513, Val: 0.5300, Test: 0.5367\n",
      "Epoch: 079, Loss: 0.6890, Train: 0.5583, Val: 0.5100, Test: 0.5700\n",
      "Epoch: 080, Loss: 0.6890, Train: 0.5387, Val: 0.5400, Test: 0.5467\n",
      "Epoch: 081, Loss: 0.6875, Train: 0.5687, Val: 0.5167, Test: 0.5500\n",
      "Epoch: 082, Loss: 0.6885, Train: 0.5583, Val: 0.5200, Test: 0.5500\n",
      "Epoch: 083, Loss: 0.6882, Train: 0.5596, Val: 0.5267, Test: 0.5400\n",
      "Epoch: 084, Loss: 0.6832, Train: 0.5667, Val: 0.5200, Test: 0.5533\n",
      "Epoch: 085, Loss: 0.6876, Train: 0.5642, Val: 0.5300, Test: 0.5567\n",
      "Epoch: 086, Loss: 0.6870, Train: 0.5642, Val: 0.5500, Test: 0.5567\n",
      "Epoch: 087, Loss: 0.6868, Train: 0.5796, Val: 0.5400, Test: 0.5700\n",
      "Epoch: 088, Loss: 0.6837, Train: 0.5742, Val: 0.5300, Test: 0.5733\n",
      "Epoch: 089, Loss: 0.6849, Train: 0.5754, Val: 0.5333, Test: 0.5633\n",
      "Epoch: 090, Loss: 0.6835, Train: 0.5613, Val: 0.5533, Test: 0.5400\n",
      "Epoch: 091, Loss: 0.6860, Train: 0.5779, Val: 0.5300, Test: 0.5633\n",
      "Epoch: 092, Loss: 0.6807, Train: 0.5629, Val: 0.5333, Test: 0.5533\n",
      "Epoch: 093, Loss: 0.6865, Train: 0.5742, Val: 0.5500, Test: 0.5633\n",
      "Epoch: 094, Loss: 0.6820, Train: 0.5608, Val: 0.5400, Test: 0.5500\n",
      "Epoch: 095, Loss: 0.6829, Train: 0.5650, Val: 0.5533, Test: 0.5467\n",
      "Epoch: 096, Loss: 0.6801, Train: 0.5825, Val: 0.5467, Test: 0.5600\n",
      "Epoch: 097, Loss: 0.6839, Train: 0.5587, Val: 0.5133, Test: 0.5633\n",
      "Epoch: 098, Loss: 0.6858, Train: 0.5646, Val: 0.5367, Test: 0.5667\n",
      "Epoch: 099, Loss: 0.6775, Train: 0.5683, Val: 0.5333, Test: 0.5567\n",
      "Epoch: 100, Loss: 0.6819, Train: 0.5750, Val: 0.5567, Test: 0.5667\n",
      "Epoch: 101, Loss: 0.6806, Train: 0.5554, Val: 0.5467, Test: 0.5467\n",
      "Epoch: 102, Loss: 0.6836, Train: 0.5754, Val: 0.5433, Test: 0.5700\n",
      "Epoch: 103, Loss: 0.6831, Train: 0.5750, Val: 0.5400, Test: 0.5733\n",
      "Epoch: 104, Loss: 0.6811, Train: 0.5713, Val: 0.5500, Test: 0.5700\n",
      "Epoch: 105, Loss: 0.6839, Train: 0.5663, Val: 0.5267, Test: 0.5633\n",
      "Epoch: 106, Loss: 0.6837, Train: 0.5275, Val: 0.5000, Test: 0.5500\n",
      "Epoch: 107, Loss: 0.6804, Train: 0.5471, Val: 0.5167, Test: 0.5567\n",
      "Epoch: 108, Loss: 0.6851, Train: 0.5750, Val: 0.5433, Test: 0.5733\n",
      "Epoch: 109, Loss: 0.6832, Train: 0.5783, Val: 0.5500, Test: 0.5833\n",
      "Epoch: 110, Loss: 0.6840, Train: 0.5754, Val: 0.5400, Test: 0.5733\n",
      "Epoch: 111, Loss: 0.6851, Train: 0.5792, Val: 0.5533, Test: 0.5667\n",
      "Epoch: 112, Loss: 0.6838, Train: 0.5796, Val: 0.5467, Test: 0.5767\n",
      "Epoch: 113, Loss: 0.6770, Train: 0.5629, Val: 0.5100, Test: 0.5633\n",
      "Epoch: 114, Loss: 0.6812, Train: 0.5733, Val: 0.5500, Test: 0.5733\n",
      "Epoch: 115, Loss: 0.6780, Train: 0.5642, Val: 0.5467, Test: 0.5533\n",
      "Epoch: 116, Loss: 0.6800, Train: 0.5613, Val: 0.5267, Test: 0.5733\n",
      "Epoch: 117, Loss: 0.6796, Train: 0.5517, Val: 0.5467, Test: 0.5267\n",
      "Epoch: 118, Loss: 0.6817, Train: 0.5575, Val: 0.5267, Test: 0.5633\n",
      "Epoch: 119, Loss: 0.6795, Train: 0.5621, Val: 0.5267, Test: 0.5700\n",
      "Epoch: 120, Loss: 0.6790, Train: 0.5696, Val: 0.5533, Test: 0.5767\n",
      "Epoch: 121, Loss: 0.6810, Train: 0.5800, Val: 0.5500, Test: 0.5767\n",
      "Epoch: 122, Loss: 0.6768, Train: 0.5558, Val: 0.5200, Test: 0.5500\n",
      "Epoch: 123, Loss: 0.6807, Train: 0.5637, Val: 0.5433, Test: 0.5433\n",
      "Epoch: 124, Loss: 0.6782, Train: 0.5621, Val: 0.5367, Test: 0.5433\n",
      "Epoch: 125, Loss: 0.6789, Train: 0.5792, Val: 0.5467, Test: 0.5700\n",
      "Epoch: 126, Loss: 0.6775, Train: 0.5583, Val: 0.5333, Test: 0.5633\n",
      "Epoch: 127, Loss: 0.6792, Train: 0.5783, Val: 0.5500, Test: 0.5733\n",
      "Epoch: 128, Loss: 0.6854, Train: 0.5646, Val: 0.5400, Test: 0.5467\n",
      "Epoch: 129, Loss: 0.6804, Train: 0.5787, Val: 0.5467, Test: 0.5767\n",
      "Epoch: 130, Loss: 0.6783, Train: 0.5792, Val: 0.5533, Test: 0.5700\n",
      "Epoch: 131, Loss: 0.6781, Train: 0.5808, Val: 0.5633, Test: 0.5733\n",
      "Epoch: 132, Loss: 0.6814, Train: 0.5671, Val: 0.5333, Test: 0.5767\n",
      "Epoch: 133, Loss: 0.6759, Train: 0.5625, Val: 0.5167, Test: 0.5633\n",
      "Epoch: 134, Loss: 0.6847, Train: 0.5763, Val: 0.5333, Test: 0.5767\n",
      "Epoch: 135, Loss: 0.6790, Train: 0.5696, Val: 0.5367, Test: 0.5767\n",
      "Epoch: 136, Loss: 0.6759, Train: 0.5654, Val: 0.5233, Test: 0.5667\n",
      "Epoch: 137, Loss: 0.6762, Train: 0.5754, Val: 0.5433, Test: 0.5900\n",
      "Epoch: 138, Loss: 0.6778, Train: 0.5692, Val: 0.5333, Test: 0.5567\n",
      "Epoch: 139, Loss: 0.6785, Train: 0.5842, Val: 0.5500, Test: 0.5767\n",
      "Epoch: 140, Loss: 0.6782, Train: 0.5721, Val: 0.5500, Test: 0.5767\n",
      "Epoch: 141, Loss: 0.6734, Train: 0.5675, Val: 0.5600, Test: 0.5600\n",
      "Epoch: 142, Loss: 0.6830, Train: 0.5850, Val: 0.5567, Test: 0.5667\n",
      "Epoch: 143, Loss: 0.6771, Train: 0.5763, Val: 0.5433, Test: 0.5900\n",
      "Epoch: 144, Loss: 0.6743, Train: 0.5875, Val: 0.5533, Test: 0.5733\n",
      "Epoch: 145, Loss: 0.6769, Train: 0.5796, Val: 0.5600, Test: 0.5800\n",
      "Epoch: 146, Loss: 0.6792, Train: 0.5813, Val: 0.5400, Test: 0.5733\n",
      "Epoch: 147, Loss: 0.6781, Train: 0.5808, Val: 0.5500, Test: 0.5700\n",
      "Epoch: 148, Loss: 0.6849, Train: 0.5642, Val: 0.5433, Test: 0.5333\n",
      "Epoch: 149, Loss: 0.6770, Train: 0.5671, Val: 0.5667, Test: 0.5733\n",
      "Epoch: 150, Loss: 0.6800, Train: 0.5575, Val: 0.5333, Test: 0.5633\n",
      "Epoch: 151, Loss: 0.6777, Train: 0.5754, Val: 0.5533, Test: 0.5733\n",
      "Epoch: 152, Loss: 0.6779, Train: 0.5663, Val: 0.5733, Test: 0.5633\n",
      "Epoch: 153, Loss: 0.6771, Train: 0.5675, Val: 0.5333, Test: 0.5667\n",
      "Epoch: 154, Loss: 0.6750, Train: 0.5587, Val: 0.5200, Test: 0.5600\n",
      "Epoch: 155, Loss: 0.6763, Train: 0.5737, Val: 0.5400, Test: 0.5633\n",
      "Epoch: 156, Loss: 0.6780, Train: 0.5867, Val: 0.5533, Test: 0.5767\n",
      "Epoch: 157, Loss: 0.6815, Train: 0.5783, Val: 0.5367, Test: 0.6033\n",
      "Epoch: 158, Loss: 0.6751, Train: 0.5792, Val: 0.5467, Test: 0.5767\n",
      "Epoch: 159, Loss: 0.6754, Train: 0.5796, Val: 0.5433, Test: 0.5767\n",
      "Epoch: 160, Loss: 0.6767, Train: 0.5787, Val: 0.5567, Test: 0.5700\n",
      "Epoch: 161, Loss: 0.6728, Train: 0.5796, Val: 0.5400, Test: 0.5900\n",
      "Epoch: 162, Loss: 0.6809, Train: 0.5767, Val: 0.5467, Test: 0.5867\n",
      "Epoch: 163, Loss: 0.6766, Train: 0.5754, Val: 0.5500, Test: 0.5733\n",
      "Epoch: 164, Loss: 0.6791, Train: 0.5692, Val: 0.5633, Test: 0.5633\n",
      "Epoch: 165, Loss: 0.6751, Train: 0.5671, Val: 0.5200, Test: 0.5700\n",
      "Epoch: 166, Loss: 0.6790, Train: 0.5771, Val: 0.5433, Test: 0.5867\n",
      "Epoch: 167, Loss: 0.6785, Train: 0.5663, Val: 0.5333, Test: 0.5700\n",
      "Epoch: 168, Loss: 0.6783, Train: 0.5833, Val: 0.5433, Test: 0.5767\n",
      "Epoch: 169, Loss: 0.6778, Train: 0.5804, Val: 0.5533, Test: 0.5767\n",
      "Epoch: 170, Loss: 0.6743, Train: 0.5842, Val: 0.5567, Test: 0.5833\n",
      "Epoch: 171, Loss: 0.6821, Train: 0.5713, Val: 0.5333, Test: 0.5767\n",
      "Epoch: 172, Loss: 0.6768, Train: 0.5879, Val: 0.5533, Test: 0.5733\n",
      "Epoch: 173, Loss: 0.6733, Train: 0.5725, Val: 0.5467, Test: 0.5767\n",
      "Epoch: 174, Loss: 0.6797, Train: 0.5608, Val: 0.5200, Test: 0.5600\n",
      "Epoch: 175, Loss: 0.6786, Train: 0.5671, Val: 0.5533, Test: 0.5567\n",
      "Epoch: 176, Loss: 0.6769, Train: 0.5667, Val: 0.5200, Test: 0.5767\n",
      "Epoch: 177, Loss: 0.6763, Train: 0.5758, Val: 0.5433, Test: 0.5733\n",
      "Epoch: 178, Loss: 0.6764, Train: 0.5842, Val: 0.5367, Test: 0.5800\n",
      "Epoch: 179, Loss: 0.6742, Train: 0.5642, Val: 0.5233, Test: 0.5800\n",
      "Epoch: 180, Loss: 0.6757, Train: 0.5875, Val: 0.5567, Test: 0.5767\n",
      "Epoch: 181, Loss: 0.6744, Train: 0.5842, Val: 0.5467, Test: 0.5800\n",
      "Epoch: 182, Loss: 0.6758, Train: 0.5883, Val: 0.5633, Test: 0.5867\n",
      "Epoch: 183, Loss: 0.6780, Train: 0.5667, Val: 0.5567, Test: 0.5633\n",
      "Epoch: 184, Loss: 0.6762, Train: 0.5821, Val: 0.5567, Test: 0.5767\n",
      "Epoch: 185, Loss: 0.6769, Train: 0.5687, Val: 0.5700, Test: 0.5700\n",
      "Epoch: 186, Loss: 0.6782, Train: 0.5700, Val: 0.5600, Test: 0.5700\n",
      "Epoch: 187, Loss: 0.6789, Train: 0.5671, Val: 0.5467, Test: 0.5500\n",
      "Epoch: 188, Loss: 0.6797, Train: 0.5546, Val: 0.5100, Test: 0.5500\n",
      "Epoch: 189, Loss: 0.6725, Train: 0.5771, Val: 0.5433, Test: 0.5833\n",
      "Epoch: 190, Loss: 0.6724, Train: 0.5779, Val: 0.5467, Test: 0.5767\n",
      "Epoch: 191, Loss: 0.6789, Train: 0.5779, Val: 0.5533, Test: 0.5767\n",
      "Epoch: 192, Loss: 0.6783, Train: 0.5517, Val: 0.5067, Test: 0.5500\n",
      "Epoch: 193, Loss: 0.6789, Train: 0.5796, Val: 0.5433, Test: 0.5933\n",
      "Epoch: 194, Loss: 0.6777, Train: 0.5700, Val: 0.5467, Test: 0.5667\n",
      "Epoch: 195, Loss: 0.6772, Train: 0.5792, Val: 0.5367, Test: 0.5900\n",
      "Epoch: 196, Loss: 0.6757, Train: 0.5554, Val: 0.5433, Test: 0.5267\n",
      "Epoch: 197, Loss: 0.6784, Train: 0.5763, Val: 0.5400, Test: 0.5933\n",
      "Epoch: 198, Loss: 0.6750, Train: 0.5783, Val: 0.5633, Test: 0.5800\n",
      "Epoch: 199, Loss: 0.6748, Train: 0.5858, Val: 0.5567, Test: 0.5867\n",
      "Epoch: 200, Loss: 0.6788, Train: 0.5883, Val: 0.5600, Test: 0.5833\n",
      "Epoch: 201, Loss: 0.6741, Train: 0.5825, Val: 0.5533, Test: 0.5767\n",
      "Epoch: 202, Loss: 0.6768, Train: 0.5683, Val: 0.5800, Test: 0.5667\n",
      "Epoch: 203, Loss: 0.6796, Train: 0.5858, Val: 0.5467, Test: 0.5800\n",
      "Epoch: 204, Loss: 0.6799, Train: 0.5579, Val: 0.5367, Test: 0.5400\n",
      "Epoch: 205, Loss: 0.6793, Train: 0.5779, Val: 0.5667, Test: 0.5800\n",
      "Epoch: 206, Loss: 0.6765, Train: 0.5804, Val: 0.5433, Test: 0.5767\n",
      "Epoch: 207, Loss: 0.6758, Train: 0.5821, Val: 0.5633, Test: 0.5800\n",
      "Epoch: 208, Loss: 0.6759, Train: 0.5779, Val: 0.5700, Test: 0.5800\n",
      "Epoch: 209, Loss: 0.6800, Train: 0.5704, Val: 0.5433, Test: 0.5867\n",
      "Epoch: 210, Loss: 0.6797, Train: 0.5617, Val: 0.5300, Test: 0.5533\n",
      "Epoch: 211, Loss: 0.6772, Train: 0.5658, Val: 0.5433, Test: 0.5600\n",
      "Epoch: 212, Loss: 0.6750, Train: 0.5800, Val: 0.5400, Test: 0.5933\n",
      "Epoch: 213, Loss: 0.6791, Train: 0.5492, Val: 0.5433, Test: 0.5167\n",
      "Epoch: 214, Loss: 0.6812, Train: 0.5492, Val: 0.5433, Test: 0.5167\n",
      "Epoch: 215, Loss: 0.6775, Train: 0.5813, Val: 0.5400, Test: 0.5900\n",
      "Epoch: 216, Loss: 0.6774, Train: 0.5754, Val: 0.5667, Test: 0.5800\n",
      "Epoch: 217, Loss: 0.6784, Train: 0.5713, Val: 0.5600, Test: 0.5600\n",
      "Epoch: 218, Loss: 0.6746, Train: 0.5821, Val: 0.5533, Test: 0.5767\n",
      "Epoch: 219, Loss: 0.6733, Train: 0.5850, Val: 0.5500, Test: 0.5767\n",
      "Epoch: 220, Loss: 0.6728, Train: 0.5696, Val: 0.5300, Test: 0.5900\n",
      "Epoch: 221, Loss: 0.6763, Train: 0.5808, Val: 0.5333, Test: 0.5933\n",
      "Epoch: 222, Loss: 0.6743, Train: 0.5887, Val: 0.5533, Test: 0.5900\n",
      "Epoch: 223, Loss: 0.6768, Train: 0.5846, Val: 0.5300, Test: 0.5800\n",
      "Epoch: 224, Loss: 0.6766, Train: 0.5779, Val: 0.5467, Test: 0.5700\n",
      "Epoch: 225, Loss: 0.6764, Train: 0.5825, Val: 0.5500, Test: 0.5767\n",
      "Epoch: 226, Loss: 0.6796, Train: 0.5846, Val: 0.5567, Test: 0.5833\n",
      "Epoch: 227, Loss: 0.6777, Train: 0.5817, Val: 0.5600, Test: 0.5833\n",
      "Epoch: 228, Loss: 0.6760, Train: 0.5817, Val: 0.5533, Test: 0.5800\n",
      "Epoch: 229, Loss: 0.6747, Train: 0.5863, Val: 0.5467, Test: 0.5800\n",
      "Epoch: 230, Loss: 0.6760, Train: 0.5808, Val: 0.5433, Test: 0.6000\n",
      "Epoch: 231, Loss: 0.6781, Train: 0.5863, Val: 0.5367, Test: 0.5800\n",
      "Epoch: 232, Loss: 0.6799, Train: 0.5825, Val: 0.5367, Test: 0.6067\n",
      "Epoch: 233, Loss: 0.6777, Train: 0.5758, Val: 0.5633, Test: 0.5667\n",
      "Epoch: 234, Loss: 0.6779, Train: 0.5479, Val: 0.5467, Test: 0.5167\n",
      "Epoch: 235, Loss: 0.6747, Train: 0.5796, Val: 0.5467, Test: 0.5867\n",
      "Epoch: 236, Loss: 0.6759, Train: 0.5800, Val: 0.5400, Test: 0.5833\n",
      "Epoch: 237, Loss: 0.6762, Train: 0.5642, Val: 0.5367, Test: 0.5500\n",
      "Epoch: 238, Loss: 0.6754, Train: 0.5737, Val: 0.5633, Test: 0.5800\n",
      "Epoch: 239, Loss: 0.6753, Train: 0.5692, Val: 0.5533, Test: 0.5567\n",
      "Epoch: 240, Loss: 0.6753, Train: 0.5825, Val: 0.5300, Test: 0.5900\n",
      "Epoch: 241, Loss: 0.6757, Train: 0.5846, Val: 0.5433, Test: 0.5867\n",
      "Epoch: 242, Loss: 0.6744, Train: 0.5850, Val: 0.5400, Test: 0.5867\n",
      "Epoch: 243, Loss: 0.6715, Train: 0.5771, Val: 0.5433, Test: 0.6033\n",
      "Epoch: 244, Loss: 0.6718, Train: 0.5767, Val: 0.5733, Test: 0.5867\n",
      "Epoch: 245, Loss: 0.6733, Train: 0.5771, Val: 0.5400, Test: 0.5800\n",
      "Epoch: 246, Loss: 0.6738, Train: 0.5854, Val: 0.5567, Test: 0.5833\n",
      "Epoch: 247, Loss: 0.6759, Train: 0.5817, Val: 0.5667, Test: 0.5767\n",
      "Epoch: 248, Loss: 0.6772, Train: 0.5846, Val: 0.5433, Test: 0.5900\n",
      "Epoch: 249, Loss: 0.6729, Train: 0.5654, Val: 0.5333, Test: 0.5800\n",
      "Epoch: 250, Loss: 0.6765, Train: 0.5837, Val: 0.5467, Test: 0.5933\n",
      "Epoch: 251, Loss: 0.6735, Train: 0.5725, Val: 0.5400, Test: 0.5833\n",
      "Epoch: 252, Loss: 0.6739, Train: 0.5763, Val: 0.5667, Test: 0.5700\n",
      "Epoch: 253, Loss: 0.6736, Train: 0.5692, Val: 0.5400, Test: 0.5800\n",
      "Epoch: 254, Loss: 0.6744, Train: 0.5854, Val: 0.5300, Test: 0.5833\n",
      "Epoch: 255, Loss: 0.6785, Train: 0.5771, Val: 0.5700, Test: 0.5700\n",
      "Epoch: 256, Loss: 0.6772, Train: 0.5596, Val: 0.5433, Test: 0.5300\n",
      "Epoch: 257, Loss: 0.6735, Train: 0.5804, Val: 0.5500, Test: 0.5833\n",
      "Epoch: 258, Loss: 0.6740, Train: 0.5763, Val: 0.5433, Test: 0.5800\n",
      "Epoch: 259, Loss: 0.6820, Train: 0.5854, Val: 0.5300, Test: 0.5900\n",
      "Epoch: 260, Loss: 0.6756, Train: 0.5792, Val: 0.5567, Test: 0.5833\n",
      "Epoch: 261, Loss: 0.6725, Train: 0.5713, Val: 0.5467, Test: 0.5967\n",
      "Epoch: 262, Loss: 0.6706, Train: 0.5775, Val: 0.5533, Test: 0.5767\n",
      "Epoch: 263, Loss: 0.6777, Train: 0.5671, Val: 0.5367, Test: 0.5533\n",
      "Epoch: 264, Loss: 0.6741, Train: 0.5854, Val: 0.5400, Test: 0.5900\n",
      "Epoch: 265, Loss: 0.6770, Train: 0.5842, Val: 0.5333, Test: 0.5800\n",
      "Epoch: 266, Loss: 0.6769, Train: 0.5700, Val: 0.5533, Test: 0.5600\n",
      "Epoch: 267, Loss: 0.6767, Train: 0.5771, Val: 0.5533, Test: 0.5833\n",
      "Epoch: 268, Loss: 0.6785, Train: 0.5437, Val: 0.5500, Test: 0.4933\n",
      "Epoch: 269, Loss: 0.6735, Train: 0.5842, Val: 0.5467, Test: 0.5967\n",
      "Epoch: 270, Loss: 0.6756, Train: 0.5854, Val: 0.5300, Test: 0.5833\n",
      "Epoch: 271, Loss: 0.6740, Train: 0.5808, Val: 0.5633, Test: 0.5867\n",
      "Epoch: 272, Loss: 0.6777, Train: 0.5863, Val: 0.5267, Test: 0.5900\n",
      "Epoch: 273, Loss: 0.6793, Train: 0.5858, Val: 0.5467, Test: 0.5900\n",
      "Epoch: 274, Loss: 0.6748, Train: 0.5863, Val: 0.5500, Test: 0.5933\n",
      "Epoch: 275, Loss: 0.6778, Train: 0.5817, Val: 0.5600, Test: 0.5800\n",
      "Epoch: 276, Loss: 0.6742, Train: 0.5750, Val: 0.5600, Test: 0.5733\n",
      "Epoch: 277, Loss: 0.6765, Train: 0.5796, Val: 0.5600, Test: 0.5833\n",
      "Epoch: 278, Loss: 0.6741, Train: 0.5813, Val: 0.5467, Test: 0.5867\n",
      "Epoch: 279, Loss: 0.6723, Train: 0.5787, Val: 0.5600, Test: 0.5867\n",
      "Epoch: 280, Loss: 0.6735, Train: 0.5763, Val: 0.5700, Test: 0.5733\n",
      "Epoch: 281, Loss: 0.6716, Train: 0.5787, Val: 0.5700, Test: 0.5767\n",
      "Epoch: 282, Loss: 0.6778, Train: 0.5687, Val: 0.5600, Test: 0.5667\n",
      "Epoch: 283, Loss: 0.6729, Train: 0.5871, Val: 0.5367, Test: 0.5867\n",
      "Epoch: 284, Loss: 0.6758, Train: 0.5858, Val: 0.5467, Test: 0.5933\n",
      "Epoch: 285, Loss: 0.6753, Train: 0.5883, Val: 0.5333, Test: 0.5867\n",
      "Epoch: 286, Loss: 0.6772, Train: 0.5837, Val: 0.5500, Test: 0.5900\n",
      "Epoch: 287, Loss: 0.6757, Train: 0.5863, Val: 0.5400, Test: 0.5900\n",
      "Epoch: 288, Loss: 0.6718, Train: 0.5654, Val: 0.5400, Test: 0.5700\n",
      "Epoch: 289, Loss: 0.6768, Train: 0.5796, Val: 0.5467, Test: 0.5900\n",
      "Epoch: 290, Loss: 0.6713, Train: 0.5871, Val: 0.5267, Test: 0.5833\n",
      "Epoch: 291, Loss: 0.6729, Train: 0.5842, Val: 0.5567, Test: 0.5867\n",
      "Epoch: 292, Loss: 0.6748, Train: 0.5796, Val: 0.5567, Test: 0.5833\n",
      "Epoch: 293, Loss: 0.6737, Train: 0.5675, Val: 0.5233, Test: 0.5733\n",
      "Epoch: 294, Loss: 0.6763, Train: 0.5729, Val: 0.5633, Test: 0.5733\n",
      "Epoch: 295, Loss: 0.6731, Train: 0.5721, Val: 0.5700, Test: 0.5667\n",
      "Epoch: 296, Loss: 0.6740, Train: 0.5742, Val: 0.5667, Test: 0.5700\n",
      "Epoch: 297, Loss: 0.6730, Train: 0.5663, Val: 0.5433, Test: 0.5600\n",
      "Epoch: 298, Loss: 0.6752, Train: 0.5687, Val: 0.5267, Test: 0.5933\n",
      "Epoch: 299, Loss: 0.6763, Train: 0.5825, Val: 0.5467, Test: 0.5867\n",
      "Epoch: 300, Loss: 0.6734, Train: 0.5833, Val: 0.5500, Test: 0.5933\n",
      "Epoch: 301, Loss: 0.6742, Train: 0.5896, Val: 0.5433, Test: 0.5967\n",
      "Epoch: 302, Loss: 0.6728, Train: 0.5646, Val: 0.5433, Test: 0.5500\n",
      "Epoch: 303, Loss: 0.6750, Train: 0.5863, Val: 0.5400, Test: 0.5933\n",
      "Epoch: 304, Loss: 0.6718, Train: 0.5758, Val: 0.5433, Test: 0.5833\n",
      "Epoch: 305, Loss: 0.6740, Train: 0.5863, Val: 0.5367, Test: 0.5967\n",
      "Epoch: 306, Loss: 0.6745, Train: 0.5887, Val: 0.5333, Test: 0.5933\n",
      "Epoch: 307, Loss: 0.6729, Train: 0.5850, Val: 0.5467, Test: 0.5967\n",
      "Epoch: 308, Loss: 0.6750, Train: 0.5883, Val: 0.5367, Test: 0.5933\n",
      "Epoch: 309, Loss: 0.6722, Train: 0.5787, Val: 0.5733, Test: 0.5667\n",
      "Epoch: 310, Loss: 0.6745, Train: 0.5871, Val: 0.5367, Test: 0.5833\n",
      "Epoch: 311, Loss: 0.6722, Train: 0.5858, Val: 0.5333, Test: 0.5800\n",
      "Epoch: 312, Loss: 0.6753, Train: 0.5863, Val: 0.5400, Test: 0.5900\n",
      "Epoch: 313, Loss: 0.6720, Train: 0.5892, Val: 0.5433, Test: 0.5933\n",
      "Epoch: 314, Loss: 0.6752, Train: 0.5875, Val: 0.5400, Test: 0.6033\n",
      "Epoch: 315, Loss: 0.6714, Train: 0.5908, Val: 0.5267, Test: 0.6067\n",
      "Epoch: 316, Loss: 0.6755, Train: 0.5771, Val: 0.5567, Test: 0.5567\n",
      "Epoch: 317, Loss: 0.6741, Train: 0.5875, Val: 0.5367, Test: 0.5900\n",
      "Epoch: 318, Loss: 0.6731, Train: 0.5775, Val: 0.5700, Test: 0.5833\n",
      "Epoch: 319, Loss: 0.6755, Train: 0.5883, Val: 0.5433, Test: 0.6000\n",
      "Epoch: 320, Loss: 0.6759, Train: 0.5787, Val: 0.5400, Test: 0.6033\n",
      "Epoch: 321, Loss: 0.6723, Train: 0.5737, Val: 0.5400, Test: 0.6000\n",
      "Epoch: 322, Loss: 0.6739, Train: 0.5821, Val: 0.5500, Test: 0.5967\n",
      "Epoch: 323, Loss: 0.6717, Train: 0.5813, Val: 0.5567, Test: 0.5933\n",
      "Epoch: 324, Loss: 0.6760, Train: 0.5379, Val: 0.5467, Test: 0.5000\n",
      "Epoch: 325, Loss: 0.6714, Train: 0.5887, Val: 0.5400, Test: 0.5933\n",
      "Epoch: 326, Loss: 0.6748, Train: 0.5883, Val: 0.5300, Test: 0.6033\n",
      "Epoch: 327, Loss: 0.6725, Train: 0.5629, Val: 0.5300, Test: 0.5500\n",
      "Epoch: 328, Loss: 0.6713, Train: 0.5846, Val: 0.5500, Test: 0.5867\n",
      "Epoch: 329, Loss: 0.6771, Train: 0.5883, Val: 0.5500, Test: 0.6000\n",
      "Epoch: 330, Loss: 0.6738, Train: 0.5737, Val: 0.5433, Test: 0.6033\n",
      "Epoch: 331, Loss: 0.6715, Train: 0.5746, Val: 0.5700, Test: 0.5767\n",
      "Epoch: 332, Loss: 0.6744, Train: 0.5825, Val: 0.5667, Test: 0.5700\n",
      "Epoch: 333, Loss: 0.6762, Train: 0.5800, Val: 0.5600, Test: 0.5900\n",
      "Epoch: 334, Loss: 0.6750, Train: 0.5813, Val: 0.5700, Test: 0.5900\n",
      "Epoch: 335, Loss: 0.6741, Train: 0.5513, Val: 0.5633, Test: 0.5133\n",
      "Epoch: 336, Loss: 0.6739, Train: 0.5858, Val: 0.5467, Test: 0.5967\n",
      "Epoch: 337, Loss: 0.6781, Train: 0.5550, Val: 0.5467, Test: 0.5333\n",
      "Epoch: 338, Loss: 0.6730, Train: 0.5808, Val: 0.5533, Test: 0.5967\n",
      "Epoch: 339, Loss: 0.6717, Train: 0.5850, Val: 0.5433, Test: 0.5967\n",
      "Epoch: 340, Loss: 0.6713, Train: 0.5908, Val: 0.5500, Test: 0.5867\n",
      "Epoch: 341, Loss: 0.6741, Train: 0.5829, Val: 0.5500, Test: 0.5967\n",
      "Epoch: 342, Loss: 0.6710, Train: 0.5908, Val: 0.5433, Test: 0.6033\n",
      "Epoch: 343, Loss: 0.6748, Train: 0.5733, Val: 0.5467, Test: 0.5533\n",
      "Epoch: 344, Loss: 0.6737, Train: 0.5896, Val: 0.5367, Test: 0.5900\n",
      "Epoch: 345, Loss: 0.6730, Train: 0.5850, Val: 0.5533, Test: 0.5933\n",
      "Epoch: 346, Loss: 0.6722, Train: 0.5696, Val: 0.5500, Test: 0.5500\n",
      "Epoch: 347, Loss: 0.6725, Train: 0.5700, Val: 0.5433, Test: 0.5933\n",
      "Epoch: 348, Loss: 0.6759, Train: 0.5883, Val: 0.5500, Test: 0.5967\n",
      "Epoch: 349, Loss: 0.6708, Train: 0.5887, Val: 0.5467, Test: 0.6033\n",
      "Epoch: 350, Loss: 0.6731, Train: 0.5887, Val: 0.5500, Test: 0.6033\n",
      "Epoch: 351, Loss: 0.6715, Train: 0.5825, Val: 0.5767, Test: 0.5833\n",
      "Epoch: 352, Loss: 0.6721, Train: 0.5792, Val: 0.5400, Test: 0.6100\n",
      "Epoch: 353, Loss: 0.6756, Train: 0.5800, Val: 0.5567, Test: 0.5900\n",
      "Epoch: 354, Loss: 0.6742, Train: 0.5800, Val: 0.5467, Test: 0.6100\n",
      "Epoch: 355, Loss: 0.6743, Train: 0.5771, Val: 0.5533, Test: 0.5967\n",
      "Epoch: 356, Loss: 0.6712, Train: 0.5875, Val: 0.5600, Test: 0.6033\n",
      "Epoch: 357, Loss: 0.6758, Train: 0.5800, Val: 0.5367, Test: 0.6067\n",
      "Epoch: 358, Loss: 0.6728, Train: 0.5808, Val: 0.5700, Test: 0.5633\n",
      "Epoch: 359, Loss: 0.6724, Train: 0.5833, Val: 0.5733, Test: 0.5800\n",
      "Epoch: 360, Loss: 0.6712, Train: 0.5671, Val: 0.5500, Test: 0.5800\n",
      "Epoch: 361, Loss: 0.6742, Train: 0.5921, Val: 0.5400, Test: 0.6033\n",
      "Epoch: 362, Loss: 0.6701, Train: 0.5808, Val: 0.5433, Test: 0.6133\n",
      "Epoch: 363, Loss: 0.6719, Train: 0.5825, Val: 0.5500, Test: 0.6133\n",
      "Epoch: 364, Loss: 0.6696, Train: 0.5783, Val: 0.5733, Test: 0.5667\n",
      "Epoch: 365, Loss: 0.6749, Train: 0.5692, Val: 0.5467, Test: 0.6000\n",
      "Epoch: 366, Loss: 0.6708, Train: 0.5904, Val: 0.5533, Test: 0.5967\n",
      "Epoch: 367, Loss: 0.6755, Train: 0.5825, Val: 0.5667, Test: 0.5567\n",
      "Epoch: 368, Loss: 0.6732, Train: 0.5804, Val: 0.5600, Test: 0.5933\n",
      "Epoch: 369, Loss: 0.6778, Train: 0.5767, Val: 0.5467, Test: 0.5967\n",
      "Epoch: 370, Loss: 0.6751, Train: 0.5783, Val: 0.5567, Test: 0.5933\n",
      "Epoch: 371, Loss: 0.6723, Train: 0.5787, Val: 0.5567, Test: 0.5900\n",
      "Epoch: 372, Loss: 0.6714, Train: 0.5879, Val: 0.5533, Test: 0.6000\n",
      "Epoch: 373, Loss: 0.6712, Train: 0.5713, Val: 0.5667, Test: 0.5467\n",
      "Epoch: 374, Loss: 0.6700, Train: 0.5796, Val: 0.5533, Test: 0.5900\n",
      "Epoch: 375, Loss: 0.6760, Train: 0.5900, Val: 0.5433, Test: 0.6000\n",
      "Epoch: 376, Loss: 0.6718, Train: 0.5917, Val: 0.5567, Test: 0.5833\n",
      "Epoch: 377, Loss: 0.6722, Train: 0.5863, Val: 0.5600, Test: 0.5900\n",
      "Epoch: 378, Loss: 0.6782, Train: 0.5783, Val: 0.5600, Test: 0.5900\n",
      "Epoch: 379, Loss: 0.6729, Train: 0.5813, Val: 0.5533, Test: 0.5933\n",
      "Epoch: 380, Loss: 0.6772, Train: 0.5833, Val: 0.5433, Test: 0.6033\n",
      "Epoch: 381, Loss: 0.6731, Train: 0.5787, Val: 0.5633, Test: 0.5933\n",
      "Epoch: 382, Loss: 0.6734, Train: 0.5779, Val: 0.5700, Test: 0.5767\n",
      "Epoch: 383, Loss: 0.6732, Train: 0.5875, Val: 0.5433, Test: 0.6100\n",
      "Epoch: 384, Loss: 0.6740, Train: 0.5958, Val: 0.5400, Test: 0.5967\n",
      "Epoch: 385, Loss: 0.6718, Train: 0.5917, Val: 0.5367, Test: 0.6033\n",
      "Epoch: 386, Loss: 0.6720, Train: 0.5833, Val: 0.5767, Test: 0.5600\n",
      "Epoch: 387, Loss: 0.6740, Train: 0.5617, Val: 0.5267, Test: 0.5667\n",
      "Epoch: 388, Loss: 0.6746, Train: 0.5700, Val: 0.5467, Test: 0.5833\n",
      "Epoch: 389, Loss: 0.6744, Train: 0.5896, Val: 0.5533, Test: 0.6133\n",
      "Epoch: 390, Loss: 0.6765, Train: 0.5913, Val: 0.5500, Test: 0.5933\n",
      "Epoch: 391, Loss: 0.6716, Train: 0.5733, Val: 0.5767, Test: 0.5533\n",
      "Epoch: 392, Loss: 0.6691, Train: 0.5737, Val: 0.5767, Test: 0.5467\n",
      "Epoch: 393, Loss: 0.6717, Train: 0.5800, Val: 0.5533, Test: 0.5800\n",
      "Epoch: 394, Loss: 0.6722, Train: 0.5879, Val: 0.5533, Test: 0.6033\n",
      "Epoch: 395, Loss: 0.6728, Train: 0.5896, Val: 0.5367, Test: 0.5967\n",
      "Epoch: 396, Loss: 0.6748, Train: 0.5825, Val: 0.5833, Test: 0.5700\n",
      "Epoch: 397, Loss: 0.6752, Train: 0.5808, Val: 0.5633, Test: 0.5933\n",
      "Epoch: 398, Loss: 0.6701, Train: 0.5837, Val: 0.5367, Test: 0.5933\n",
      "Epoch: 399, Loss: 0.6733, Train: 0.5917, Val: 0.5500, Test: 0.6067\n",
      "Epoch: 400, Loss: 0.6692, Train: 0.5846, Val: 0.5433, Test: 0.6200\n",
      "Epoch: 401, Loss: 0.6759, Train: 0.5713, Val: 0.5467, Test: 0.5933\n",
      "Epoch: 402, Loss: 0.6743, Train: 0.5808, Val: 0.5733, Test: 0.5767\n",
      "Epoch: 403, Loss: 0.6714, Train: 0.5837, Val: 0.5567, Test: 0.6233\n",
      "Epoch: 404, Loss: 0.6748, Train: 0.5867, Val: 0.5500, Test: 0.6000\n",
      "Epoch: 405, Loss: 0.6712, Train: 0.5675, Val: 0.5433, Test: 0.5867\n",
      "Epoch: 406, Loss: 0.6743, Train: 0.5792, Val: 0.5467, Test: 0.6000\n",
      "Epoch: 407, Loss: 0.6744, Train: 0.5779, Val: 0.5800, Test: 0.5500\n",
      "Epoch: 408, Loss: 0.6769, Train: 0.5775, Val: 0.5400, Test: 0.6133\n",
      "Epoch: 409, Loss: 0.6727, Train: 0.5763, Val: 0.5533, Test: 0.6033\n",
      "Epoch: 410, Loss: 0.6723, Train: 0.5925, Val: 0.5500, Test: 0.6000\n",
      "Epoch: 411, Loss: 0.6734, Train: 0.5871, Val: 0.5333, Test: 0.6100\n",
      "Epoch: 412, Loss: 0.6712, Train: 0.5854, Val: 0.5367, Test: 0.5967\n",
      "Epoch: 413, Loss: 0.6701, Train: 0.5854, Val: 0.5500, Test: 0.6267\n",
      "Epoch: 414, Loss: 0.6716, Train: 0.5783, Val: 0.5533, Test: 0.6167\n",
      "Epoch: 415, Loss: 0.6701, Train: 0.5887, Val: 0.5467, Test: 0.6233\n",
      "Epoch: 416, Loss: 0.6697, Train: 0.5825, Val: 0.5500, Test: 0.6200\n",
      "Epoch: 417, Loss: 0.6688, Train: 0.5817, Val: 0.5800, Test: 0.5933\n",
      "Epoch: 418, Loss: 0.6712, Train: 0.5763, Val: 0.5833, Test: 0.5467\n",
      "Epoch: 419, Loss: 0.6730, Train: 0.5821, Val: 0.5667, Test: 0.5967\n",
      "Epoch: 420, Loss: 0.6688, Train: 0.5887, Val: 0.5567, Test: 0.6000\n",
      "Epoch: 421, Loss: 0.6675, Train: 0.5800, Val: 0.5433, Test: 0.6067\n",
      "Epoch: 422, Loss: 0.6696, Train: 0.5900, Val: 0.5533, Test: 0.6000\n",
      "Epoch: 423, Loss: 0.6683, Train: 0.5833, Val: 0.5433, Test: 0.5967\n",
      "Epoch: 424, Loss: 0.6721, Train: 0.5950, Val: 0.5500, Test: 0.6133\n",
      "Epoch: 425, Loss: 0.6702, Train: 0.5663, Val: 0.5433, Test: 0.5200\n",
      "Epoch: 426, Loss: 0.6742, Train: 0.5825, Val: 0.5700, Test: 0.5833\n",
      "Epoch: 427, Loss: 0.6746, Train: 0.5808, Val: 0.5467, Test: 0.6133\n",
      "Epoch: 428, Loss: 0.6728, Train: 0.5883, Val: 0.5567, Test: 0.6033\n",
      "Epoch: 429, Loss: 0.6756, Train: 0.5813, Val: 0.5600, Test: 0.6067\n",
      "Epoch: 430, Loss: 0.6737, Train: 0.5808, Val: 0.5667, Test: 0.6000\n",
      "Epoch: 431, Loss: 0.6687, Train: 0.5867, Val: 0.5500, Test: 0.6167\n",
      "Epoch: 432, Loss: 0.6691, Train: 0.5817, Val: 0.5567, Test: 0.5733\n",
      "Epoch: 433, Loss: 0.6732, Train: 0.5808, Val: 0.5567, Test: 0.6100\n",
      "Epoch: 434, Loss: 0.6751, Train: 0.5750, Val: 0.5633, Test: 0.5367\n",
      "Epoch: 435, Loss: 0.6670, Train: 0.5596, Val: 0.5333, Test: 0.5133\n",
      "Epoch: 436, Loss: 0.6720, Train: 0.5687, Val: 0.5733, Test: 0.5433\n",
      "Epoch: 437, Loss: 0.6729, Train: 0.5896, Val: 0.5433, Test: 0.5967\n",
      "Epoch: 438, Loss: 0.6734, Train: 0.5867, Val: 0.5667, Test: 0.6033\n",
      "Epoch: 439, Loss: 0.6696, Train: 0.5783, Val: 0.5667, Test: 0.5767\n",
      "Epoch: 440, Loss: 0.6723, Train: 0.5879, Val: 0.5567, Test: 0.5833\n",
      "Epoch: 441, Loss: 0.6705, Train: 0.5808, Val: 0.5500, Test: 0.6033\n",
      "Epoch: 442, Loss: 0.6708, Train: 0.5821, Val: 0.5567, Test: 0.6067\n",
      "Epoch: 443, Loss: 0.6691, Train: 0.5787, Val: 0.5700, Test: 0.5967\n",
      "Epoch: 444, Loss: 0.6728, Train: 0.5746, Val: 0.5633, Test: 0.6067\n",
      "Epoch: 445, Loss: 0.6718, Train: 0.5954, Val: 0.5367, Test: 0.6100\n",
      "Epoch: 446, Loss: 0.6735, Train: 0.5542, Val: 0.5467, Test: 0.5167\n",
      "Epoch: 447, Loss: 0.6724, Train: 0.5950, Val: 0.5333, Test: 0.6033\n",
      "Epoch: 448, Loss: 0.6741, Train: 0.5854, Val: 0.5533, Test: 0.6067\n",
      "Epoch: 449, Loss: 0.6730, Train: 0.5925, Val: 0.5567, Test: 0.6000\n",
      "Epoch: 450, Loss: 0.6707, Train: 0.5658, Val: 0.5533, Test: 0.5267\n",
      "Epoch: 451, Loss: 0.6692, Train: 0.5892, Val: 0.5500, Test: 0.5867\n",
      "Epoch: 452, Loss: 0.6709, Train: 0.5904, Val: 0.5600, Test: 0.6133\n",
      "Epoch: 453, Loss: 0.6728, Train: 0.5837, Val: 0.5800, Test: 0.5733\n",
      "Epoch: 454, Loss: 0.6750, Train: 0.5871, Val: 0.5600, Test: 0.6100\n",
      "Epoch: 455, Loss: 0.6690, Train: 0.5938, Val: 0.5333, Test: 0.6067\n",
      "Epoch: 456, Loss: 0.6695, Train: 0.5908, Val: 0.5533, Test: 0.5833\n",
      "Epoch: 457, Loss: 0.6696, Train: 0.5900, Val: 0.5433, Test: 0.5933\n",
      "Epoch: 458, Loss: 0.6731, Train: 0.5808, Val: 0.5600, Test: 0.5900\n",
      "Epoch: 459, Loss: 0.6712, Train: 0.5792, Val: 0.5633, Test: 0.5967\n",
      "Epoch: 460, Loss: 0.6700, Train: 0.5917, Val: 0.5433, Test: 0.5833\n",
      "Epoch: 461, Loss: 0.6719, Train: 0.5962, Val: 0.5467, Test: 0.5933\n",
      "Epoch: 462, Loss: 0.6692, Train: 0.5892, Val: 0.5567, Test: 0.5867\n",
      "Epoch: 463, Loss: 0.6662, Train: 0.5913, Val: 0.5600, Test: 0.5967\n",
      "Epoch: 464, Loss: 0.6696, Train: 0.5804, Val: 0.5667, Test: 0.5933\n",
      "Epoch: 465, Loss: 0.6694, Train: 0.5879, Val: 0.5533, Test: 0.5933\n",
      "Epoch: 466, Loss: 0.6722, Train: 0.5771, Val: 0.5567, Test: 0.5933\n",
      "Epoch: 467, Loss: 0.6701, Train: 0.5817, Val: 0.5600, Test: 0.5833\n",
      "Epoch: 468, Loss: 0.6695, Train: 0.5850, Val: 0.5500, Test: 0.6067\n",
      "Epoch: 469, Loss: 0.6686, Train: 0.5842, Val: 0.5500, Test: 0.6033\n",
      "Epoch: 470, Loss: 0.6720, Train: 0.5683, Val: 0.5733, Test: 0.5367\n",
      "Epoch: 471, Loss: 0.6690, Train: 0.5808, Val: 0.5633, Test: 0.5767\n",
      "Epoch: 472, Loss: 0.6713, Train: 0.5904, Val: 0.5500, Test: 0.5867\n",
      "Epoch: 473, Loss: 0.6672, Train: 0.5813, Val: 0.5533, Test: 0.6100\n",
      "Epoch: 474, Loss: 0.6734, Train: 0.5892, Val: 0.5467, Test: 0.6167\n",
      "Epoch: 475, Loss: 0.6715, Train: 0.5842, Val: 0.5533, Test: 0.6033\n",
      "Epoch: 476, Loss: 0.6697, Train: 0.5713, Val: 0.5567, Test: 0.6000\n",
      "Epoch: 477, Loss: 0.6663, Train: 0.5829, Val: 0.5567, Test: 0.6100\n",
      "Epoch: 478, Loss: 0.6705, Train: 0.5867, Val: 0.5433, Test: 0.5933\n",
      "Epoch: 479, Loss: 0.6683, Train: 0.5908, Val: 0.5533, Test: 0.5967\n",
      "Epoch: 480, Loss: 0.6735, Train: 0.5925, Val: 0.5367, Test: 0.5700\n",
      "Epoch: 481, Loss: 0.6693, Train: 0.5925, Val: 0.5500, Test: 0.5867\n",
      "Epoch: 482, Loss: 0.6703, Train: 0.5929, Val: 0.5433, Test: 0.5867\n",
      "Epoch: 483, Loss: 0.6701, Train: 0.5913, Val: 0.5500, Test: 0.5867\n",
      "Epoch: 484, Loss: 0.6675, Train: 0.5908, Val: 0.5467, Test: 0.5967\n",
      "Epoch: 485, Loss: 0.6718, Train: 0.5821, Val: 0.5600, Test: 0.5500\n",
      "Epoch: 486, Loss: 0.6720, Train: 0.5942, Val: 0.5433, Test: 0.5800\n",
      "Epoch: 487, Loss: 0.6700, Train: 0.5863, Val: 0.5633, Test: 0.5833\n",
      "Epoch: 488, Loss: 0.6640, Train: 0.5725, Val: 0.5567, Test: 0.5300\n",
      "Epoch: 489, Loss: 0.6709, Train: 0.5813, Val: 0.5767, Test: 0.5667\n",
      "Epoch: 490, Loss: 0.6688, Train: 0.5817, Val: 0.5633, Test: 0.5833\n",
      "Epoch: 491, Loss: 0.6672, Train: 0.5850, Val: 0.5600, Test: 0.6100\n",
      "Epoch: 492, Loss: 0.6678, Train: 0.5950, Val: 0.5367, Test: 0.6000\n",
      "Epoch: 493, Loss: 0.6675, Train: 0.5808, Val: 0.5433, Test: 0.6100\n",
      "Epoch: 494, Loss: 0.6681, Train: 0.5817, Val: 0.5567, Test: 0.5733\n",
      "Epoch: 495, Loss: 0.6681, Train: 0.5808, Val: 0.5500, Test: 0.5900\n",
      "Epoch: 496, Loss: 0.6694, Train: 0.5913, Val: 0.5400, Test: 0.5833\n",
      "Epoch: 497, Loss: 0.6715, Train: 0.5887, Val: 0.5400, Test: 0.5900\n",
      "Epoch: 498, Loss: 0.6674, Train: 0.5913, Val: 0.5400, Test: 0.5867\n",
      "Epoch: 499, Loss: 0.6699, Train: 0.5988, Val: 0.5367, Test: 0.5867\n",
      "Epoch: 500, Loss: 0.6694, Train: 0.5821, Val: 0.5400, Test: 0.6033\n",
      "### Run 0 - val loss: 0.677, test acc: 0.610\n",
      "Accuracies in each run:  [0.61]\n",
      "test acc - mean: 0.610, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'panpool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asapool\n",
      "Epoch: 001, Loss: 0.7508, Train: 0.5071, Val: 0.5133, Test: 0.5800\n",
      "Epoch: 002, Loss: 0.7031, Train: 0.5212, Val: 0.5400, Test: 0.5400\n",
      "Epoch: 003, Loss: 0.6992, Train: 0.5225, Val: 0.5200, Test: 0.5000\n",
      "Epoch: 004, Loss: 0.7000, Train: 0.5825, Val: 0.5533, Test: 0.5733\n",
      "Epoch: 005, Loss: 0.6940, Train: 0.5437, Val: 0.5233, Test: 0.5367\n",
      "Epoch: 006, Loss: 0.6956, Train: 0.5342, Val: 0.5167, Test: 0.5233\n",
      "Epoch: 007, Loss: 0.6591, Train: 0.6704, Val: 0.6533, Test: 0.6867\n",
      "Epoch: 008, Loss: 0.6463, Train: 0.6775, Val: 0.6667, Test: 0.6933\n",
      "Epoch: 009, Loss: 0.5717, Train: 0.7863, Val: 0.7833, Test: 0.7833\n",
      "Epoch: 010, Loss: 0.3676, Train: 0.8008, Val: 0.8033, Test: 0.7900\n",
      "Epoch: 011, Loss: 0.5334, Train: 0.6629, Val: 0.6933, Test: 0.6833\n",
      "Epoch: 012, Loss: 0.4098, Train: 0.8842, Val: 0.8900, Test: 0.8767\n",
      "Epoch: 013, Loss: 0.3391, Train: 0.8896, Val: 0.8933, Test: 0.8800\n",
      "Epoch: 014, Loss: 0.3488, Train: 0.8617, Val: 0.8533, Test: 0.8467\n",
      "Epoch: 015, Loss: 0.3345, Train: 0.8946, Val: 0.9033, Test: 0.8867\n",
      "Epoch: 016, Loss: 0.3425, Train: 0.8796, Val: 0.8867, Test: 0.8733\n",
      "Epoch: 017, Loss: 0.3881, Train: 0.7712, Val: 0.7600, Test: 0.7667\n",
      "Epoch: 018, Loss: 0.6058, Train: 0.4696, Val: 0.4800, Test: 0.5067\n",
      "Epoch: 019, Loss: 0.7311, Train: 0.6371, Val: 0.6633, Test: 0.6400\n",
      "Epoch: 020, Loss: 0.6911, Train: 0.6417, Val: 0.6933, Test: 0.6200\n",
      "Epoch: 021, Loss: 0.6700, Train: 0.6471, Val: 0.7033, Test: 0.6333\n",
      "Epoch: 022, Loss: 0.6570, Train: 0.6629, Val: 0.7067, Test: 0.6267\n",
      "Epoch: 023, Loss: 0.6605, Train: 0.6713, Val: 0.6900, Test: 0.6600\n",
      "Epoch: 024, Loss: 0.6464, Train: 0.6604, Val: 0.7233, Test: 0.6433\n",
      "Epoch: 025, Loss: 0.6571, Train: 0.6746, Val: 0.7233, Test: 0.6433\n",
      "Epoch: 026, Loss: 0.6385, Train: 0.6438, Val: 0.6533, Test: 0.6400\n",
      "Epoch: 027, Loss: 0.6419, Train: 0.6787, Val: 0.7100, Test: 0.6567\n",
      "Epoch: 028, Loss: 0.6356, Train: 0.6637, Val: 0.6667, Test: 0.6400\n",
      "Epoch: 029, Loss: 0.6338, Train: 0.6033, Val: 0.6133, Test: 0.5967\n",
      "Epoch: 030, Loss: 0.6256, Train: 0.6175, Val: 0.6500, Test: 0.6000\n",
      "Epoch: 031, Loss: 0.6344, Train: 0.6758, Val: 0.6933, Test: 0.6733\n",
      "Epoch: 032, Loss: 0.6280, Train: 0.6804, Val: 0.7000, Test: 0.6800\n",
      "Epoch: 033, Loss: 0.6284, Train: 0.6783, Val: 0.7000, Test: 0.6833\n",
      "Epoch: 034, Loss: 0.6360, Train: 0.6783, Val: 0.6900, Test: 0.6767\n",
      "Epoch: 035, Loss: 0.6237, Train: 0.6721, Val: 0.7167, Test: 0.6567\n",
      "Epoch: 036, Loss: 0.6117, Train: 0.6837, Val: 0.7000, Test: 0.6900\n",
      "Epoch: 037, Loss: 0.6070, Train: 0.6783, Val: 0.7200, Test: 0.6767\n",
      "Epoch: 038, Loss: 0.6108, Train: 0.6925, Val: 0.7200, Test: 0.6767\n",
      "Epoch: 039, Loss: 0.6189, Train: 0.6604, Val: 0.7033, Test: 0.6600\n",
      "Epoch: 040, Loss: 0.6156, Train: 0.5492, Val: 0.5567, Test: 0.5767\n",
      "Epoch: 041, Loss: 0.6147, Train: 0.6850, Val: 0.7100, Test: 0.6900\n",
      "Epoch: 042, Loss: 0.6142, Train: 0.6837, Val: 0.6900, Test: 0.6967\n",
      "Epoch: 043, Loss: 0.6179, Train: 0.6917, Val: 0.7267, Test: 0.6867\n",
      "Epoch: 044, Loss: 0.6221, Train: 0.6596, Val: 0.7067, Test: 0.6500\n",
      "Epoch: 045, Loss: 0.6196, Train: 0.6842, Val: 0.7133, Test: 0.6667\n",
      "Epoch: 046, Loss: 0.6180, Train: 0.6729, Val: 0.6933, Test: 0.6867\n",
      "Epoch: 047, Loss: 0.6073, Train: 0.6283, Val: 0.6267, Test: 0.6167\n",
      "Epoch: 048, Loss: 0.6054, Train: 0.6967, Val: 0.7100, Test: 0.7033\n",
      "Epoch: 049, Loss: 0.6049, Train: 0.6417, Val: 0.6767, Test: 0.6333\n",
      "Epoch: 050, Loss: 0.5978, Train: 0.6829, Val: 0.7300, Test: 0.6700\n",
      "Epoch: 051, Loss: 0.6067, Train: 0.6579, Val: 0.7067, Test: 0.6533\n",
      "Epoch: 052, Loss: 0.5952, Train: 0.6492, Val: 0.6800, Test: 0.6800\n",
      "Epoch: 053, Loss: 0.5535, Train: 0.6979, Val: 0.6967, Test: 0.7033\n",
      "Epoch: 054, Loss: 0.4933, Train: 0.7104, Val: 0.7067, Test: 0.7067\n",
      "Epoch: 055, Loss: 0.4779, Train: 0.8000, Val: 0.7967, Test: 0.7900\n",
      "Epoch: 056, Loss: 0.4628, Train: 0.6704, Val: 0.6667, Test: 0.6900\n",
      "Epoch: 057, Loss: 0.4458, Train: 0.8146, Val: 0.7833, Test: 0.8067\n",
      "Epoch: 058, Loss: 0.4143, Train: 0.8500, Val: 0.8267, Test: 0.8533\n",
      "Epoch: 059, Loss: 0.3995, Train: 0.8746, Val: 0.8700, Test: 0.8800\n",
      "Epoch: 060, Loss: 0.3178, Train: 0.8733, Val: 0.8733, Test: 0.8700\n",
      "Epoch: 061, Loss: 0.2654, Train: 0.9033, Val: 0.9100, Test: 0.9033\n",
      "Epoch: 062, Loss: 0.2556, Train: 0.9204, Val: 0.9200, Test: 0.9333\n",
      "Epoch: 063, Loss: 0.2311, Train: 0.9196, Val: 0.9167, Test: 0.9200\n",
      "Epoch: 064, Loss: 0.2101, Train: 0.9208, Val: 0.9100, Test: 0.9100\n",
      "Epoch: 065, Loss: 0.2149, Train: 0.9308, Val: 0.9200, Test: 0.9267\n",
      "Epoch: 066, Loss: 0.1799, Train: 0.9342, Val: 0.9300, Test: 0.9367\n",
      "Epoch: 067, Loss: 0.1655, Train: 0.9463, Val: 0.9367, Test: 0.9433\n",
      "Epoch: 068, Loss: 0.1666, Train: 0.9400, Val: 0.9200, Test: 0.9467\n",
      "Epoch: 069, Loss: 0.1512, Train: 0.9529, Val: 0.9500, Test: 0.9633\n",
      "Epoch: 070, Loss: 0.1562, Train: 0.9292, Val: 0.9100, Test: 0.9367\n",
      "Epoch: 071, Loss: 0.1473, Train: 0.9637, Val: 0.9667, Test: 0.9567\n",
      "Epoch: 072, Loss: 0.1366, Train: 0.9483, Val: 0.9267, Test: 0.9367\n",
      "Epoch: 073, Loss: 0.1355, Train: 0.9729, Val: 0.9800, Test: 0.9667\n",
      "Epoch: 074, Loss: 0.1290, Train: 0.9587, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 075, Loss: 0.1144, Train: 0.9708, Val: 0.9800, Test: 0.9567\n",
      "Epoch: 076, Loss: 0.1267, Train: 0.9587, Val: 0.9667, Test: 0.9433\n",
      "Epoch: 077, Loss: 0.1015, Train: 0.9750, Val: 0.9833, Test: 0.9567\n",
      "Epoch: 078, Loss: 0.0875, Train: 0.9817, Val: 0.9800, Test: 0.9700\n",
      "Epoch: 079, Loss: 0.0744, Train: 0.9563, Val: 0.9300, Test: 0.9400\n",
      "Epoch: 080, Loss: 0.1054, Train: 0.9792, Val: 0.9800, Test: 0.9700\n",
      "Epoch: 081, Loss: 0.0737, Train: 0.9804, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 082, Loss: 0.0824, Train: 0.9721, Val: 0.9800, Test: 0.9667\n",
      "Epoch: 083, Loss: 0.1238, Train: 0.9779, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 084, Loss: 0.0653, Train: 0.9829, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 085, Loss: 0.0685, Train: 0.9754, Val: 0.9800, Test: 0.9700\n",
      "Epoch: 086, Loss: 0.0641, Train: 0.9821, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 087, Loss: 0.0737, Train: 0.9821, Val: 0.9867, Test: 0.9733\n",
      "Epoch: 088, Loss: 0.1047, Train: 0.9842, Val: 0.9867, Test: 0.9767\n",
      "Epoch: 089, Loss: 0.0564, Train: 0.9871, Val: 0.9800, Test: 0.9800\n",
      "Epoch: 090, Loss: 0.0569, Train: 0.9675, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 091, Loss: 0.0755, Train: 0.9842, Val: 0.9900, Test: 0.9767\n",
      "Epoch: 092, Loss: 0.0555, Train: 0.9875, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 093, Loss: 0.0495, Train: 0.9858, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 094, Loss: 0.0629, Train: 0.9833, Val: 0.9867, Test: 0.9767\n",
      "Epoch: 095, Loss: 0.0513, Train: 0.9858, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 096, Loss: 0.0535, Train: 0.9833, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 097, Loss: 0.0585, Train: 0.9463, Val: 0.9467, Test: 0.9533\n",
      "Epoch: 098, Loss: 0.0934, Train: 0.9817, Val: 0.9833, Test: 0.9700\n",
      "Epoch: 099, Loss: 0.0486, Train: 0.9871, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 100, Loss: 0.0441, Train: 0.9912, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 101, Loss: 0.0376, Train: 0.9808, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 102, Loss: 0.0371, Train: 0.9908, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 103, Loss: 0.0423, Train: 0.9838, Val: 0.9900, Test: 0.9567\n",
      "Epoch: 104, Loss: 0.0543, Train: 0.9875, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 105, Loss: 0.0488, Train: 0.9900, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 106, Loss: 0.0440, Train: 0.9883, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 107, Loss: 0.0377, Train: 0.9904, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 108, Loss: 0.0499, Train: 0.9708, Val: 0.9900, Test: 0.9667\n",
      "Epoch: 109, Loss: 0.0492, Train: 0.9912, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 110, Loss: 0.0445, Train: 0.9896, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 111, Loss: 0.0376, Train: 0.9888, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 112, Loss: 0.0406, Train: 0.9908, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 113, Loss: 0.0399, Train: 0.9929, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 114, Loss: 0.0340, Train: 0.9900, Val: 0.9900, Test: 0.9733\n",
      "Epoch: 115, Loss: 0.0466, Train: 0.9904, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 116, Loss: 0.0378, Train: 0.9933, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 117, Loss: 0.0355, Train: 0.9933, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 118, Loss: 0.0329, Train: 0.9888, Val: 0.9867, Test: 0.9733\n",
      "Epoch: 119, Loss: 0.0496, Train: 0.9933, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 120, Loss: 0.0375, Train: 0.9942, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 121, Loss: 0.0422, Train: 0.9921, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 122, Loss: 0.0356, Train: 0.9879, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 123, Loss: 0.0359, Train: 0.9862, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 124, Loss: 0.0397, Train: 0.9875, Val: 0.9900, Test: 0.9733\n",
      "Epoch: 125, Loss: 0.0398, Train: 0.9921, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 126, Loss: 0.0341, Train: 0.9942, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 127, Loss: 0.0359, Train: 0.9908, Val: 0.9967, Test: 0.9800\n",
      "Epoch: 128, Loss: 0.0449, Train: 0.9833, Val: 0.9900, Test: 0.9733\n",
      "Epoch: 129, Loss: 0.1025, Train: 0.9767, Val: 0.9767, Test: 0.9767\n",
      "Epoch: 130, Loss: 0.1256, Train: 0.8263, Val: 0.8233, Test: 0.8067\n",
      "Epoch: 131, Loss: 0.2594, Train: 0.9654, Val: 0.9500, Test: 0.9567\n",
      "Epoch: 132, Loss: 0.0617, Train: 0.9908, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 133, Loss: 0.0380, Train: 0.9925, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 134, Loss: 0.0697, Train: 0.9508, Val: 0.9400, Test: 0.9467\n",
      "Epoch: 135, Loss: 0.1549, Train: 0.9479, Val: 0.9500, Test: 0.9500\n",
      "Epoch: 136, Loss: 0.1032, Train: 0.9854, Val: 0.9800, Test: 0.9867\n",
      "Epoch: 137, Loss: 0.0443, Train: 0.9883, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 138, Loss: 0.0378, Train: 0.9908, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 139, Loss: 0.0389, Train: 0.9925, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 140, Loss: 0.0325, Train: 0.9942, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 141, Loss: 0.0301, Train: 0.9942, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 142, Loss: 0.0463, Train: 0.9921, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 143, Loss: 0.0301, Train: 0.9942, Val: 0.9933, Test: 0.9733\n",
      "Epoch: 144, Loss: 0.0418, Train: 0.9946, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 145, Loss: 0.0295, Train: 0.9938, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 146, Loss: 0.0601, Train: 0.9700, Val: 0.9767, Test: 0.9667\n",
      "Epoch: 147, Loss: 0.0602, Train: 0.9854, Val: 0.9767, Test: 0.9867\n",
      "Epoch: 148, Loss: 0.0398, Train: 0.9925, Val: 0.9967, Test: 0.9900\n",
      "Epoch: 149, Loss: 0.0467, Train: 0.9912, Val: 0.9867, Test: 0.9767\n",
      "Epoch: 150, Loss: 0.0499, Train: 0.9917, Val: 0.9867, Test: 0.9733\n",
      "Epoch: 151, Loss: 0.0390, Train: 0.9929, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 152, Loss: 0.0294, Train: 0.9938, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 153, Loss: 0.0319, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 154, Loss: 0.0403, Train: 0.9958, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 155, Loss: 0.0253, Train: 0.9912, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 156, Loss: 0.0290, Train: 0.9933, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 157, Loss: 0.0309, Train: 0.9938, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 158, Loss: 0.0231, Train: 0.9917, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 159, Loss: 0.0245, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 160, Loss: 0.0234, Train: 0.9950, Val: 0.9900, Test: 0.9933\n",
      "Epoch: 161, Loss: 0.0257, Train: 0.9908, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 162, Loss: 0.0292, Train: 0.9875, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 163, Loss: 0.0317, Train: 0.9929, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 164, Loss: 0.0307, Train: 0.9912, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 165, Loss: 0.0296, Train: 0.9917, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 166, Loss: 0.0320, Train: 0.9933, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 167, Loss: 0.1240, Train: 0.9217, Val: 0.9067, Test: 0.9200\n",
      "Epoch: 168, Loss: 0.1598, Train: 0.9463, Val: 0.9400, Test: 0.9567\n",
      "Epoch: 169, Loss: 0.1138, Train: 0.9696, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 170, Loss: 0.0949, Train: 0.9833, Val: 0.9833, Test: 0.9867\n",
      "Epoch: 171, Loss: 0.0701, Train: 0.9846, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 172, Loss: 0.0388, Train: 0.9908, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 173, Loss: 0.0275, Train: 0.9929, Val: 0.9967, Test: 0.9733\n",
      "Epoch: 174, Loss: 0.0587, Train: 0.9829, Val: 0.9867, Test: 0.9733\n",
      "Epoch: 175, Loss: 0.0471, Train: 0.9900, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 176, Loss: 0.0398, Train: 0.9921, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 177, Loss: 0.0312, Train: 0.9904, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 178, Loss: 0.0299, Train: 0.9925, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 179, Loss: 0.0357, Train: 0.9908, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 180, Loss: 0.0288, Train: 0.9917, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 181, Loss: 0.0277, Train: 0.9925, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 182, Loss: 0.0330, Train: 0.9917, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 183, Loss: 0.0266, Train: 0.9950, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 184, Loss: 0.0266, Train: 0.9942, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 185, Loss: 0.0306, Train: 0.9942, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 186, Loss: 0.0251, Train: 0.9942, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 187, Loss: 0.0265, Train: 0.9933, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 188, Loss: 0.0470, Train: 0.9808, Val: 0.9700, Test: 0.9800\n",
      "Epoch: 189, Loss: 0.0514, Train: 0.9783, Val: 0.9767, Test: 0.9633\n",
      "Epoch: 190, Loss: 0.0424, Train: 0.9904, Val: 0.9733, Test: 0.9800\n",
      "Epoch: 191, Loss: 0.0489, Train: 0.9900, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 192, Loss: 0.0453, Train: 0.9904, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 193, Loss: 0.0384, Train: 0.9912, Val: 0.9900, Test: 0.9833\n",
      "Epoch: 194, Loss: 0.0439, Train: 0.9896, Val: 0.9833, Test: 0.9833\n",
      "Epoch: 195, Loss: 0.0469, Train: 0.9904, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 196, Loss: 0.0429, Train: 0.9925, Val: 0.9867, Test: 0.9867\n",
      "Epoch: 197, Loss: 0.0379, Train: 0.9912, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 198, Loss: 0.0346, Train: 0.9912, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 199, Loss: 0.0298, Train: 0.9900, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 200, Loss: 0.0308, Train: 0.9925, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 201, Loss: 0.0286, Train: 0.9917, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 202, Loss: 0.0301, Train: 0.9938, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 203, Loss: 0.0358, Train: 0.9908, Val: 0.9833, Test: 0.9767\n",
      "Epoch: 204, Loss: 0.0455, Train: 0.9875, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 205, Loss: 0.0336, Train: 0.9929, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 206, Loss: 0.0372, Train: 0.9858, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 207, Loss: 0.0349, Train: 0.9896, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 208, Loss: 0.0285, Train: 0.9912, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 209, Loss: 0.0246, Train: 0.9946, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 210, Loss: 0.0321, Train: 0.9925, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 211, Loss: 0.0223, Train: 0.9946, Val: 0.9933, Test: 0.9900\n",
      "Epoch: 212, Loss: 0.0277, Train: 0.9908, Val: 0.9900, Test: 0.9767\n",
      "Epoch: 213, Loss: 0.0335, Train: 0.9921, Val: 0.9900, Test: 0.9900\n",
      "Epoch: 214, Loss: 0.0405, Train: 0.9950, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 215, Loss: 0.0209, Train: 0.9958, Val: 0.9933, Test: 0.9867\n",
      "Epoch: 216, Loss: 0.0313, Train: 0.9938, Val: 0.9967, Test: 0.9867\n",
      "Epoch: 217, Loss: 0.0197, Train: 0.9938, Val: 0.9967, Test: 0.9933\n",
      "Epoch: 218, Loss: 0.0397, Train: 0.9846, Val: 0.9933, Test: 0.9767\n",
      "Epoch: 219, Loss: 0.0382, Train: 0.9929, Val: 0.9867, Test: 0.9800\n",
      "Epoch: 220, Loss: 0.0326, Train: 0.9921, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 221, Loss: 0.0495, Train: 0.9896, Val: 0.9967, Test: 0.9833\n",
      "Epoch: 222, Loss: 0.0964, Train: 0.8862, Val: 0.9000, Test: 0.8700\n",
      "Epoch: 223, Loss: 0.1611, Train: 0.9417, Val: 0.9267, Test: 0.9267\n",
      "Epoch: 224, Loss: 0.1462, Train: 0.9554, Val: 0.9433, Test: 0.9433\n",
      "Epoch: 225, Loss: 0.0849, Train: 0.9892, Val: 0.9867, Test: 0.9767\n",
      "Epoch: 226, Loss: 0.0421, Train: 0.9912, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 227, Loss: 0.0347, Train: 0.9929, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 228, Loss: 0.0368, Train: 0.9921, Val: 0.9933, Test: 0.9800\n",
      "Epoch: 229, Loss: 0.0356, Train: 0.9933, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 230, Loss: 0.0322, Train: 0.9942, Val: 0.9933, Test: 0.9833\n",
      "Epoch: 231, Loss: 0.0358, Train: 0.9921, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 232, Loss: 0.0425, Train: 0.9808, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 233, Loss: 0.0646, Train: 0.9904, Val: 0.9833, Test: 0.9700\n",
      "Epoch: 234, Loss: 0.0430, Train: 0.9925, Val: 0.9900, Test: 0.9767\n",
      "Epoch: 235, Loss: 0.0389, Train: 0.9929, Val: 0.9900, Test: 0.9767\n",
      "Epoch: 236, Loss: 0.0356, Train: 0.9925, Val: 0.9867, Test: 0.9833\n",
      "Epoch: 237, Loss: 0.0304, Train: 0.9933, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 238, Loss: 0.0359, Train: 0.9942, Val: 0.9900, Test: 0.9800\n",
      "Epoch: 239, Loss: 0.1543, Train: 0.9304, Val: 0.9300, Test: 0.9367\n",
      "Epoch: 240, Loss: 0.1879, Train: 0.9458, Val: 0.9200, Test: 0.9300\n",
      "Epoch: 241, Loss: 0.1529, Train: 0.9446, Val: 0.9300, Test: 0.9433\n",
      "Epoch: 242, Loss: 0.1394, Train: 0.9633, Val: 0.9567, Test: 0.9400\n",
      "Epoch: 243, Loss: 0.1162, Train: 0.9675, Val: 0.9800, Test: 0.9433\n",
      "Epoch: 244, Loss: 0.1149, Train: 0.9646, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 245, Loss: 0.0835, Train: 0.9750, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 246, Loss: 0.0693, Train: 0.9800, Val: 0.9767, Test: 0.9633\n",
      "Epoch: 247, Loss: 0.0600, Train: 0.9871, Val: 0.9867, Test: 0.9733\n",
      "Epoch: 248, Loss: 0.0515, Train: 0.9904, Val: 0.9833, Test: 0.9733\n",
      "Epoch: 249, Loss: 0.0462, Train: 0.9908, Val: 0.9800, Test: 0.9767\n",
      "Epoch: 250, Loss: 0.0398, Train: 0.9925, Val: 0.9900, Test: 0.9867\n",
      "Epoch: 251, Loss: 0.0707, Train: 0.9771, Val: 0.9800, Test: 0.9633\n",
      "Epoch: 252, Loss: 0.0823, Train: 0.9896, Val: 0.9767, Test: 0.9833\n",
      "Epoch: 253, Loss: 0.0505, Train: 0.9200, Val: 0.9200, Test: 0.9300\n",
      "Epoch: 254, Loss: 0.1981, Train: 0.9333, Val: 0.9200, Test: 0.9467\n",
      "Epoch: 255, Loss: 0.1712, Train: 0.9437, Val: 0.9333, Test: 0.9467\n",
      "Epoch: 256, Loss: 0.1606, Train: 0.9458, Val: 0.9500, Test: 0.9533\n",
      "Epoch: 257, Loss: 0.1537, Train: 0.9467, Val: 0.9467, Test: 0.9367\n",
      "Epoch: 258, Loss: 0.1476, Train: 0.9492, Val: 0.9433, Test: 0.9467\n",
      "Epoch: 259, Loss: 0.1645, Train: 0.9158, Val: 0.9200, Test: 0.9000\n",
      "Epoch: 260, Loss: 0.1614, Train: 0.9433, Val: 0.9533, Test: 0.9400\n",
      "Epoch: 261, Loss: 0.1568, Train: 0.9500, Val: 0.9533, Test: 0.9433\n",
      "Epoch: 262, Loss: 0.1371, Train: 0.9529, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 263, Loss: 0.1395, Train: 0.9504, Val: 0.9467, Test: 0.9500\n",
      "Epoch: 264, Loss: 0.1451, Train: 0.9500, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 265, Loss: 0.1330, Train: 0.9571, Val: 0.9467, Test: 0.9467\n",
      "Epoch: 266, Loss: 0.1450, Train: 0.9529, Val: 0.9467, Test: 0.9467\n",
      "Epoch: 267, Loss: 0.1350, Train: 0.9563, Val: 0.9467, Test: 0.9500\n",
      "Epoch: 268, Loss: 0.1326, Train: 0.9458, Val: 0.9433, Test: 0.9500\n",
      "Epoch: 269, Loss: 0.1448, Train: 0.9429, Val: 0.9367, Test: 0.9500\n",
      "Epoch: 270, Loss: 0.1489, Train: 0.9513, Val: 0.9433, Test: 0.9533\n",
      "Epoch: 271, Loss: 0.1536, Train: 0.9454, Val: 0.9367, Test: 0.9433\n",
      "Epoch: 272, Loss: 0.1471, Train: 0.9517, Val: 0.9267, Test: 0.9267\n",
      "Epoch: 273, Loss: 0.1486, Train: 0.9421, Val: 0.9367, Test: 0.9467\n",
      "Epoch: 274, Loss: 0.1435, Train: 0.9417, Val: 0.9367, Test: 0.9500\n",
      "Epoch: 275, Loss: 0.1715, Train: 0.9463, Val: 0.9433, Test: 0.9433\n",
      "Epoch: 276, Loss: 0.1489, Train: 0.9513, Val: 0.9367, Test: 0.9300\n",
      "Epoch: 277, Loss: 0.1609, Train: 0.9442, Val: 0.9367, Test: 0.9433\n",
      "Epoch: 278, Loss: 0.1563, Train: 0.9375, Val: 0.9300, Test: 0.9400\n",
      "Epoch: 279, Loss: 0.1580, Train: 0.9442, Val: 0.9167, Test: 0.9267\n",
      "Epoch: 280, Loss: 0.1450, Train: 0.9437, Val: 0.9367, Test: 0.9333\n",
      "Epoch: 281, Loss: 0.1649, Train: 0.9279, Val: 0.9100, Test: 0.9267\n",
      "Epoch: 282, Loss: 0.1518, Train: 0.9400, Val: 0.9200, Test: 0.9200\n",
      "Epoch: 283, Loss: 0.1737, Train: 0.9454, Val: 0.9400, Test: 0.9500\n",
      "Epoch: 284, Loss: 0.1608, Train: 0.9467, Val: 0.9367, Test: 0.9467\n",
      "Epoch: 285, Loss: 0.1545, Train: 0.9383, Val: 0.9233, Test: 0.9467\n",
      "Epoch: 286, Loss: 0.1688, Train: 0.9425, Val: 0.9133, Test: 0.9367\n",
      "Epoch: 287, Loss: 0.1682, Train: 0.9400, Val: 0.9267, Test: 0.9300\n",
      "Epoch: 288, Loss: 0.1658, Train: 0.9421, Val: 0.9267, Test: 0.9400\n",
      "Epoch: 289, Loss: 0.1604, Train: 0.9450, Val: 0.9133, Test: 0.9467\n",
      "Epoch: 290, Loss: 0.1545, Train: 0.9458, Val: 0.9267, Test: 0.9333\n",
      "Epoch: 291, Loss: 0.1710, Train: 0.9429, Val: 0.9167, Test: 0.9367\n",
      "Epoch: 292, Loss: 0.1598, Train: 0.9425, Val: 0.9267, Test: 0.9367\n",
      "Epoch: 293, Loss: 0.1519, Train: 0.9296, Val: 0.9133, Test: 0.9433\n",
      "Epoch: 294, Loss: 0.1692, Train: 0.9367, Val: 0.9133, Test: 0.9333\n",
      "Epoch: 295, Loss: 0.1640, Train: 0.9400, Val: 0.9200, Test: 0.9233\n",
      "Epoch: 296, Loss: 0.1685, Train: 0.9179, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 297, Loss: 0.1954, Train: 0.9308, Val: 0.9200, Test: 0.9233\n",
      "Epoch: 298, Loss: 0.1808, Train: 0.9308, Val: 0.9100, Test: 0.9333\n",
      "Epoch: 299, Loss: 0.1739, Train: 0.9396, Val: 0.9033, Test: 0.9233\n",
      "Epoch: 300, Loss: 0.1651, Train: 0.9387, Val: 0.9100, Test: 0.9267\n",
      "Epoch: 301, Loss: 0.1748, Train: 0.9417, Val: 0.9133, Test: 0.9367\n",
      "Epoch: 302, Loss: 0.1709, Train: 0.9354, Val: 0.9000, Test: 0.9300\n",
      "Epoch: 303, Loss: 0.1680, Train: 0.9313, Val: 0.9100, Test: 0.9200\n",
      "Epoch: 304, Loss: 0.1767, Train: 0.9354, Val: 0.9167, Test: 0.9200\n",
      "Epoch: 305, Loss: 0.1797, Train: 0.9279, Val: 0.9167, Test: 0.9300\n",
      "Epoch: 306, Loss: 0.1720, Train: 0.9367, Val: 0.8967, Test: 0.9400\n",
      "Epoch: 307, Loss: 0.1686, Train: 0.9213, Val: 0.9200, Test: 0.9267\n",
      "Epoch: 308, Loss: 0.1754, Train: 0.9313, Val: 0.9200, Test: 0.9233\n",
      "Epoch: 309, Loss: 0.1659, Train: 0.9337, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 310, Loss: 0.1715, Train: 0.9308, Val: 0.9133, Test: 0.9300\n",
      "Epoch: 311, Loss: 0.1573, Train: 0.9367, Val: 0.9233, Test: 0.9233\n",
      "Epoch: 312, Loss: 0.1653, Train: 0.9342, Val: 0.9333, Test: 0.9267\n",
      "Epoch: 313, Loss: 0.1808, Train: 0.9367, Val: 0.9067, Test: 0.9433\n",
      "Epoch: 314, Loss: 0.1669, Train: 0.9400, Val: 0.9233, Test: 0.9367\n",
      "Epoch: 315, Loss: 0.1698, Train: 0.9354, Val: 0.9033, Test: 0.9333\n",
      "Epoch: 316, Loss: 0.1731, Train: 0.9363, Val: 0.9200, Test: 0.9300\n",
      "Epoch: 317, Loss: 0.1823, Train: 0.9213, Val: 0.8967, Test: 0.9233\n",
      "Epoch: 318, Loss: 0.1833, Train: 0.9325, Val: 0.9067, Test: 0.9333\n",
      "Epoch: 319, Loss: 0.1792, Train: 0.9367, Val: 0.9333, Test: 0.9267\n",
      "Epoch: 320, Loss: 0.2000, Train: 0.9029, Val: 0.8733, Test: 0.8867\n",
      "Epoch: 321, Loss: 0.2173, Train: 0.9163, Val: 0.8933, Test: 0.9033\n",
      "Epoch: 322, Loss: 0.2004, Train: 0.9183, Val: 0.8933, Test: 0.8967\n",
      "Epoch: 323, Loss: 0.1898, Train: 0.9337, Val: 0.9133, Test: 0.9167\n",
      "Epoch: 324, Loss: 0.1755, Train: 0.9317, Val: 0.9133, Test: 0.9300\n",
      "Epoch: 325, Loss: 0.1794, Train: 0.9333, Val: 0.9200, Test: 0.9300\n",
      "Epoch: 326, Loss: 0.1744, Train: 0.9392, Val: 0.9400, Test: 0.9333\n",
      "Epoch: 327, Loss: 0.1713, Train: 0.9342, Val: 0.9267, Test: 0.9233\n",
      "Epoch: 328, Loss: 0.1834, Train: 0.9308, Val: 0.9233, Test: 0.9267\n",
      "Epoch: 329, Loss: 0.1763, Train: 0.9392, Val: 0.9467, Test: 0.9300\n",
      "Epoch: 330, Loss: 0.1711, Train: 0.9292, Val: 0.9267, Test: 0.9167\n",
      "Epoch: 331, Loss: 0.1773, Train: 0.9229, Val: 0.9033, Test: 0.9133\n",
      "Epoch: 332, Loss: 0.1886, Train: 0.9192, Val: 0.9033, Test: 0.9233\n",
      "Epoch: 333, Loss: 0.1772, Train: 0.9271, Val: 0.9067, Test: 0.9267\n",
      "Epoch: 334, Loss: 0.1757, Train: 0.9275, Val: 0.9100, Test: 0.9200\n",
      "Epoch: 335, Loss: 0.1764, Train: 0.9400, Val: 0.9433, Test: 0.9267\n",
      "Epoch: 336, Loss: 0.1801, Train: 0.9308, Val: 0.9067, Test: 0.9300\n",
      "Epoch: 337, Loss: 0.1749, Train: 0.9387, Val: 0.9333, Test: 0.9233\n",
      "Epoch: 338, Loss: 0.1693, Train: 0.9375, Val: 0.9467, Test: 0.9167\n",
      "Epoch: 339, Loss: 0.1696, Train: 0.9292, Val: 0.9133, Test: 0.9233\n",
      "Epoch: 340, Loss: 0.1856, Train: 0.9275, Val: 0.9133, Test: 0.9233\n",
      "Epoch: 341, Loss: 0.1875, Train: 0.9308, Val: 0.9100, Test: 0.9267\n",
      "Epoch: 342, Loss: 0.1801, Train: 0.9367, Val: 0.9200, Test: 0.9333\n",
      "Epoch: 343, Loss: 0.1675, Train: 0.9396, Val: 0.9300, Test: 0.9233\n",
      "Epoch: 344, Loss: 0.1663, Train: 0.9371, Val: 0.9267, Test: 0.9300\n",
      "Epoch: 345, Loss: 0.1758, Train: 0.9317, Val: 0.9067, Test: 0.9367\n",
      "Epoch: 346, Loss: 0.1624, Train: 0.9337, Val: 0.9167, Test: 0.9333\n",
      "Epoch: 347, Loss: 0.1699, Train: 0.9321, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 348, Loss: 0.1716, Train: 0.9221, Val: 0.9133, Test: 0.9133\n",
      "Epoch: 349, Loss: 0.1730, Train: 0.9371, Val: 0.9167, Test: 0.9300\n",
      "Epoch: 350, Loss: 0.1673, Train: 0.9329, Val: 0.9133, Test: 0.9267\n",
      "Epoch: 351, Loss: 0.1745, Train: 0.9325, Val: 0.9167, Test: 0.9200\n",
      "Epoch: 352, Loss: 0.1720, Train: 0.9321, Val: 0.9167, Test: 0.9200\n",
      "Epoch: 353, Loss: 0.1718, Train: 0.9396, Val: 0.9333, Test: 0.9267\n",
      "Epoch: 354, Loss: 0.1795, Train: 0.9263, Val: 0.9000, Test: 0.9167\n",
      "Epoch: 355, Loss: 0.1843, Train: 0.9321, Val: 0.9167, Test: 0.9167\n",
      "Epoch: 356, Loss: 0.1814, Train: 0.9221, Val: 0.9000, Test: 0.9233\n",
      "Epoch: 357, Loss: 0.1712, Train: 0.9137, Val: 0.8867, Test: 0.9067\n",
      "Epoch: 358, Loss: 0.1758, Train: 0.9321, Val: 0.9100, Test: 0.9167\n",
      "Epoch: 359, Loss: 0.2007, Train: 0.8717, Val: 0.8567, Test: 0.8733\n",
      "Epoch: 360, Loss: 0.2727, Train: 0.9208, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 361, Loss: 0.2020, Train: 0.9004, Val: 0.8967, Test: 0.9067\n",
      "Epoch: 362, Loss: 0.2163, Train: 0.9133, Val: 0.8900, Test: 0.9000\n",
      "Epoch: 363, Loss: 0.2226, Train: 0.9067, Val: 0.8967, Test: 0.8967\n",
      "Epoch: 364, Loss: 0.2203, Train: 0.9067, Val: 0.8933, Test: 0.9033\n",
      "Epoch: 365, Loss: 0.2153, Train: 0.9108, Val: 0.9000, Test: 0.9067\n",
      "Epoch: 366, Loss: 0.2067, Train: 0.9121, Val: 0.8967, Test: 0.9100\n",
      "Epoch: 367, Loss: 0.1992, Train: 0.9163, Val: 0.8800, Test: 0.9033\n",
      "Epoch: 368, Loss: 0.1941, Train: 0.9196, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 369, Loss: 0.1884, Train: 0.9317, Val: 0.9100, Test: 0.9133\n",
      "Epoch: 370, Loss: 0.1834, Train: 0.9313, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 371, Loss: 0.1942, Train: 0.9258, Val: 0.8967, Test: 0.9033\n",
      "Epoch: 372, Loss: 0.1840, Train: 0.9317, Val: 0.9033, Test: 0.9100\n",
      "Epoch: 373, Loss: 0.1811, Train: 0.9267, Val: 0.9000, Test: 0.9167\n",
      "Epoch: 374, Loss: 0.1845, Train: 0.9329, Val: 0.9067, Test: 0.9267\n",
      "Epoch: 375, Loss: 0.1848, Train: 0.9371, Val: 0.9067, Test: 0.9300\n",
      "Epoch: 376, Loss: 0.1801, Train: 0.9346, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 377, Loss: 0.1840, Train: 0.9225, Val: 0.9067, Test: 0.9000\n",
      "Epoch: 378, Loss: 0.1774, Train: 0.9250, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 379, Loss: 0.1740, Train: 0.9242, Val: 0.9000, Test: 0.9233\n",
      "Epoch: 380, Loss: 0.1880, Train: 0.9158, Val: 0.8733, Test: 0.9233\n",
      "Epoch: 381, Loss: 0.1948, Train: 0.9292, Val: 0.8967, Test: 0.9133\n",
      "Epoch: 382, Loss: 0.1828, Train: 0.9367, Val: 0.9133, Test: 0.9300\n",
      "Epoch: 383, Loss: 0.1847, Train: 0.9358, Val: 0.9067, Test: 0.9267\n",
      "Epoch: 384, Loss: 0.1917, Train: 0.9192, Val: 0.8867, Test: 0.9233\n",
      "Epoch: 385, Loss: 0.1975, Train: 0.9287, Val: 0.8933, Test: 0.9167\n",
      "Epoch: 386, Loss: 0.1994, Train: 0.9171, Val: 0.9033, Test: 0.9267\n",
      "Epoch: 387, Loss: 0.2036, Train: 0.9154, Val: 0.8900, Test: 0.9167\n",
      "Epoch: 388, Loss: 0.1901, Train: 0.9150, Val: 0.8867, Test: 0.9133\n",
      "Epoch: 389, Loss: 0.1808, Train: 0.9300, Val: 0.9067, Test: 0.9167\n",
      "Epoch: 390, Loss: 0.1803, Train: 0.9279, Val: 0.9100, Test: 0.9200\n",
      "Epoch: 391, Loss: 0.1928, Train: 0.8888, Val: 0.8633, Test: 0.8967\n",
      "Epoch: 392, Loss: 0.2269, Train: 0.9067, Val: 0.8733, Test: 0.9033\n",
      "Epoch: 393, Loss: 0.2165, Train: 0.9192, Val: 0.8767, Test: 0.9167\n",
      "Epoch: 394, Loss: 0.2116, Train: 0.9183, Val: 0.8733, Test: 0.9133\n",
      "Epoch: 395, Loss: 0.2028, Train: 0.9167, Val: 0.8900, Test: 0.9033\n",
      "Epoch: 396, Loss: 0.2148, Train: 0.9142, Val: 0.8933, Test: 0.9100\n",
      "Epoch: 397, Loss: 0.2081, Train: 0.9129, Val: 0.9033, Test: 0.9033\n",
      "Epoch: 398, Loss: 0.2012, Train: 0.9129, Val: 0.8733, Test: 0.9100\n",
      "Epoch: 399, Loss: 0.2072, Train: 0.8783, Val: 0.8433, Test: 0.8500\n",
      "Epoch: 400, Loss: 0.2151, Train: 0.8967, Val: 0.8567, Test: 0.9133\n",
      "Epoch: 401, Loss: 0.2146, Train: 0.9117, Val: 0.8767, Test: 0.9067\n",
      "Epoch: 402, Loss: 0.2163, Train: 0.9096, Val: 0.8767, Test: 0.9133\n",
      "Epoch: 403, Loss: 0.2139, Train: 0.9071, Val: 0.8667, Test: 0.9300\n",
      "Epoch: 404, Loss: 0.2144, Train: 0.9154, Val: 0.8900, Test: 0.9133\n",
      "Epoch: 405, Loss: 0.2056, Train: 0.9100, Val: 0.8867, Test: 0.9267\n",
      "Epoch: 406, Loss: 0.2040, Train: 0.9125, Val: 0.8833, Test: 0.9233\n",
      "Epoch: 407, Loss: 0.2155, Train: 0.9287, Val: 0.9100, Test: 0.9333\n",
      "Epoch: 408, Loss: 0.2009, Train: 0.9187, Val: 0.8967, Test: 0.9133\n",
      "Epoch: 409, Loss: 0.2070, Train: 0.9179, Val: 0.8900, Test: 0.9133\n",
      "Epoch: 410, Loss: 0.2156, Train: 0.9154, Val: 0.8767, Test: 0.9100\n",
      "Epoch: 411, Loss: 0.2166, Train: 0.9025, Val: 0.8600, Test: 0.9033\n",
      "Epoch: 412, Loss: 0.2287, Train: 0.8988, Val: 0.8633, Test: 0.9100\n",
      "Epoch: 413, Loss: 0.2388, Train: 0.8929, Val: 0.8833, Test: 0.9133\n",
      "Epoch: 414, Loss: 0.4038, Train: 0.7096, Val: 0.7133, Test: 0.7000\n",
      "Epoch: 415, Loss: 0.6280, Train: 0.6392, Val: 0.6733, Test: 0.6333\n",
      "Epoch: 416, Loss: 0.6553, Train: 0.6388, Val: 0.6633, Test: 0.6367\n",
      "Epoch: 417, Loss: 0.6495, Train: 0.6508, Val: 0.6167, Test: 0.6267\n",
      "Epoch: 418, Loss: 0.6468, Train: 0.6246, Val: 0.6100, Test: 0.6533\n",
      "Epoch: 419, Loss: 0.6554, Train: 0.6304, Val: 0.6067, Test: 0.6133\n",
      "Epoch: 420, Loss: 0.6484, Train: 0.6338, Val: 0.6433, Test: 0.6433\n",
      "Epoch: 421, Loss: 0.6516, Train: 0.6275, Val: 0.6400, Test: 0.6167\n",
      "Epoch: 422, Loss: 0.6485, Train: 0.6417, Val: 0.6300, Test: 0.6333\n",
      "Epoch: 423, Loss: 0.6498, Train: 0.6400, Val: 0.6167, Test: 0.6233\n",
      "Epoch: 424, Loss: 0.6408, Train: 0.6288, Val: 0.5867, Test: 0.6333\n",
      "Epoch: 425, Loss: 0.6457, Train: 0.6438, Val: 0.6200, Test: 0.6400\n",
      "Epoch: 426, Loss: 0.6364, Train: 0.6579, Val: 0.6433, Test: 0.6267\n",
      "Epoch: 427, Loss: 0.6311, Train: 0.6508, Val: 0.6433, Test: 0.6533\n",
      "Epoch: 428, Loss: 0.6165, Train: 0.6525, Val: 0.6200, Test: 0.6200\n",
      "Epoch: 429, Loss: 0.6272, Train: 0.6679, Val: 0.6700, Test: 0.6633\n",
      "Epoch: 430, Loss: 0.6099, Train: 0.6725, Val: 0.6900, Test: 0.6767\n",
      "Epoch: 431, Loss: 0.6102, Train: 0.6746, Val: 0.6700, Test: 0.6967\n",
      "Epoch: 432, Loss: 0.6116, Train: 0.6608, Val: 0.6467, Test: 0.6433\n",
      "Epoch: 433, Loss: 0.6022, Train: 0.6704, Val: 0.6333, Test: 0.6767\n",
      "Epoch: 434, Loss: 0.6001, Train: 0.6842, Val: 0.6633, Test: 0.7167\n",
      "Epoch: 435, Loss: 0.5819, Train: 0.6817, Val: 0.6533, Test: 0.7267\n",
      "Epoch: 436, Loss: 0.5794, Train: 0.7071, Val: 0.6767, Test: 0.7033\n",
      "Epoch: 437, Loss: 0.5653, Train: 0.7154, Val: 0.7333, Test: 0.7167\n",
      "Epoch: 438, Loss: 0.5706, Train: 0.7167, Val: 0.7300, Test: 0.7300\n",
      "Epoch: 439, Loss: 0.5548, Train: 0.7075, Val: 0.7100, Test: 0.7367\n",
      "Epoch: 440, Loss: 0.5580, Train: 0.7192, Val: 0.7133, Test: 0.7400\n",
      "Epoch: 441, Loss: 0.5455, Train: 0.7354, Val: 0.7067, Test: 0.7433\n",
      "Epoch: 442, Loss: 0.5572, Train: 0.7208, Val: 0.7000, Test: 0.7367\n",
      "Epoch: 443, Loss: 0.5500, Train: 0.7246, Val: 0.6967, Test: 0.7233\n",
      "Epoch: 444, Loss: 0.5481, Train: 0.7258, Val: 0.7200, Test: 0.7167\n",
      "Epoch: 445, Loss: 0.5384, Train: 0.7517, Val: 0.7267, Test: 0.7567\n",
      "Epoch: 446, Loss: 0.5345, Train: 0.7475, Val: 0.7200, Test: 0.7600\n",
      "Epoch: 447, Loss: 0.5498, Train: 0.7404, Val: 0.7133, Test: 0.7400\n",
      "Epoch: 448, Loss: 0.5468, Train: 0.7412, Val: 0.6933, Test: 0.7167\n",
      "Epoch: 449, Loss: 0.5545, Train: 0.7404, Val: 0.7033, Test: 0.7300\n",
      "Epoch: 450, Loss: 0.5611, Train: 0.7354, Val: 0.7000, Test: 0.7133\n",
      "Epoch: 451, Loss: 0.5547, Train: 0.7275, Val: 0.6900, Test: 0.7367\n",
      "Epoch: 452, Loss: 0.5540, Train: 0.7417, Val: 0.7000, Test: 0.7333\n",
      "Epoch: 453, Loss: 0.5396, Train: 0.7488, Val: 0.7333, Test: 0.7500\n",
      "Epoch: 454, Loss: 0.5402, Train: 0.7546, Val: 0.7167, Test: 0.7467\n",
      "Epoch: 455, Loss: 0.5355, Train: 0.7296, Val: 0.7067, Test: 0.7433\n",
      "Epoch: 456, Loss: 0.5367, Train: 0.7342, Val: 0.7400, Test: 0.7300\n",
      "Epoch: 457, Loss: 0.5307, Train: 0.7500, Val: 0.7367, Test: 0.7433\n",
      "Epoch: 458, Loss: 0.5318, Train: 0.7392, Val: 0.7433, Test: 0.7500\n",
      "Epoch: 459, Loss: 0.5362, Train: 0.7429, Val: 0.7067, Test: 0.7533\n",
      "Epoch: 460, Loss: 0.5328, Train: 0.7504, Val: 0.7367, Test: 0.7633\n",
      "Epoch: 461, Loss: 0.5279, Train: 0.7483, Val: 0.7300, Test: 0.7500\n",
      "Epoch: 462, Loss: 0.5153, Train: 0.7454, Val: 0.7233, Test: 0.7433\n",
      "Epoch: 463, Loss: 0.5180, Train: 0.7629, Val: 0.7500, Test: 0.7633\n",
      "Epoch: 464, Loss: 0.5175, Train: 0.7671, Val: 0.7433, Test: 0.7700\n",
      "Epoch: 465, Loss: 0.5002, Train: 0.7446, Val: 0.7300, Test: 0.7667\n",
      "Epoch: 466, Loss: 0.5254, Train: 0.7508, Val: 0.7133, Test: 0.7567\n",
      "Epoch: 467, Loss: 0.5119, Train: 0.7750, Val: 0.7367, Test: 0.7667\n",
      "Epoch: 468, Loss: 0.5147, Train: 0.7542, Val: 0.7300, Test: 0.7767\n",
      "Epoch: 469, Loss: 0.5102, Train: 0.7567, Val: 0.7400, Test: 0.7567\n",
      "Epoch: 470, Loss: 0.5144, Train: 0.7629, Val: 0.7300, Test: 0.7900\n",
      "Epoch: 471, Loss: 0.5184, Train: 0.7588, Val: 0.7567, Test: 0.7700\n",
      "Epoch: 472, Loss: 0.5079, Train: 0.7621, Val: 0.7333, Test: 0.7667\n",
      "Epoch: 473, Loss: 0.5159, Train: 0.7562, Val: 0.7200, Test: 0.7733\n",
      "Epoch: 474, Loss: 0.5191, Train: 0.7792, Val: 0.7333, Test: 0.7800\n",
      "Epoch: 475, Loss: 0.5044, Train: 0.7642, Val: 0.7600, Test: 0.7833\n",
      "Epoch: 476, Loss: 0.4962, Train: 0.7638, Val: 0.7367, Test: 0.7633\n",
      "Epoch: 477, Loss: 0.5084, Train: 0.7742, Val: 0.7467, Test: 0.7800\n",
      "Epoch: 478, Loss: 0.4994, Train: 0.7742, Val: 0.7467, Test: 0.7767\n",
      "Epoch: 479, Loss: 0.4934, Train: 0.7800, Val: 0.7267, Test: 0.7767\n",
      "Epoch: 480, Loss: 0.4982, Train: 0.7717, Val: 0.7400, Test: 0.7933\n",
      "Epoch: 481, Loss: 0.4964, Train: 0.7725, Val: 0.7500, Test: 0.7967\n",
      "Epoch: 482, Loss: 0.4809, Train: 0.7846, Val: 0.7433, Test: 0.7933\n",
      "Epoch: 483, Loss: 0.4851, Train: 0.7746, Val: 0.7700, Test: 0.7967\n",
      "Epoch: 484, Loss: 0.4912, Train: 0.7875, Val: 0.7600, Test: 0.7933\n",
      "Epoch: 485, Loss: 0.4889, Train: 0.7746, Val: 0.7500, Test: 0.7867\n",
      "Epoch: 486, Loss: 0.4814, Train: 0.7833, Val: 0.7433, Test: 0.8067\n",
      "Epoch: 487, Loss: 0.4703, Train: 0.7833, Val: 0.7567, Test: 0.8267\n",
      "Epoch: 488, Loss: 0.4570, Train: 0.7937, Val: 0.7600, Test: 0.8133\n",
      "Epoch: 489, Loss: 0.4676, Train: 0.8037, Val: 0.7700, Test: 0.8200\n",
      "Epoch: 490, Loss: 0.4563, Train: 0.7992, Val: 0.7833, Test: 0.8100\n",
      "Epoch: 491, Loss: 0.4564, Train: 0.7858, Val: 0.7467, Test: 0.7900\n",
      "Epoch: 492, Loss: 0.4690, Train: 0.7825, Val: 0.7533, Test: 0.8067\n",
      "Epoch: 493, Loss: 0.4633, Train: 0.8029, Val: 0.7733, Test: 0.8067\n",
      "Epoch: 494, Loss: 0.4614, Train: 0.8029, Val: 0.7800, Test: 0.8100\n",
      "Epoch: 495, Loss: 0.4497, Train: 0.7904, Val: 0.7600, Test: 0.8233\n",
      "Epoch: 496, Loss: 0.4549, Train: 0.8021, Val: 0.7733, Test: 0.8067\n",
      "Epoch: 497, Loss: 0.4377, Train: 0.7987, Val: 0.7667, Test: 0.8200\n",
      "Epoch: 498, Loss: 0.4325, Train: 0.7908, Val: 0.7733, Test: 0.8100\n",
      "Epoch: 499, Loss: 0.4371, Train: 0.8150, Val: 0.7767, Test: 0.8100\n",
      "Epoch: 500, Loss: 0.4397, Train: 0.8075, Val: 0.7833, Test: 0.8033\n",
      "### Run 0 - val loss: 0.010, test acc: 0.993\n",
      "Accuracies in each run:  [0.9933333333333333]\n",
      "test acc - mean: 0.993, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'asapool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagpool\n",
      "Epoch: 001, Loss: 0.6997, Train: 0.5062, Val: 0.4700, Test: 0.4800\n",
      "Epoch: 002, Loss: 0.6989, Train: 0.5021, Val: 0.5167, Test: 0.4833\n",
      "Epoch: 003, Loss: 0.7013, Train: 0.5067, Val: 0.4600, Test: 0.4800\n",
      "Epoch: 004, Loss: 0.6976, Train: 0.5050, Val: 0.5000, Test: 0.4700\n",
      "Epoch: 005, Loss: 0.6945, Train: 0.4871, Val: 0.4367, Test: 0.4767\n",
      "Epoch: 006, Loss: 0.6939, Train: 0.5096, Val: 0.5067, Test: 0.5767\n",
      "Epoch: 007, Loss: 0.6925, Train: 0.6512, Val: 0.6600, Test: 0.6333\n",
      "Epoch: 008, Loss: 0.6528, Train: 0.6558, Val: 0.6467, Test: 0.6733\n",
      "Epoch: 009, Loss: 0.5654, Train: 0.6887, Val: 0.7167, Test: 0.7167\n",
      "Epoch: 010, Loss: 0.5096, Train: 0.8071, Val: 0.7733, Test: 0.7967\n",
      "Epoch: 011, Loss: 0.4519, Train: 0.8129, Val: 0.8267, Test: 0.7933\n",
      "Epoch: 012, Loss: 0.4315, Train: 0.8287, Val: 0.8233, Test: 0.8367\n",
      "Epoch: 013, Loss: 0.4082, Train: 0.8263, Val: 0.8367, Test: 0.8433\n",
      "Epoch: 014, Loss: 0.4503, Train: 0.8279, Val: 0.8500, Test: 0.8633\n",
      "Epoch: 015, Loss: 0.4161, Train: 0.8033, Val: 0.8100, Test: 0.7867\n",
      "Epoch: 016, Loss: 0.3427, Train: 0.8804, Val: 0.8767, Test: 0.8967\n",
      "Epoch: 017, Loss: 0.3990, Train: 0.8612, Val: 0.8433, Test: 0.8900\n",
      "Epoch: 018, Loss: 0.3350, Train: 0.8946, Val: 0.9033, Test: 0.9067\n",
      "Epoch: 019, Loss: 0.3796, Train: 0.9029, Val: 0.9300, Test: 0.9067\n",
      "Epoch: 020, Loss: 0.3941, Train: 0.8329, Val: 0.8667, Test: 0.8567\n",
      "Epoch: 021, Loss: 0.3659, Train: 0.8725, Val: 0.8800, Test: 0.8800\n",
      "Epoch: 022, Loss: 0.3178, Train: 0.7887, Val: 0.8300, Test: 0.7867\n",
      "Epoch: 023, Loss: 0.3268, Train: 0.8938, Val: 0.9100, Test: 0.9133\n",
      "Epoch: 024, Loss: 0.3553, Train: 0.8754, Val: 0.8867, Test: 0.9000\n",
      "Epoch: 025, Loss: 0.3630, Train: 0.8579, Val: 0.8733, Test: 0.8633\n",
      "Epoch: 026, Loss: 0.3434, Train: 0.9079, Val: 0.9333, Test: 0.9200\n",
      "Epoch: 027, Loss: 0.2727, Train: 0.9158, Val: 0.9300, Test: 0.9233\n",
      "Epoch: 028, Loss: 0.2682, Train: 0.6083, Val: 0.5967, Test: 0.6200\n",
      "Epoch: 029, Loss: 0.3158, Train: 0.8517, Val: 0.8400, Test: 0.8533\n",
      "Epoch: 030, Loss: 0.2876, Train: 0.9150, Val: 0.9233, Test: 0.9300\n",
      "Epoch: 031, Loss: 0.2465, Train: 0.8933, Val: 0.8833, Test: 0.8967\n",
      "Epoch: 032, Loss: 0.2804, Train: 0.9237, Val: 0.9300, Test: 0.9200\n",
      "Epoch: 033, Loss: 0.2552, Train: 0.9237, Val: 0.9233, Test: 0.9033\n",
      "Epoch: 034, Loss: 0.4852, Train: 0.5192, Val: 0.4867, Test: 0.5167\n",
      "Epoch: 035, Loss: 0.4755, Train: 0.9058, Val: 0.9133, Test: 0.9267\n",
      "Epoch: 036, Loss: 0.2842, Train: 0.9142, Val: 0.9100, Test: 0.9033\n",
      "Epoch: 037, Loss: 0.2510, Train: 0.9158, Val: 0.9033, Test: 0.9067\n",
      "Epoch: 038, Loss: 0.2721, Train: 0.8771, Val: 0.8833, Test: 0.8767\n",
      "Epoch: 039, Loss: 0.2388, Train: 0.9300, Val: 0.9267, Test: 0.9333\n",
      "Epoch: 040, Loss: 0.2379, Train: 0.9271, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 041, Loss: 0.2162, Train: 0.9283, Val: 0.9267, Test: 0.9300\n",
      "Epoch: 042, Loss: 0.2700, Train: 0.9371, Val: 0.9333, Test: 0.9467\n",
      "Epoch: 043, Loss: 0.2163, Train: 0.9271, Val: 0.9067, Test: 0.9300\n",
      "Epoch: 044, Loss: 0.2001, Train: 0.9429, Val: 0.9267, Test: 0.9500\n",
      "Epoch: 045, Loss: 0.2269, Train: 0.9413, Val: 0.9233, Test: 0.9500\n",
      "Epoch: 046, Loss: 0.1923, Train: 0.9467, Val: 0.9467, Test: 0.9533\n",
      "Epoch: 047, Loss: 0.1999, Train: 0.9146, Val: 0.9000, Test: 0.9000\n",
      "Epoch: 048, Loss: 0.2183, Train: 0.9404, Val: 0.9133, Test: 0.9467\n",
      "Epoch: 049, Loss: 0.2058, Train: 0.9371, Val: 0.9100, Test: 0.9467\n",
      "Epoch: 050, Loss: 0.2025, Train: 0.9387, Val: 0.9300, Test: 0.9400\n",
      "Epoch: 051, Loss: 0.1838, Train: 0.9425, Val: 0.9233, Test: 0.9533\n",
      "Epoch: 052, Loss: 0.1672, Train: 0.9487, Val: 0.9367, Test: 0.9533\n",
      "Epoch: 053, Loss: 0.2382, Train: 0.9029, Val: 0.8900, Test: 0.9133\n",
      "Epoch: 054, Loss: 0.1906, Train: 0.9175, Val: 0.9033, Test: 0.9267\n",
      "Epoch: 055, Loss: 0.1965, Train: 0.9142, Val: 0.9067, Test: 0.9033\n",
      "Epoch: 056, Loss: 0.1621, Train: 0.9350, Val: 0.9200, Test: 0.9467\n",
      "Epoch: 057, Loss: 0.1684, Train: 0.9463, Val: 0.9333, Test: 0.9500\n",
      "Epoch: 058, Loss: 0.2031, Train: 0.8875, Val: 0.8633, Test: 0.8800\n",
      "Epoch: 059, Loss: 0.1933, Train: 0.8037, Val: 0.7800, Test: 0.8033\n",
      "Epoch: 060, Loss: 0.1690, Train: 0.9517, Val: 0.9267, Test: 0.9533\n",
      "Epoch: 061, Loss: 0.1551, Train: 0.9554, Val: 0.9433, Test: 0.9567\n",
      "Epoch: 062, Loss: 0.1721, Train: 0.9508, Val: 0.9367, Test: 0.9433\n",
      "Epoch: 063, Loss: 0.1476, Train: 0.9596, Val: 0.9500, Test: 0.9600\n",
      "Epoch: 064, Loss: 0.1715, Train: 0.9442, Val: 0.9267, Test: 0.9467\n",
      "Epoch: 065, Loss: 0.1321, Train: 0.9629, Val: 0.9467, Test: 0.9600\n",
      "Epoch: 066, Loss: 0.1968, Train: 0.9171, Val: 0.9000, Test: 0.9233\n",
      "Epoch: 067, Loss: 0.1578, Train: 0.9596, Val: 0.9433, Test: 0.9533\n",
      "Epoch: 068, Loss: 0.1512, Train: 0.9213, Val: 0.8933, Test: 0.9067\n",
      "Epoch: 069, Loss: 0.1524, Train: 0.9517, Val: 0.9433, Test: 0.9367\n",
      "Epoch: 070, Loss: 0.1489, Train: 0.9229, Val: 0.9100, Test: 0.9267\n",
      "Epoch: 071, Loss: 0.1367, Train: 0.9621, Val: 0.9500, Test: 0.9533\n",
      "Epoch: 072, Loss: 0.1743, Train: 0.9579, Val: 0.9433, Test: 0.9567\n",
      "Epoch: 073, Loss: 0.1378, Train: 0.9558, Val: 0.9367, Test: 0.9467\n",
      "Epoch: 074, Loss: 0.1557, Train: 0.8554, Val: 0.8267, Test: 0.8500\n",
      "Epoch: 075, Loss: 0.1590, Train: 0.9525, Val: 0.9300, Test: 0.9467\n",
      "Epoch: 076, Loss: 0.1286, Train: 0.9642, Val: 0.9567, Test: 0.9633\n",
      "Epoch: 077, Loss: 0.1547, Train: 0.9513, Val: 0.9333, Test: 0.9367\n",
      "Epoch: 078, Loss: 0.1343, Train: 0.9542, Val: 0.9533, Test: 0.9367\n",
      "Epoch: 079, Loss: 0.1307, Train: 0.9508, Val: 0.9467, Test: 0.9467\n",
      "Epoch: 080, Loss: 0.1488, Train: 0.9604, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 081, Loss: 0.1311, Train: 0.9637, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 082, Loss: 0.1425, Train: 0.9496, Val: 0.9267, Test: 0.9433\n",
      "Epoch: 083, Loss: 0.1265, Train: 0.9592, Val: 0.9567, Test: 0.9467\n",
      "Epoch: 084, Loss: 0.1446, Train: 0.9621, Val: 0.9500, Test: 0.9533\n",
      "Epoch: 085, Loss: 0.1250, Train: 0.9387, Val: 0.9300, Test: 0.9133\n",
      "Epoch: 086, Loss: 0.1489, Train: 0.9583, Val: 0.9433, Test: 0.9500\n",
      "Epoch: 087, Loss: 0.1533, Train: 0.9625, Val: 0.9400, Test: 0.9567\n",
      "Epoch: 088, Loss: 0.1377, Train: 0.9517, Val: 0.9500, Test: 0.9467\n",
      "Epoch: 089, Loss: 0.1191, Train: 0.9675, Val: 0.9533, Test: 0.9600\n",
      "Epoch: 090, Loss: 0.1560, Train: 0.9154, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 091, Loss: 0.1359, Train: 0.9683, Val: 0.9633, Test: 0.9567\n",
      "Epoch: 092, Loss: 0.1358, Train: 0.9583, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 093, Loss: 0.1091, Train: 0.9671, Val: 0.9667, Test: 0.9567\n",
      "Epoch: 094, Loss: 0.1210, Train: 0.9696, Val: 0.9633, Test: 0.9567\n",
      "Epoch: 095, Loss: 0.1186, Train: 0.9463, Val: 0.9367, Test: 0.9233\n",
      "Epoch: 096, Loss: 0.1290, Train: 0.9558, Val: 0.9367, Test: 0.9567\n",
      "Epoch: 097, Loss: 0.1177, Train: 0.9650, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 098, Loss: 0.1274, Train: 0.9554, Val: 0.9500, Test: 0.9533\n",
      "Epoch: 099, Loss: 0.1078, Train: 0.9629, Val: 0.9500, Test: 0.9567\n",
      "Epoch: 100, Loss: 0.1183, Train: 0.9633, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 101, Loss: 0.1210, Train: 0.9600, Val: 0.9533, Test: 0.9533\n",
      "Epoch: 102, Loss: 0.2187, Train: 0.9346, Val: 0.9267, Test: 0.9333\n",
      "Epoch: 103, Loss: 0.1272, Train: 0.9688, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 104, Loss: 0.1249, Train: 0.9333, Val: 0.9400, Test: 0.9167\n",
      "Epoch: 105, Loss: 0.1641, Train: 0.9604, Val: 0.9500, Test: 0.9467\n",
      "Epoch: 106, Loss: 0.1287, Train: 0.9517, Val: 0.9533, Test: 0.9367\n",
      "Epoch: 107, Loss: 0.1826, Train: 0.9375, Val: 0.9267, Test: 0.9333\n",
      "Epoch: 108, Loss: 0.1240, Train: 0.9583, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 109, Loss: 0.1245, Train: 0.9242, Val: 0.9367, Test: 0.9267\n",
      "Epoch: 110, Loss: 0.1146, Train: 0.9688, Val: 0.9600, Test: 0.9633\n",
      "Epoch: 111, Loss: 0.1213, Train: 0.9633, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 112, Loss: 0.1282, Train: 0.9600, Val: 0.9467, Test: 0.9600\n",
      "Epoch: 113, Loss: 0.1068, Train: 0.9667, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 114, Loss: 0.1109, Train: 0.9721, Val: 0.9667, Test: 0.9567\n",
      "Epoch: 115, Loss: 0.1153, Train: 0.9712, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 116, Loss: 0.1126, Train: 0.9692, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 117, Loss: 0.1090, Train: 0.9700, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 118, Loss: 0.1236, Train: 0.9637, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 119, Loss: 0.1092, Train: 0.9187, Val: 0.9100, Test: 0.9233\n",
      "Epoch: 120, Loss: 0.1365, Train: 0.9592, Val: 0.9433, Test: 0.9567\n",
      "Epoch: 121, Loss: 0.1193, Train: 0.9688, Val: 0.9633, Test: 0.9533\n",
      "Epoch: 122, Loss: 0.1274, Train: 0.9617, Val: 0.9500, Test: 0.9567\n",
      "Epoch: 123, Loss: 0.1137, Train: 0.9596, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 124, Loss: 0.1103, Train: 0.9671, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 125, Loss: 0.1204, Train: 0.9675, Val: 0.9533, Test: 0.9533\n",
      "Epoch: 126, Loss: 0.1245, Train: 0.9358, Val: 0.9267, Test: 0.9067\n",
      "Epoch: 127, Loss: 0.1488, Train: 0.9600, Val: 0.9467, Test: 0.9533\n",
      "Epoch: 128, Loss: 0.1028, Train: 0.9671, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 129, Loss: 0.1218, Train: 0.9521, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 130, Loss: 0.1088, Train: 0.9592, Val: 0.9500, Test: 0.9533\n",
      "Epoch: 131, Loss: 0.1068, Train: 0.9579, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 132, Loss: 0.1141, Train: 0.9496, Val: 0.9433, Test: 0.9467\n",
      "Epoch: 133, Loss: 0.0968, Train: 0.9700, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 134, Loss: 0.1401, Train: 0.9658, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 135, Loss: 0.1032, Train: 0.9463, Val: 0.9400, Test: 0.9333\n",
      "Epoch: 136, Loss: 0.0957, Train: 0.9558, Val: 0.9533, Test: 0.9533\n",
      "Epoch: 137, Loss: 0.0968, Train: 0.9187, Val: 0.9100, Test: 0.9267\n",
      "Epoch: 138, Loss: 0.1385, Train: 0.9417, Val: 0.9367, Test: 0.9433\n",
      "Epoch: 139, Loss: 0.1014, Train: 0.9708, Val: 0.9567, Test: 0.9633\n",
      "Epoch: 140, Loss: 0.1021, Train: 0.9663, Val: 0.9600, Test: 0.9633\n",
      "Epoch: 141, Loss: 0.1054, Train: 0.9708, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 142, Loss: 0.1310, Train: 0.9350, Val: 0.9500, Test: 0.9267\n",
      "Epoch: 143, Loss: 0.1522, Train: 0.9042, Val: 0.9100, Test: 0.9067\n",
      "Epoch: 144, Loss: 0.1276, Train: 0.9679, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 145, Loss: 0.1087, Train: 0.9600, Val: 0.9633, Test: 0.9467\n",
      "Epoch: 146, Loss: 0.1083, Train: 0.9729, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 147, Loss: 0.1101, Train: 0.9600, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 148, Loss: 0.1215, Train: 0.9696, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 149, Loss: 0.1030, Train: 0.9625, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 150, Loss: 0.1286, Train: 0.9563, Val: 0.9500, Test: 0.9633\n",
      "Epoch: 151, Loss: 0.0968, Train: 0.9704, Val: 0.9600, Test: 0.9633\n",
      "Epoch: 152, Loss: 0.1006, Train: 0.9692, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 153, Loss: 0.1089, Train: 0.9521, Val: 0.9433, Test: 0.9533\n",
      "Epoch: 154, Loss: 0.1207, Train: 0.9613, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 155, Loss: 0.1784, Train: 0.9454, Val: 0.9500, Test: 0.9433\n",
      "Epoch: 156, Loss: 0.1151, Train: 0.9596, Val: 0.9500, Test: 0.9533\n",
      "Epoch: 157, Loss: 0.1271, Train: 0.9637, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 158, Loss: 0.1025, Train: 0.9350, Val: 0.9367, Test: 0.9367\n",
      "Epoch: 159, Loss: 0.1136, Train: 0.9700, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 160, Loss: 0.1204, Train: 0.9608, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 161, Loss: 0.1060, Train: 0.9513, Val: 0.9533, Test: 0.9400\n",
      "Epoch: 162, Loss: 0.1103, Train: 0.9496, Val: 0.9500, Test: 0.9367\n",
      "Epoch: 163, Loss: 0.1064, Train: 0.9675, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 164, Loss: 0.0937, Train: 0.9717, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 165, Loss: 0.1009, Train: 0.9712, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 166, Loss: 0.1115, Train: 0.9692, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 167, Loss: 0.0947, Train: 0.9642, Val: 0.9600, Test: 0.9467\n",
      "Epoch: 168, Loss: 0.1096, Train: 0.9621, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 169, Loss: 0.1036, Train: 0.9700, Val: 0.9567, Test: 0.9633\n",
      "Epoch: 170, Loss: 0.1293, Train: 0.9663, Val: 0.9567, Test: 0.9500\n",
      "Epoch: 171, Loss: 0.1015, Train: 0.9712, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 172, Loss: 0.1992, Train: 0.9646, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 173, Loss: 0.1167, Train: 0.9633, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 174, Loss: 0.0977, Train: 0.9583, Val: 0.9567, Test: 0.9500\n",
      "Epoch: 175, Loss: 0.0899, Train: 0.9508, Val: 0.9500, Test: 0.9433\n",
      "Epoch: 176, Loss: 0.1127, Train: 0.9067, Val: 0.9200, Test: 0.9000\n",
      "Epoch: 177, Loss: 0.1103, Train: 0.9617, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 178, Loss: 0.1014, Train: 0.9463, Val: 0.9433, Test: 0.9300\n",
      "Epoch: 179, Loss: 0.1431, Train: 0.9683, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 180, Loss: 0.0937, Train: 0.9675, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 181, Loss: 0.0947, Train: 0.9679, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 182, Loss: 0.1067, Train: 0.9483, Val: 0.9433, Test: 0.9400\n",
      "Epoch: 183, Loss: 0.1292, Train: 0.9654, Val: 0.9500, Test: 0.9567\n",
      "Epoch: 184, Loss: 0.1029, Train: 0.9696, Val: 0.9533, Test: 0.9600\n",
      "Epoch: 185, Loss: 0.1188, Train: 0.9729, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 186, Loss: 0.0938, Train: 0.9704, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 187, Loss: 0.1044, Train: 0.9708, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 188, Loss: 0.1112, Train: 0.9675, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 189, Loss: 0.1167, Train: 0.9675, Val: 0.9567, Test: 0.9667\n",
      "Epoch: 190, Loss: 0.0962, Train: 0.9587, Val: 0.9467, Test: 0.9567\n",
      "Epoch: 191, Loss: 0.0991, Train: 0.9613, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 192, Loss: 0.0986, Train: 0.9708, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 193, Loss: 0.0937, Train: 0.9671, Val: 0.9500, Test: 0.9600\n",
      "Epoch: 194, Loss: 0.0996, Train: 0.9742, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 195, Loss: 0.0905, Train: 0.9721, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 196, Loss: 0.0966, Train: 0.9529, Val: 0.9500, Test: 0.9500\n",
      "Epoch: 197, Loss: 0.1196, Train: 0.9513, Val: 0.9433, Test: 0.9500\n",
      "Epoch: 198, Loss: 0.1031, Train: 0.9708, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 199, Loss: 0.0963, Train: 0.9629, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 200, Loss: 0.0926, Train: 0.9571, Val: 0.9567, Test: 0.9500\n",
      "Epoch: 201, Loss: 0.1317, Train: 0.9658, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 202, Loss: 0.1622, Train: 0.9529, Val: 0.9533, Test: 0.9500\n",
      "Epoch: 203, Loss: 0.0936, Train: 0.9708, Val: 0.9567, Test: 0.9633\n",
      "Epoch: 204, Loss: 0.0925, Train: 0.9729, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 205, Loss: 0.0955, Train: 0.9650, Val: 0.9500, Test: 0.9567\n",
      "Epoch: 206, Loss: 0.0949, Train: 0.9671, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 207, Loss: 0.0989, Train: 0.9717, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 208, Loss: 0.0997, Train: 0.9712, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 209, Loss: 0.1144, Train: 0.9650, Val: 0.9700, Test: 0.9567\n",
      "Epoch: 210, Loss: 0.1204, Train: 0.9704, Val: 0.9767, Test: 0.9633\n",
      "Epoch: 211, Loss: 0.1005, Train: 0.9671, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 212, Loss: 0.0977, Train: 0.9663, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 213, Loss: 0.1056, Train: 0.9696, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 214, Loss: 0.0879, Train: 0.9721, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 215, Loss: 0.1018, Train: 0.9717, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 216, Loss: 0.0898, Train: 0.9700, Val: 0.9600, Test: 0.9633\n",
      "Epoch: 217, Loss: 0.0959, Train: 0.9742, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 218, Loss: 0.1674, Train: 0.9525, Val: 0.9467, Test: 0.9533\n",
      "Epoch: 219, Loss: 0.1132, Train: 0.9242, Val: 0.9200, Test: 0.9367\n",
      "Epoch: 220, Loss: 0.1257, Train: 0.9537, Val: 0.9500, Test: 0.9567\n",
      "Epoch: 221, Loss: 0.0926, Train: 0.9738, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 222, Loss: 0.0969, Train: 0.9583, Val: 0.9567, Test: 0.9633\n",
      "Epoch: 223, Loss: 0.1431, Train: 0.9637, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 224, Loss: 0.0980, Train: 0.9592, Val: 0.9567, Test: 0.9400\n",
      "Epoch: 225, Loss: 0.0939, Train: 0.9733, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 226, Loss: 0.1014, Train: 0.9742, Val: 0.9667, Test: 0.9567\n",
      "Epoch: 227, Loss: 0.1097, Train: 0.9646, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 228, Loss: 0.0922, Train: 0.9683, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 229, Loss: 0.0955, Train: 0.9729, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 230, Loss: 0.0970, Train: 0.9754, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 231, Loss: 0.0976, Train: 0.9729, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 232, Loss: 0.0881, Train: 0.9754, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 233, Loss: 0.1104, Train: 0.9708, Val: 0.9700, Test: 0.9567\n",
      "Epoch: 234, Loss: 0.0960, Train: 0.9717, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 235, Loss: 0.0876, Train: 0.9721, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 236, Loss: 0.0929, Train: 0.9754, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 237, Loss: 0.0949, Train: 0.9708, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 238, Loss: 0.0955, Train: 0.9746, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 239, Loss: 0.0892, Train: 0.9621, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 240, Loss: 0.1064, Train: 0.9733, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 241, Loss: 0.0872, Train: 0.9704, Val: 0.9567, Test: 0.9633\n",
      "Epoch: 242, Loss: 0.0926, Train: 0.9754, Val: 0.9633, Test: 0.9700\n",
      "Epoch: 243, Loss: 0.0961, Train: 0.9729, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 244, Loss: 0.0875, Train: 0.9650, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 245, Loss: 0.0866, Train: 0.9758, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 246, Loss: 0.0882, Train: 0.9650, Val: 0.9500, Test: 0.9600\n",
      "Epoch: 247, Loss: 0.1445, Train: 0.9500, Val: 0.9400, Test: 0.9400\n",
      "Epoch: 248, Loss: 0.1057, Train: 0.9725, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 249, Loss: 0.0996, Train: 0.9758, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 250, Loss: 0.0905, Train: 0.9704, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 251, Loss: 0.1031, Train: 0.9700, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 252, Loss: 0.1027, Train: 0.9750, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 253, Loss: 0.1042, Train: 0.9683, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 254, Loss: 0.0990, Train: 0.9396, Val: 0.9367, Test: 0.9333\n",
      "Epoch: 255, Loss: 0.0960, Train: 0.9633, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 256, Loss: 0.0885, Train: 0.9733, Val: 0.9600, Test: 0.9633\n",
      "Epoch: 257, Loss: 0.0858, Train: 0.9746, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 258, Loss: 0.0934, Train: 0.9721, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 259, Loss: 0.0874, Train: 0.9704, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 260, Loss: 0.1083, Train: 0.9621, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 261, Loss: 0.0973, Train: 0.9608, Val: 0.9567, Test: 0.9433\n",
      "Epoch: 262, Loss: 0.0920, Train: 0.9750, Val: 0.9733, Test: 0.9567\n",
      "Epoch: 263, Loss: 0.1023, Train: 0.9629, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 264, Loss: 0.0941, Train: 0.9688, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 265, Loss: 0.0847, Train: 0.9746, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 266, Loss: 0.0881, Train: 0.9546, Val: 0.9533, Test: 0.9533\n",
      "Epoch: 267, Loss: 0.0885, Train: 0.9750, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 268, Loss: 0.0952, Train: 0.9637, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 269, Loss: 0.0877, Train: 0.9563, Val: 0.9567, Test: 0.9500\n",
      "Epoch: 270, Loss: 0.1158, Train: 0.9767, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 271, Loss: 0.0844, Train: 0.9717, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 272, Loss: 0.0874, Train: 0.9596, Val: 0.9467, Test: 0.9567\n",
      "Epoch: 273, Loss: 0.0825, Train: 0.9775, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 274, Loss: 0.0991, Train: 0.9742, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 275, Loss: 0.0892, Train: 0.9700, Val: 0.9633, Test: 0.9700\n",
      "Epoch: 276, Loss: 0.1056, Train: 0.8771, Val: 0.8633, Test: 0.8900\n",
      "Epoch: 277, Loss: 0.1641, Train: 0.9637, Val: 0.9567, Test: 0.9433\n",
      "Epoch: 278, Loss: 0.0951, Train: 0.9700, Val: 0.9633, Test: 0.9700\n",
      "Epoch: 279, Loss: 0.0869, Train: 0.9667, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 280, Loss: 0.0918, Train: 0.9613, Val: 0.9600, Test: 0.9533\n",
      "Epoch: 281, Loss: 0.0863, Train: 0.9696, Val: 0.9567, Test: 0.9567\n",
      "Epoch: 282, Loss: 0.1053, Train: 0.9738, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 283, Loss: 0.1049, Train: 0.9658, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 284, Loss: 0.0805, Train: 0.9721, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 285, Loss: 0.0826, Train: 0.9742, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 286, Loss: 0.0918, Train: 0.9750, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 287, Loss: 0.0875, Train: 0.9771, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 288, Loss: 0.0842, Train: 0.9750, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 289, Loss: 0.0968, Train: 0.9717, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 290, Loss: 0.1450, Train: 0.9563, Val: 0.9533, Test: 0.9500\n",
      "Epoch: 291, Loss: 0.0891, Train: 0.9754, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 292, Loss: 0.0923, Train: 0.9758, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 293, Loss: 0.0853, Train: 0.9663, Val: 0.9600, Test: 0.9533\n",
      "Epoch: 294, Loss: 0.0905, Train: 0.9721, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 295, Loss: 0.0932, Train: 0.9642, Val: 0.9533, Test: 0.9467\n",
      "Epoch: 296, Loss: 0.0884, Train: 0.9642, Val: 0.9533, Test: 0.9500\n",
      "Epoch: 297, Loss: 0.0873, Train: 0.9658, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 298, Loss: 0.1057, Train: 0.9671, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 299, Loss: 0.0903, Train: 0.9754, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 300, Loss: 0.0821, Train: 0.9667, Val: 0.9500, Test: 0.9600\n",
      "Epoch: 301, Loss: 0.0987, Train: 0.9762, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 302, Loss: 0.0901, Train: 0.9629, Val: 0.9533, Test: 0.9500\n",
      "Epoch: 303, Loss: 0.0830, Train: 0.9696, Val: 0.9600, Test: 0.9767\n",
      "Epoch: 304, Loss: 0.0862, Train: 0.9742, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 305, Loss: 0.0795, Train: 0.9754, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 306, Loss: 0.0884, Train: 0.9725, Val: 0.9533, Test: 0.9667\n",
      "Epoch: 307, Loss: 0.0896, Train: 0.9513, Val: 0.9500, Test: 0.9500\n",
      "Epoch: 308, Loss: 0.0846, Train: 0.9792, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 309, Loss: 0.0824, Train: 0.9738, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 310, Loss: 0.0756, Train: 0.9762, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 311, Loss: 0.0924, Train: 0.9625, Val: 0.9567, Test: 0.9533\n",
      "Epoch: 312, Loss: 0.1323, Train: 0.9700, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 313, Loss: 0.0838, Train: 0.9754, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 314, Loss: 0.1044, Train: 0.9712, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 315, Loss: 0.0856, Train: 0.9771, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 316, Loss: 0.1025, Train: 0.9575, Val: 0.9567, Test: 0.9367\n",
      "Epoch: 317, Loss: 0.0917, Train: 0.9658, Val: 0.9500, Test: 0.9600\n",
      "Epoch: 318, Loss: 0.0796, Train: 0.9775, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 319, Loss: 0.0793, Train: 0.9733, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 320, Loss: 0.0854, Train: 0.9729, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 321, Loss: 0.0911, Train: 0.9762, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 322, Loss: 0.0881, Train: 0.9642, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 323, Loss: 0.0899, Train: 0.9758, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 324, Loss: 0.0768, Train: 0.9733, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 325, Loss: 0.0861, Train: 0.9783, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 326, Loss: 0.1071, Train: 0.9754, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 327, Loss: 0.0890, Train: 0.9762, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 328, Loss: 0.1013, Train: 0.9750, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 329, Loss: 0.0776, Train: 0.9771, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 330, Loss: 0.0971, Train: 0.9700, Val: 0.9567, Test: 0.9633\n",
      "Epoch: 331, Loss: 0.1056, Train: 0.9704, Val: 0.9700, Test: 0.9567\n",
      "Epoch: 332, Loss: 0.0907, Train: 0.9717, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 333, Loss: 0.0857, Train: 0.9729, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 334, Loss: 0.0941, Train: 0.9779, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 335, Loss: 0.0865, Train: 0.9758, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 336, Loss: 0.0911, Train: 0.9717, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 337, Loss: 0.0850, Train: 0.9750, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 338, Loss: 0.0913, Train: 0.9679, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 339, Loss: 0.0840, Train: 0.9775, Val: 0.9600, Test: 0.9700\n",
      "Epoch: 340, Loss: 0.0803, Train: 0.9758, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 341, Loss: 0.0869, Train: 0.9663, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 342, Loss: 0.0833, Train: 0.9754, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 343, Loss: 0.0708, Train: 0.9692, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 344, Loss: 0.0922, Train: 0.9767, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 345, Loss: 0.0824, Train: 0.9679, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 346, Loss: 0.0817, Train: 0.9754, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 347, Loss: 0.0873, Train: 0.9758, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 348, Loss: 0.0806, Train: 0.9758, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 349, Loss: 0.0848, Train: 0.9754, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 350, Loss: 0.0892, Train: 0.9767, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 351, Loss: 0.0720, Train: 0.9767, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 352, Loss: 0.0760, Train: 0.9796, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 353, Loss: 0.0872, Train: 0.9771, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 354, Loss: 0.0815, Train: 0.9742, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 355, Loss: 0.0872, Train: 0.9771, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 356, Loss: 0.1017, Train: 0.9571, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 357, Loss: 0.0864, Train: 0.9762, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 358, Loss: 0.0733, Train: 0.9767, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 359, Loss: 0.0848, Train: 0.9696, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 360, Loss: 0.0785, Train: 0.9650, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 361, Loss: 0.1088, Train: 0.9708, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 362, Loss: 0.0802, Train: 0.9762, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 363, Loss: 0.0777, Train: 0.9708, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 364, Loss: 0.0789, Train: 0.9783, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 365, Loss: 0.0767, Train: 0.9779, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 366, Loss: 0.0823, Train: 0.9746, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 367, Loss: 0.0818, Train: 0.9704, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 368, Loss: 0.0901, Train: 0.9758, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 369, Loss: 0.0979, Train: 0.9733, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 370, Loss: 0.0794, Train: 0.9692, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 371, Loss: 0.0781, Train: 0.9758, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 372, Loss: 0.0775, Train: 0.9767, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 373, Loss: 0.0855, Train: 0.9671, Val: 0.9667, Test: 0.9533\n",
      "Epoch: 374, Loss: 0.0954, Train: 0.9375, Val: 0.9267, Test: 0.9467\n",
      "Epoch: 375, Loss: 0.0850, Train: 0.9704, Val: 0.9633, Test: 0.9600\n",
      "Epoch: 376, Loss: 0.0946, Train: 0.9692, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 377, Loss: 0.0977, Train: 0.9721, Val: 0.9567, Test: 0.9633\n",
      "Epoch: 378, Loss: 0.0909, Train: 0.9746, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 379, Loss: 0.0795, Train: 0.9783, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 380, Loss: 0.0746, Train: 0.9700, Val: 0.9600, Test: 0.9667\n",
      "Epoch: 381, Loss: 0.0736, Train: 0.9762, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 382, Loss: 0.0861, Train: 0.9746, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 383, Loss: 0.0821, Train: 0.9800, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 384, Loss: 0.0740, Train: 0.9783, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 385, Loss: 0.0835, Train: 0.9692, Val: 0.9533, Test: 0.9633\n",
      "Epoch: 386, Loss: 0.0920, Train: 0.9792, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 387, Loss: 0.0652, Train: 0.9800, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 388, Loss: 0.0772, Train: 0.9796, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 389, Loss: 0.0838, Train: 0.9700, Val: 0.9600, Test: 0.9567\n",
      "Epoch: 390, Loss: 0.0697, Train: 0.9800, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 391, Loss: 0.0810, Train: 0.9796, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 392, Loss: 0.0829, Train: 0.9788, Val: 0.9767, Test: 0.9667\n",
      "Epoch: 393, Loss: 0.0828, Train: 0.9771, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 394, Loss: 0.0721, Train: 0.9804, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 395, Loss: 0.0788, Train: 0.9792, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 396, Loss: 0.0814, Train: 0.9762, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 397, Loss: 0.0708, Train: 0.9754, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 398, Loss: 0.0748, Train: 0.9717, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 399, Loss: 0.0763, Train: 0.9796, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 400, Loss: 0.0765, Train: 0.9775, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 401, Loss: 0.0708, Train: 0.9821, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 402, Loss: 0.0773, Train: 0.9788, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 403, Loss: 0.1107, Train: 0.9663, Val: 0.9533, Test: 0.9567\n",
      "Epoch: 404, Loss: 0.0797, Train: 0.9771, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 405, Loss: 0.0748, Train: 0.9796, Val: 0.9767, Test: 0.9667\n",
      "Epoch: 406, Loss: 0.0905, Train: 0.9604, Val: 0.9567, Test: 0.9600\n",
      "Epoch: 407, Loss: 0.0797, Train: 0.9775, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 408, Loss: 0.0726, Train: 0.9800, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 409, Loss: 0.1016, Train: 0.9712, Val: 0.9733, Test: 0.9567\n",
      "Epoch: 410, Loss: 0.0790, Train: 0.9804, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 411, Loss: 0.0700, Train: 0.9742, Val: 0.9600, Test: 0.9733\n",
      "Epoch: 412, Loss: 0.0773, Train: 0.9775, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 413, Loss: 0.0695, Train: 0.9829, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 414, Loss: 0.0673, Train: 0.9754, Val: 0.9633, Test: 0.9667\n",
      "Epoch: 415, Loss: 0.0718, Train: 0.9812, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 416, Loss: 0.0762, Train: 0.9721, Val: 0.9633, Test: 0.9700\n",
      "Epoch: 417, Loss: 0.0721, Train: 0.9762, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 418, Loss: 0.0709, Train: 0.9817, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 419, Loss: 0.0748, Train: 0.9808, Val: 0.9667, Test: 0.9700\n",
      "Epoch: 420, Loss: 0.0866, Train: 0.9704, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 421, Loss: 0.0759, Train: 0.9792, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 422, Loss: 0.0783, Train: 0.9771, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 423, Loss: 0.0805, Train: 0.9800, Val: 0.9633, Test: 0.9700\n",
      "Epoch: 424, Loss: 0.0841, Train: 0.9721, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 425, Loss: 0.0775, Train: 0.9758, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 426, Loss: 0.0760, Train: 0.9371, Val: 0.9300, Test: 0.9633\n",
      "Epoch: 427, Loss: 0.0864, Train: 0.9758, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 428, Loss: 0.0748, Train: 0.9779, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 429, Loss: 0.0667, Train: 0.9762, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 430, Loss: 0.0687, Train: 0.9800, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 431, Loss: 0.0681, Train: 0.9812, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 432, Loss: 0.0843, Train: 0.9617, Val: 0.9633, Test: 0.9433\n",
      "Epoch: 433, Loss: 0.0817, Train: 0.9804, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 434, Loss: 0.0676, Train: 0.9804, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 435, Loss: 0.0731, Train: 0.9779, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 436, Loss: 0.0723, Train: 0.9738, Val: 0.9633, Test: 0.9633\n",
      "Epoch: 437, Loss: 0.0772, Train: 0.9825, Val: 0.9767, Test: 0.9633\n",
      "Epoch: 438, Loss: 0.0779, Train: 0.9804, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 439, Loss: 0.0664, Train: 0.9812, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 440, Loss: 0.0683, Train: 0.9775, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 441, Loss: 0.0820, Train: 0.9583, Val: 0.9633, Test: 0.9467\n",
      "Epoch: 442, Loss: 0.0740, Train: 0.9812, Val: 0.9700, Test: 0.9767\n",
      "Epoch: 443, Loss: 0.0895, Train: 0.9742, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 444, Loss: 0.0657, Train: 0.9704, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 445, Loss: 0.0670, Train: 0.9804, Val: 0.9767, Test: 0.9600\n",
      "Epoch: 446, Loss: 0.0720, Train: 0.9783, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 447, Loss: 0.0813, Train: 0.9704, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 448, Loss: 0.0780, Train: 0.9775, Val: 0.9700, Test: 0.9567\n",
      "Epoch: 449, Loss: 0.0707, Train: 0.9804, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 450, Loss: 0.0721, Train: 0.9800, Val: 0.9733, Test: 0.9533\n",
      "Epoch: 451, Loss: 0.0710, Train: 0.9804, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 452, Loss: 0.0679, Train: 0.9804, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 453, Loss: 0.0674, Train: 0.9821, Val: 0.9767, Test: 0.9733\n",
      "Epoch: 454, Loss: 0.0631, Train: 0.9829, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 455, Loss: 0.0802, Train: 0.9817, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 456, Loss: 0.0643, Train: 0.9796, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 457, Loss: 0.0680, Train: 0.9742, Val: 0.9667, Test: 0.9600\n",
      "Epoch: 458, Loss: 0.0687, Train: 0.9775, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 459, Loss: 0.0697, Train: 0.9812, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 460, Loss: 0.0765, Train: 0.9817, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 461, Loss: 0.0633, Train: 0.9767, Val: 0.9667, Test: 0.9667\n",
      "Epoch: 462, Loss: 0.0620, Train: 0.9825, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 463, Loss: 0.0724, Train: 0.9829, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 464, Loss: 0.0724, Train: 0.9817, Val: 0.9733, Test: 0.9767\n",
      "Epoch: 465, Loss: 0.0700, Train: 0.9758, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 466, Loss: 0.0676, Train: 0.9792, Val: 0.9733, Test: 0.9533\n",
      "Epoch: 467, Loss: 0.0775, Train: 0.9767, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 468, Loss: 0.0699, Train: 0.9838, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 469, Loss: 0.0624, Train: 0.9812, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 470, Loss: 0.0757, Train: 0.9788, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 471, Loss: 0.0660, Train: 0.9783, Val: 0.9733, Test: 0.9700\n",
      "Epoch: 472, Loss: 0.0705, Train: 0.9779, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 473, Loss: 0.0734, Train: 0.9800, Val: 0.9733, Test: 0.9733\n",
      "Epoch: 474, Loss: 0.0690, Train: 0.9779, Val: 0.9667, Test: 0.9633\n",
      "Epoch: 475, Loss: 0.0753, Train: 0.9754, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 476, Loss: 0.0771, Train: 0.9650, Val: 0.9600, Test: 0.9500\n",
      "Epoch: 477, Loss: 0.0751, Train: 0.9842, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 478, Loss: 0.0692, Train: 0.9796, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 479, Loss: 0.0698, Train: 0.9800, Val: 0.9700, Test: 0.9567\n",
      "Epoch: 480, Loss: 0.0662, Train: 0.9808, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 481, Loss: 0.0900, Train: 0.9733, Val: 0.9600, Test: 0.9600\n",
      "Epoch: 482, Loss: 0.0672, Train: 0.9804, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 483, Loss: 0.0607, Train: 0.9829, Val: 0.9667, Test: 0.9733\n",
      "Epoch: 484, Loss: 0.0649, Train: 0.9808, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 485, Loss: 0.0784, Train: 0.9800, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 486, Loss: 0.0724, Train: 0.9696, Val: 0.9700, Test: 0.9533\n",
      "Epoch: 487, Loss: 0.0614, Train: 0.9812, Val: 0.9733, Test: 0.9600\n",
      "Epoch: 488, Loss: 0.0706, Train: 0.9808, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 489, Loss: 0.0685, Train: 0.9700, Val: 0.9500, Test: 0.9567\n",
      "Epoch: 490, Loss: 0.0708, Train: 0.9821, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 491, Loss: 0.0646, Train: 0.9833, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 492, Loss: 0.0649, Train: 0.9792, Val: 0.9700, Test: 0.9633\n",
      "Epoch: 493, Loss: 0.0700, Train: 0.9800, Val: 0.9733, Test: 0.9633\n",
      "Epoch: 494, Loss: 0.0669, Train: 0.9838, Val: 0.9733, Test: 0.9667\n",
      "Epoch: 495, Loss: 0.0603, Train: 0.9733, Val: 0.9700, Test: 0.9667\n",
      "Epoch: 496, Loss: 0.0752, Train: 0.9817, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 497, Loss: 0.0649, Train: 0.9842, Val: 0.9700, Test: 0.9733\n",
      "Epoch: 498, Loss: 0.0654, Train: 0.9846, Val: 0.9700, Test: 0.9700\n",
      "Epoch: 499, Loss: 0.0695, Train: 0.9792, Val: 0.9700, Test: 0.9600\n",
      "Epoch: 500, Loss: 0.0659, Train: 0.9762, Val: 0.9733, Test: 0.9600\n",
      "### Run 0 - val loss: 0.069, test acc: 0.973\n",
      "Accuracies in each run:  [0.9733333333333334]\n",
      "test acc - mean: 0.973, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'sagpool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense-random\n",
      "Epoch: 001, Loss: 1.8676, Train: 0.6613, Val: 0.6633, Test: 0.6600\n",
      "Epoch: 002, Loss: 0.7858, Train: 0.7283, Val: 0.7500, Test: 0.7400\n",
      "Epoch: 003, Loss: 0.7029, Train: 0.7462, Val: 0.7567, Test: 0.7733\n",
      "Epoch: 004, Loss: 0.6056, Train: 0.6913, Val: 0.7100, Test: 0.6933\n",
      "Epoch: 005, Loss: 0.5591, Train: 0.5600, Val: 0.5800, Test: 0.5567\n",
      "Epoch: 006, Loss: 0.5409, Train: 0.7642, Val: 0.7500, Test: 0.7633\n",
      "Epoch: 007, Loss: 0.5011, Train: 0.8142, Val: 0.8100, Test: 0.8300\n",
      "Epoch: 008, Loss: 0.4773, Train: 0.8233, Val: 0.8233, Test: 0.8433\n",
      "Epoch: 009, Loss: 0.4199, Train: 0.8017, Val: 0.8133, Test: 0.8433\n",
      "Epoch: 010, Loss: 0.4124, Train: 0.8283, Val: 0.8133, Test: 0.8433\n",
      "Epoch: 011, Loss: 0.3978, Train: 0.8733, Val: 0.8700, Test: 0.8600\n",
      "Epoch: 012, Loss: 0.3996, Train: 0.7087, Val: 0.7167, Test: 0.7100\n",
      "Epoch: 013, Loss: 0.3868, Train: 0.8467, Val: 0.8567, Test: 0.8500\n",
      "Epoch: 014, Loss: 0.3518, Train: 0.8413, Val: 0.8467, Test: 0.8600\n",
      "Epoch: 015, Loss: 0.3800, Train: 0.8871, Val: 0.8933, Test: 0.8800\n",
      "Epoch: 016, Loss: 0.3778, Train: 0.7821, Val: 0.8100, Test: 0.7900\n",
      "Epoch: 017, Loss: 0.3735, Train: 0.8846, Val: 0.9000, Test: 0.8633\n",
      "Epoch: 018, Loss: 0.3489, Train: 0.7692, Val: 0.7967, Test: 0.8033\n",
      "Epoch: 019, Loss: 0.3448, Train: 0.8742, Val: 0.8667, Test: 0.8867\n",
      "Epoch: 020, Loss: 0.3867, Train: 0.8617, Val: 0.8633, Test: 0.8700\n",
      "Epoch: 021, Loss: 0.3478, Train: 0.8371, Val: 0.8400, Test: 0.8167\n",
      "Epoch: 022, Loss: 0.3100, Train: 0.8413, Val: 0.8567, Test: 0.8700\n",
      "Epoch: 023, Loss: 0.3128, Train: 0.8871, Val: 0.8933, Test: 0.8733\n",
      "Epoch: 024, Loss: 0.3150, Train: 0.7842, Val: 0.8133, Test: 0.8200\n",
      "Epoch: 025, Loss: 0.3113, Train: 0.6225, Val: 0.6600, Test: 0.6267\n",
      "Epoch: 026, Loss: 0.3163, Train: 0.8975, Val: 0.8800, Test: 0.9000\n",
      "Epoch: 027, Loss: 0.3257, Train: 0.8712, Val: 0.8567, Test: 0.8500\n",
      "Epoch: 028, Loss: 0.3029, Train: 0.8100, Val: 0.8133, Test: 0.8300\n",
      "Epoch: 029, Loss: 0.3687, Train: 0.8462, Val: 0.8633, Test: 0.8567\n",
      "Epoch: 030, Loss: 0.2955, Train: 0.8404, Val: 0.8567, Test: 0.8733\n",
      "Epoch: 031, Loss: 0.3030, Train: 0.8304, Val: 0.8500, Test: 0.8167\n",
      "Epoch: 032, Loss: 0.3088, Train: 0.9058, Val: 0.9000, Test: 0.8833\n",
      "Epoch: 033, Loss: 0.2959, Train: 0.8996, Val: 0.9067, Test: 0.8933\n",
      "Epoch: 034, Loss: 0.3426, Train: 0.8558, Val: 0.8700, Test: 0.8833\n",
      "Epoch: 035, Loss: 0.2969, Train: 0.8458, Val: 0.8733, Test: 0.8367\n",
      "Epoch: 036, Loss: 0.2844, Train: 0.7875, Val: 0.7967, Test: 0.7967\n",
      "Epoch: 037, Loss: 0.2927, Train: 0.8567, Val: 0.8767, Test: 0.8367\n",
      "Epoch: 038, Loss: 0.3044, Train: 0.8296, Val: 0.8533, Test: 0.8333\n",
      "Epoch: 039, Loss: 0.3067, Train: 0.8225, Val: 0.8200, Test: 0.8600\n",
      "Epoch: 040, Loss: 0.3116, Train: 0.8854, Val: 0.8833, Test: 0.8900\n",
      "Epoch: 041, Loss: 0.2865, Train: 0.9146, Val: 0.9100, Test: 0.8967\n",
      "Epoch: 042, Loss: 0.2885, Train: 0.8983, Val: 0.9067, Test: 0.8800\n",
      "Epoch: 043, Loss: 0.2962, Train: 0.7933, Val: 0.8067, Test: 0.8300\n",
      "Epoch: 044, Loss: 0.3014, Train: 0.7462, Val: 0.7700, Test: 0.7767\n",
      "Epoch: 045, Loss: 0.2720, Train: 0.8892, Val: 0.8733, Test: 0.8633\n",
      "Epoch: 046, Loss: 0.2755, Train: 0.8983, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 047, Loss: 0.2876, Train: 0.8233, Val: 0.8300, Test: 0.8400\n",
      "Epoch: 048, Loss: 0.2755, Train: 0.9154, Val: 0.9100, Test: 0.8967\n",
      "Epoch: 049, Loss: 0.2700, Train: 0.8758, Val: 0.8833, Test: 0.8900\n",
      "Epoch: 050, Loss: 0.2944, Train: 0.8771, Val: 0.8800, Test: 0.8433\n",
      "Epoch: 051, Loss: 0.2697, Train: 0.9008, Val: 0.8900, Test: 0.8967\n",
      "Epoch: 052, Loss: 0.2697, Train: 0.8767, Val: 0.8867, Test: 0.8833\n",
      "Epoch: 053, Loss: 0.2809, Train: 0.8858, Val: 0.8767, Test: 0.8767\n",
      "Epoch: 054, Loss: 0.2847, Train: 0.8683, Val: 0.8733, Test: 0.8800\n",
      "Epoch: 055, Loss: 0.2815, Train: 0.9117, Val: 0.9167, Test: 0.8833\n",
      "Epoch: 056, Loss: 0.2884, Train: 0.8158, Val: 0.8300, Test: 0.8233\n",
      "Epoch: 057, Loss: 0.2783, Train: 0.8946, Val: 0.8800, Test: 0.8833\n",
      "Epoch: 058, Loss: 0.2866, Train: 0.9096, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 059, Loss: 0.2556, Train: 0.9033, Val: 0.8867, Test: 0.8967\n",
      "Epoch: 060, Loss: 0.2690, Train: 0.9175, Val: 0.9067, Test: 0.8867\n",
      "Epoch: 061, Loss: 0.2808, Train: 0.9167, Val: 0.9100, Test: 0.8867\n",
      "Epoch: 062, Loss: 0.2779, Train: 0.8954, Val: 0.8867, Test: 0.8833\n",
      "Epoch: 063, Loss: 0.2791, Train: 0.8812, Val: 0.8767, Test: 0.8833\n",
      "Epoch: 064, Loss: 0.2741, Train: 0.9033, Val: 0.9033, Test: 0.8800\n",
      "Epoch: 065, Loss: 0.2574, Train: 0.8842, Val: 0.8933, Test: 0.8833\n",
      "Epoch: 066, Loss: 0.2578, Train: 0.9079, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 067, Loss: 0.2858, Train: 0.8817, Val: 0.8767, Test: 0.8800\n",
      "Epoch: 068, Loss: 0.2796, Train: 0.9117, Val: 0.8967, Test: 0.8967\n",
      "Epoch: 069, Loss: 0.2482, Train: 0.8671, Val: 0.8633, Test: 0.8767\n",
      "Epoch: 070, Loss: 0.2550, Train: 0.9171, Val: 0.9133, Test: 0.8900\n",
      "Epoch: 071, Loss: 0.2817, Train: 0.8033, Val: 0.8233, Test: 0.8067\n",
      "Epoch: 072, Loss: 0.2904, Train: 0.9121, Val: 0.9067, Test: 0.9033\n",
      "Epoch: 073, Loss: 0.2746, Train: 0.9113, Val: 0.9100, Test: 0.8967\n",
      "Epoch: 074, Loss: 0.2455, Train: 0.8796, Val: 0.8833, Test: 0.9000\n",
      "Epoch: 075, Loss: 0.2472, Train: 0.8275, Val: 0.8333, Test: 0.8533\n",
      "Epoch: 076, Loss: 0.2625, Train: 0.9083, Val: 0.9133, Test: 0.8933\n",
      "Epoch: 077, Loss: 0.2497, Train: 0.9004, Val: 0.9033, Test: 0.8933\n",
      "Epoch: 078, Loss: 0.2434, Train: 0.8488, Val: 0.8467, Test: 0.8600\n",
      "Epoch: 079, Loss: 0.2539, Train: 0.8812, Val: 0.8933, Test: 0.8600\n",
      "Epoch: 080, Loss: 0.2790, Train: 0.9192, Val: 0.9067, Test: 0.9100\n",
      "Epoch: 081, Loss: 0.2426, Train: 0.9171, Val: 0.9067, Test: 0.9033\n",
      "Epoch: 082, Loss: 0.2503, Train: 0.8746, Val: 0.8833, Test: 0.8467\n",
      "Epoch: 083, Loss: 0.2708, Train: 0.9204, Val: 0.9033, Test: 0.9067\n",
      "Epoch: 084, Loss: 0.2714, Train: 0.8592, Val: 0.8533, Test: 0.8833\n",
      "Epoch: 085, Loss: 0.2379, Train: 0.8979, Val: 0.8900, Test: 0.9000\n",
      "Epoch: 086, Loss: 0.2711, Train: 0.9117, Val: 0.9000, Test: 0.8767\n",
      "Epoch: 087, Loss: 0.2756, Train: 0.8617, Val: 0.8633, Test: 0.8433\n",
      "Epoch: 088, Loss: 0.2462, Train: 0.9050, Val: 0.9000, Test: 0.9100\n",
      "Epoch: 089, Loss: 0.2491, Train: 0.9225, Val: 0.9067, Test: 0.8900\n",
      "Epoch: 090, Loss: 0.2625, Train: 0.9058, Val: 0.8800, Test: 0.8667\n",
      "Epoch: 091, Loss: 0.2610, Train: 0.9154, Val: 0.9033, Test: 0.9100\n",
      "Epoch: 092, Loss: 0.2680, Train: 0.9100, Val: 0.8933, Test: 0.9000\n",
      "Epoch: 093, Loss: 0.2646, Train: 0.8750, Val: 0.8900, Test: 0.8467\n",
      "Epoch: 094, Loss: 0.2438, Train: 0.8692, Val: 0.8633, Test: 0.8733\n",
      "Epoch: 095, Loss: 0.2429, Train: 0.9025, Val: 0.8967, Test: 0.8567\n",
      "Epoch: 096, Loss: 0.2614, Train: 0.9179, Val: 0.9100, Test: 0.9133\n",
      "Epoch: 097, Loss: 0.2316, Train: 0.9179, Val: 0.9033, Test: 0.8900\n",
      "Epoch: 098, Loss: 0.2423, Train: 0.8854, Val: 0.8967, Test: 0.8967\n",
      "Epoch: 099, Loss: 0.2316, Train: 0.8625, Val: 0.8733, Test: 0.8433\n",
      "Epoch: 100, Loss: 0.2467, Train: 0.9025, Val: 0.9000, Test: 0.8900\n",
      "Epoch: 101, Loss: 0.2541, Train: 0.8237, Val: 0.8200, Test: 0.8500\n",
      "Epoch: 102, Loss: 0.2359, Train: 0.8821, Val: 0.8733, Test: 0.8533\n",
      "Epoch: 103, Loss: 0.2515, Train: 0.9079, Val: 0.8900, Test: 0.9100\n",
      "Epoch: 104, Loss: 0.2443, Train: 0.9142, Val: 0.9067, Test: 0.9000\n",
      "Epoch: 105, Loss: 0.2538, Train: 0.9125, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 106, Loss: 0.2527, Train: 0.9200, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 107, Loss: 0.2471, Train: 0.8708, Val: 0.8767, Test: 0.8633\n",
      "Epoch: 108, Loss: 0.2518, Train: 0.9237, Val: 0.9000, Test: 0.9000\n",
      "Epoch: 109, Loss: 0.2472, Train: 0.9204, Val: 0.9067, Test: 0.9033\n",
      "Epoch: 110, Loss: 0.2318, Train: 0.8808, Val: 0.8867, Test: 0.9033\n",
      "Epoch: 111, Loss: 0.2364, Train: 0.8967, Val: 0.8933, Test: 0.8967\n",
      "Epoch: 112, Loss: 0.2486, Train: 0.8500, Val: 0.8733, Test: 0.8467\n",
      "Epoch: 113, Loss: 0.2386, Train: 0.8588, Val: 0.8600, Test: 0.8367\n",
      "Epoch: 114, Loss: 0.2570, Train: 0.9096, Val: 0.9033, Test: 0.9100\n",
      "Epoch: 115, Loss: 0.2448, Train: 0.9208, Val: 0.9200, Test: 0.8967\n",
      "Epoch: 116, Loss: 0.2400, Train: 0.7896, Val: 0.7933, Test: 0.8100\n",
      "Epoch: 117, Loss: 0.2312, Train: 0.9246, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 118, Loss: 0.2259, Train: 0.8417, Val: 0.8433, Test: 0.8567\n",
      "Epoch: 119, Loss: 0.2327, Train: 0.8925, Val: 0.8933, Test: 0.9067\n",
      "Epoch: 120, Loss: 0.2401, Train: 0.8821, Val: 0.8633, Test: 0.8867\n",
      "Epoch: 121, Loss: 0.2487, Train: 0.7725, Val: 0.7867, Test: 0.7800\n",
      "Epoch: 122, Loss: 0.2505, Train: 0.9179, Val: 0.9100, Test: 0.9033\n",
      "Epoch: 123, Loss: 0.2513, Train: 0.9113, Val: 0.8967, Test: 0.8900\n",
      "Epoch: 124, Loss: 0.2281, Train: 0.9258, Val: 0.9100, Test: 0.8933\n",
      "Epoch: 125, Loss: 0.2575, Train: 0.8662, Val: 0.8667, Test: 0.8433\n",
      "Epoch: 126, Loss: 0.2387, Train: 0.8758, Val: 0.8733, Test: 0.8567\n",
      "Epoch: 127, Loss: 0.2362, Train: 0.8725, Val: 0.8867, Test: 0.8533\n",
      "Epoch: 128, Loss: 0.2310, Train: 0.8454, Val: 0.8633, Test: 0.8733\n",
      "Epoch: 129, Loss: 0.2466, Train: 0.8725, Val: 0.8733, Test: 0.8800\n",
      "Epoch: 130, Loss: 0.2549, Train: 0.9242, Val: 0.9167, Test: 0.9000\n",
      "Epoch: 131, Loss: 0.2463, Train: 0.9113, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 132, Loss: 0.2597, Train: 0.9025, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 133, Loss: 0.2295, Train: 0.9275, Val: 0.9067, Test: 0.9167\n",
      "Epoch: 134, Loss: 0.2560, Train: 0.8217, Val: 0.8233, Test: 0.8433\n",
      "Epoch: 135, Loss: 0.2242, Train: 0.9217, Val: 0.9067, Test: 0.8933\n",
      "Epoch: 136, Loss: 0.2443, Train: 0.9154, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 137, Loss: 0.2265, Train: 0.8533, Val: 0.8600, Test: 0.8400\n",
      "Epoch: 138, Loss: 0.2561, Train: 0.9042, Val: 0.8900, Test: 0.9100\n",
      "Epoch: 139, Loss: 0.2322, Train: 0.9233, Val: 0.9167, Test: 0.9200\n",
      "Epoch: 140, Loss: 0.2346, Train: 0.8879, Val: 0.8933, Test: 0.9033\n",
      "Epoch: 141, Loss: 0.2222, Train: 0.9179, Val: 0.9100, Test: 0.9100\n",
      "Epoch: 142, Loss: 0.2593, Train: 0.9233, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 143, Loss: 0.2271, Train: 0.8529, Val: 0.8600, Test: 0.8367\n",
      "Epoch: 144, Loss: 0.2220, Train: 0.9287, Val: 0.9033, Test: 0.9100\n",
      "Epoch: 145, Loss: 0.2403, Train: 0.9038, Val: 0.9100, Test: 0.8967\n",
      "Epoch: 146, Loss: 0.2240, Train: 0.9196, Val: 0.9133, Test: 0.8833\n",
      "Epoch: 147, Loss: 0.2271, Train: 0.9258, Val: 0.9100, Test: 0.9067\n",
      "Epoch: 148, Loss: 0.2231, Train: 0.9092, Val: 0.9033, Test: 0.8567\n",
      "Epoch: 149, Loss: 0.2407, Train: 0.9271, Val: 0.9100, Test: 0.9033\n",
      "Epoch: 150, Loss: 0.2275, Train: 0.8933, Val: 0.8900, Test: 0.9100\n",
      "Epoch: 151, Loss: 0.2120, Train: 0.8717, Val: 0.8833, Test: 0.8933\n",
      "Epoch: 152, Loss: 0.2337, Train: 0.8988, Val: 0.8933, Test: 0.8900\n",
      "Epoch: 153, Loss: 0.2386, Train: 0.9175, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 154, Loss: 0.2384, Train: 0.9133, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 155, Loss: 0.2255, Train: 0.8271, Val: 0.8433, Test: 0.8400\n",
      "Epoch: 156, Loss: 0.2259, Train: 0.9213, Val: 0.9133, Test: 0.9000\n",
      "Epoch: 157, Loss: 0.2235, Train: 0.9083, Val: 0.9133, Test: 0.8767\n",
      "Epoch: 158, Loss: 0.2269, Train: 0.7867, Val: 0.7833, Test: 0.8167\n",
      "Epoch: 159, Loss: 0.2296, Train: 0.9129, Val: 0.9100, Test: 0.9067\n",
      "Epoch: 160, Loss: 0.2288, Train: 0.9008, Val: 0.8967, Test: 0.9067\n",
      "Epoch: 161, Loss: 0.2787, Train: 0.9038, Val: 0.9100, Test: 0.9133\n",
      "Epoch: 162, Loss: 0.2260, Train: 0.9196, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 163, Loss: 0.2177, Train: 0.9104, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 164, Loss: 0.2168, Train: 0.8912, Val: 0.8867, Test: 0.8767\n",
      "Epoch: 165, Loss: 0.2491, Train: 0.8488, Val: 0.8433, Test: 0.8300\n",
      "Epoch: 166, Loss: 0.2402, Train: 0.9208, Val: 0.9200, Test: 0.9167\n",
      "Epoch: 167, Loss: 0.2243, Train: 0.8625, Val: 0.8633, Test: 0.8767\n",
      "Epoch: 168, Loss: 0.2144, Train: 0.8571, Val: 0.8467, Test: 0.8300\n",
      "Epoch: 169, Loss: 0.2391, Train: 0.9292, Val: 0.9167, Test: 0.9100\n",
      "Epoch: 170, Loss: 0.2452, Train: 0.8604, Val: 0.8533, Test: 0.8633\n",
      "Epoch: 171, Loss: 0.2223, Train: 0.9100, Val: 0.8833, Test: 0.9033\n",
      "Epoch: 172, Loss: 0.2442, Train: 0.9271, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 173, Loss: 0.2307, Train: 0.9233, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 174, Loss: 0.1986, Train: 0.9071, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 175, Loss: 0.2274, Train: 0.9287, Val: 0.9167, Test: 0.9100\n",
      "Epoch: 176, Loss: 0.2147, Train: 0.9204, Val: 0.9067, Test: 0.8967\n",
      "Epoch: 177, Loss: 0.2429, Train: 0.8500, Val: 0.8567, Test: 0.8500\n",
      "Epoch: 178, Loss: 0.2257, Train: 0.9217, Val: 0.9067, Test: 0.8967\n",
      "Epoch: 179, Loss: 0.2237, Train: 0.8971, Val: 0.8833, Test: 0.8667\n",
      "Epoch: 180, Loss: 0.2256, Train: 0.9083, Val: 0.9100, Test: 0.9067\n",
      "Epoch: 181, Loss: 0.2183, Train: 0.9104, Val: 0.8900, Test: 0.8933\n",
      "Epoch: 182, Loss: 0.2317, Train: 0.9079, Val: 0.8733, Test: 0.8900\n",
      "Epoch: 183, Loss: 0.2376, Train: 0.8529, Val: 0.8500, Test: 0.8600\n",
      "Epoch: 184, Loss: 0.2291, Train: 0.8575, Val: 0.8667, Test: 0.8800\n",
      "Epoch: 185, Loss: 0.2123, Train: 0.8854, Val: 0.8700, Test: 0.8900\n",
      "Epoch: 186, Loss: 0.2448, Train: 0.9137, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 187, Loss: 0.2191, Train: 0.9100, Val: 0.9067, Test: 0.8967\n",
      "Epoch: 188, Loss: 0.2306, Train: 0.9308, Val: 0.9000, Test: 0.9067\n",
      "Epoch: 189, Loss: 0.2237, Train: 0.8967, Val: 0.9000, Test: 0.8833\n",
      "Epoch: 190, Loss: 0.2258, Train: 0.9308, Val: 0.9067, Test: 0.9000\n",
      "Epoch: 191, Loss: 0.2106, Train: 0.9217, Val: 0.9133, Test: 0.9133\n",
      "Epoch: 192, Loss: 0.2380, Train: 0.8358, Val: 0.8533, Test: 0.8367\n",
      "Epoch: 193, Loss: 0.2270, Train: 0.9129, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 194, Loss: 0.2164, Train: 0.9287, Val: 0.9133, Test: 0.9200\n",
      "Epoch: 195, Loss: 0.2158, Train: 0.8962, Val: 0.8933, Test: 0.9033\n",
      "Epoch: 196, Loss: 0.2272, Train: 0.9313, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 197, Loss: 0.2057, Train: 0.9275, Val: 0.9200, Test: 0.9000\n",
      "Epoch: 198, Loss: 0.2225, Train: 0.9246, Val: 0.9033, Test: 0.9033\n",
      "Epoch: 199, Loss: 0.2269, Train: 0.9279, Val: 0.9133, Test: 0.9033\n",
      "Epoch: 200, Loss: 0.2206, Train: 0.9321, Val: 0.9100, Test: 0.9200\n",
      "Epoch: 201, Loss: 0.2288, Train: 0.9296, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 202, Loss: 0.2240, Train: 0.9325, Val: 0.9100, Test: 0.9167\n",
      "Epoch: 203, Loss: 0.2497, Train: 0.9275, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 204, Loss: 0.2195, Train: 0.9275, Val: 0.9100, Test: 0.8933\n",
      "Epoch: 205, Loss: 0.2207, Train: 0.8146, Val: 0.8033, Test: 0.8267\n",
      "Epoch: 206, Loss: 0.2140, Train: 0.9146, Val: 0.9067, Test: 0.8833\n",
      "Epoch: 207, Loss: 0.2260, Train: 0.9246, Val: 0.9133, Test: 0.9000\n",
      "Epoch: 208, Loss: 0.2329, Train: 0.9254, Val: 0.9167, Test: 0.9033\n",
      "Epoch: 209, Loss: 0.2114, Train: 0.9350, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 210, Loss: 0.2311, Train: 0.9337, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 211, Loss: 0.2167, Train: 0.9313, Val: 0.9133, Test: 0.9133\n",
      "Epoch: 212, Loss: 0.2238, Train: 0.9313, Val: 0.9000, Test: 0.9100\n",
      "Epoch: 213, Loss: 0.2203, Train: 0.9175, Val: 0.9100, Test: 0.8833\n",
      "Epoch: 214, Loss: 0.2165, Train: 0.8788, Val: 0.8967, Test: 0.8800\n",
      "Epoch: 215, Loss: 0.2155, Train: 0.9375, Val: 0.9133, Test: 0.9133\n",
      "Epoch: 216, Loss: 0.2123, Train: 0.8521, Val: 0.8633, Test: 0.8767\n",
      "Epoch: 217, Loss: 0.2110, Train: 0.9392, Val: 0.9167, Test: 0.9100\n",
      "Epoch: 218, Loss: 0.2313, Train: 0.9200, Val: 0.9100, Test: 0.8967\n",
      "Epoch: 219, Loss: 0.2095, Train: 0.8842, Val: 0.8700, Test: 0.8833\n",
      "Epoch: 220, Loss: 0.2170, Train: 0.8933, Val: 0.8767, Test: 0.8833\n",
      "Epoch: 221, Loss: 0.1957, Train: 0.9342, Val: 0.9133, Test: 0.9167\n",
      "Epoch: 222, Loss: 0.2226, Train: 0.9275, Val: 0.9067, Test: 0.9267\n",
      "Epoch: 223, Loss: 0.2249, Train: 0.9387, Val: 0.9100, Test: 0.9167\n",
      "Epoch: 224, Loss: 0.2152, Train: 0.9204, Val: 0.9033, Test: 0.9033\n",
      "Epoch: 225, Loss: 0.2041, Train: 0.9283, Val: 0.9067, Test: 0.9200\n",
      "Epoch: 226, Loss: 0.2111, Train: 0.9121, Val: 0.8933, Test: 0.8867\n",
      "Epoch: 227, Loss: 0.2151, Train: 0.9242, Val: 0.9067, Test: 0.9033\n",
      "Epoch: 228, Loss: 0.2223, Train: 0.8604, Val: 0.8467, Test: 0.8633\n",
      "Epoch: 229, Loss: 0.2006, Train: 0.8671, Val: 0.8800, Test: 0.8567\n",
      "Epoch: 230, Loss: 0.2183, Train: 0.9283, Val: 0.9100, Test: 0.9167\n",
      "Epoch: 231, Loss: 0.2044, Train: 0.9213, Val: 0.9100, Test: 0.9133\n",
      "Epoch: 232, Loss: 0.2108, Train: 0.9279, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 233, Loss: 0.2061, Train: 0.8942, Val: 0.9067, Test: 0.8733\n",
      "Epoch: 234, Loss: 0.2045, Train: 0.9300, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 235, Loss: 0.2088, Train: 0.8179, Val: 0.8333, Test: 0.8133\n",
      "Epoch: 236, Loss: 0.2229, Train: 0.9087, Val: 0.9100, Test: 0.9167\n",
      "Epoch: 237, Loss: 0.2183, Train: 0.8904, Val: 0.8867, Test: 0.8700\n",
      "Epoch: 238, Loss: 0.2189, Train: 0.9242, Val: 0.8867, Test: 0.8933\n",
      "Epoch: 239, Loss: 0.2130, Train: 0.9079, Val: 0.8900, Test: 0.8867\n",
      "Epoch: 240, Loss: 0.2028, Train: 0.9225, Val: 0.9100, Test: 0.9000\n",
      "Epoch: 241, Loss: 0.1998, Train: 0.8433, Val: 0.8533, Test: 0.8667\n",
      "Epoch: 242, Loss: 0.1997, Train: 0.8654, Val: 0.8933, Test: 0.8533\n",
      "Epoch: 243, Loss: 0.2143, Train: 0.8917, Val: 0.8867, Test: 0.8600\n",
      "Epoch: 244, Loss: 0.2359, Train: 0.9371, Val: 0.9100, Test: 0.9133\n",
      "Epoch: 245, Loss: 0.2028, Train: 0.9396, Val: 0.9133, Test: 0.9200\n",
      "Epoch: 246, Loss: 0.2171, Train: 0.9375, Val: 0.9133, Test: 0.9067\n",
      "Epoch: 247, Loss: 0.2000, Train: 0.9258, Val: 0.9133, Test: 0.9000\n",
      "Epoch: 248, Loss: 0.2099, Train: 0.9329, Val: 0.9267, Test: 0.9033\n",
      "Epoch: 249, Loss: 0.2065, Train: 0.8962, Val: 0.8867, Test: 0.8900\n",
      "Epoch: 250, Loss: 0.1992, Train: 0.9308, Val: 0.9167, Test: 0.8967\n",
      "Epoch: 251, Loss: 0.2082, Train: 0.9408, Val: 0.9133, Test: 0.8967\n",
      "Epoch: 252, Loss: 0.2389, Train: 0.9400, Val: 0.9067, Test: 0.9167\n",
      "Epoch: 253, Loss: 0.2156, Train: 0.9142, Val: 0.9000, Test: 0.9167\n",
      "Epoch: 254, Loss: 0.2031, Train: 0.9333, Val: 0.9100, Test: 0.9200\n",
      "Epoch: 255, Loss: 0.1949, Train: 0.9233, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 256, Loss: 0.2078, Train: 0.8975, Val: 0.8900, Test: 0.8867\n",
      "Epoch: 257, Loss: 0.1979, Train: 0.9300, Val: 0.9100, Test: 0.9000\n",
      "Epoch: 258, Loss: 0.2025, Train: 0.9229, Val: 0.8933, Test: 0.8833\n",
      "Epoch: 259, Loss: 0.2044, Train: 0.8825, Val: 0.8933, Test: 0.8933\n",
      "Epoch: 260, Loss: 0.2188, Train: 0.9313, Val: 0.9200, Test: 0.8933\n",
      "Epoch: 261, Loss: 0.2099, Train: 0.9254, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 262, Loss: 0.1992, Train: 0.9233, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 263, Loss: 0.2002, Train: 0.8842, Val: 0.8600, Test: 0.8767\n",
      "Epoch: 264, Loss: 0.2237, Train: 0.8738, Val: 0.8733, Test: 0.8567\n",
      "Epoch: 265, Loss: 0.2072, Train: 0.9367, Val: 0.9233, Test: 0.9067\n",
      "Epoch: 266, Loss: 0.2034, Train: 0.8896, Val: 0.8833, Test: 0.8667\n",
      "Epoch: 267, Loss: 0.2140, Train: 0.9229, Val: 0.8967, Test: 0.9000\n",
      "Epoch: 268, Loss: 0.2165, Train: 0.8938, Val: 0.8967, Test: 0.8967\n",
      "Epoch: 269, Loss: 0.2012, Train: 0.9263, Val: 0.9100, Test: 0.8933\n",
      "Epoch: 270, Loss: 0.2123, Train: 0.9350, Val: 0.9133, Test: 0.9200\n",
      "Epoch: 271, Loss: 0.2040, Train: 0.9396, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 272, Loss: 0.1923, Train: 0.9263, Val: 0.9100, Test: 0.8967\n",
      "Epoch: 273, Loss: 0.2117, Train: 0.9087, Val: 0.8967, Test: 0.8867\n",
      "Epoch: 274, Loss: 0.1994, Train: 0.9263, Val: 0.9000, Test: 0.9033\n",
      "Epoch: 275, Loss: 0.1957, Train: 0.8896, Val: 0.8900, Test: 0.8733\n",
      "Epoch: 276, Loss: 0.1897, Train: 0.8725, Val: 0.8733, Test: 0.8667\n",
      "Epoch: 277, Loss: 0.1965, Train: 0.9371, Val: 0.9267, Test: 0.9133\n",
      "Epoch: 278, Loss: 0.2143, Train: 0.9017, Val: 0.9033, Test: 0.8800\n",
      "Epoch: 279, Loss: 0.2122, Train: 0.9308, Val: 0.9133, Test: 0.9067\n",
      "Epoch: 280, Loss: 0.2014, Train: 0.8596, Val: 0.8733, Test: 0.8467\n",
      "Epoch: 281, Loss: 0.2013, Train: 0.9254, Val: 0.9100, Test: 0.8933\n",
      "Epoch: 282, Loss: 0.1941, Train: 0.9042, Val: 0.9133, Test: 0.8600\n",
      "Epoch: 283, Loss: 0.2162, Train: 0.9375, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 284, Loss: 0.1969, Train: 0.9375, Val: 0.9033, Test: 0.9167\n",
      "Epoch: 285, Loss: 0.2018, Train: 0.9229, Val: 0.9033, Test: 0.9133\n",
      "Epoch: 286, Loss: 0.2033, Train: 0.9300, Val: 0.9100, Test: 0.9133\n",
      "Epoch: 287, Loss: 0.1990, Train: 0.9246, Val: 0.9100, Test: 0.9133\n",
      "Epoch: 288, Loss: 0.1990, Train: 0.9396, Val: 0.9267, Test: 0.9100\n",
      "Epoch: 289, Loss: 0.2109, Train: 0.9408, Val: 0.9167, Test: 0.9100\n",
      "Epoch: 290, Loss: 0.2020, Train: 0.9021, Val: 0.8967, Test: 0.8833\n",
      "Epoch: 291, Loss: 0.1975, Train: 0.9387, Val: 0.9167, Test: 0.9167\n",
      "Epoch: 292, Loss: 0.1920, Train: 0.9337, Val: 0.8933, Test: 0.9067\n",
      "Epoch: 293, Loss: 0.1868, Train: 0.8821, Val: 0.8733, Test: 0.8700\n",
      "Epoch: 294, Loss: 0.1927, Train: 0.9237, Val: 0.9067, Test: 0.9067\n",
      "Epoch: 295, Loss: 0.2018, Train: 0.9283, Val: 0.9100, Test: 0.8967\n",
      "Epoch: 296, Loss: 0.1985, Train: 0.9004, Val: 0.8900, Test: 0.9067\n",
      "Epoch: 297, Loss: 0.2060, Train: 0.9408, Val: 0.9267, Test: 0.9067\n",
      "Epoch: 298, Loss: 0.1979, Train: 0.9292, Val: 0.9033, Test: 0.9033\n",
      "Epoch: 299, Loss: 0.1947, Train: 0.9313, Val: 0.9133, Test: 0.8967\n",
      "Epoch: 300, Loss: 0.1934, Train: 0.9221, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 301, Loss: 0.1996, Train: 0.8896, Val: 0.8867, Test: 0.8633\n",
      "Epoch: 302, Loss: 0.2446, Train: 0.9329, Val: 0.9167, Test: 0.9100\n",
      "Epoch: 303, Loss: 0.1920, Train: 0.9437, Val: 0.9100, Test: 0.9167\n",
      "Epoch: 304, Loss: 0.2035, Train: 0.9004, Val: 0.8967, Test: 0.8700\n",
      "Epoch: 305, Loss: 0.1838, Train: 0.9417, Val: 0.9133, Test: 0.9133\n",
      "Epoch: 306, Loss: 0.1825, Train: 0.9433, Val: 0.9133, Test: 0.9067\n",
      "Epoch: 307, Loss: 0.1916, Train: 0.9387, Val: 0.9100, Test: 0.9100\n",
      "Epoch: 308, Loss: 0.1821, Train: 0.9233, Val: 0.9033, Test: 0.8933\n",
      "Epoch: 309, Loss: 0.1878, Train: 0.9396, Val: 0.9133, Test: 0.9033\n",
      "Epoch: 310, Loss: 0.1880, Train: 0.9296, Val: 0.9033, Test: 0.9100\n",
      "Epoch: 311, Loss: 0.2118, Train: 0.9417, Val: 0.9167, Test: 0.9167\n",
      "Epoch: 312, Loss: 0.1872, Train: 0.9204, Val: 0.9067, Test: 0.8867\n",
      "Epoch: 313, Loss: 0.1979, Train: 0.9087, Val: 0.9133, Test: 0.8767\n",
      "Epoch: 314, Loss: 0.2202, Train: 0.9367, Val: 0.9233, Test: 0.9167\n",
      "Epoch: 315, Loss: 0.1931, Train: 0.9429, Val: 0.9233, Test: 0.9167\n",
      "Epoch: 316, Loss: 0.1840, Train: 0.9179, Val: 0.9033, Test: 0.8900\n",
      "Epoch: 317, Loss: 0.1894, Train: 0.9429, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 318, Loss: 0.2231, Train: 0.9142, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 319, Loss: 0.2104, Train: 0.9279, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 320, Loss: 0.1996, Train: 0.9421, Val: 0.9067, Test: 0.9100\n",
      "Epoch: 321, Loss: 0.2048, Train: 0.9387, Val: 0.8933, Test: 0.9267\n",
      "Epoch: 322, Loss: 0.1848, Train: 0.9437, Val: 0.9333, Test: 0.9167\n",
      "Epoch: 323, Loss: 0.1919, Train: 0.9454, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 324, Loss: 0.1956, Train: 0.9142, Val: 0.9000, Test: 0.9133\n",
      "Epoch: 325, Loss: 0.1806, Train: 0.9296, Val: 0.9033, Test: 0.9067\n",
      "Epoch: 326, Loss: 0.1825, Train: 0.9308, Val: 0.8967, Test: 0.9100\n",
      "Epoch: 327, Loss: 0.1861, Train: 0.9467, Val: 0.9167, Test: 0.9167\n",
      "Epoch: 328, Loss: 0.1945, Train: 0.9371, Val: 0.9067, Test: 0.9100\n",
      "Epoch: 329, Loss: 0.1846, Train: 0.9237, Val: 0.9167, Test: 0.8900\n",
      "Epoch: 330, Loss: 0.2419, Train: 0.9163, Val: 0.8867, Test: 0.8933\n",
      "Epoch: 331, Loss: 0.1844, Train: 0.9321, Val: 0.9200, Test: 0.8967\n",
      "Epoch: 332, Loss: 0.1836, Train: 0.9396, Val: 0.9133, Test: 0.9333\n",
      "Epoch: 333, Loss: 0.1779, Train: 0.9021, Val: 0.8967, Test: 0.8700\n",
      "Epoch: 334, Loss: 0.1829, Train: 0.8450, Val: 0.8600, Test: 0.8433\n",
      "Epoch: 335, Loss: 0.1943, Train: 0.9429, Val: 0.9200, Test: 0.9267\n",
      "Epoch: 336, Loss: 0.1896, Train: 0.9229, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 337, Loss: 0.1877, Train: 0.9296, Val: 0.9000, Test: 0.9100\n",
      "Epoch: 338, Loss: 0.1742, Train: 0.9363, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 339, Loss: 0.1931, Train: 0.9129, Val: 0.9000, Test: 0.8933\n",
      "Epoch: 340, Loss: 0.1837, Train: 0.9171, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 341, Loss: 0.2065, Train: 0.9137, Val: 0.9033, Test: 0.8900\n",
      "Epoch: 342, Loss: 0.1770, Train: 0.9400, Val: 0.9133, Test: 0.9033\n",
      "Epoch: 343, Loss: 0.1788, Train: 0.9417, Val: 0.9133, Test: 0.9167\n",
      "Epoch: 344, Loss: 0.1851, Train: 0.8996, Val: 0.8833, Test: 0.8867\n",
      "Epoch: 345, Loss: 0.1983, Train: 0.9225, Val: 0.9133, Test: 0.8933\n",
      "Epoch: 346, Loss: 0.1862, Train: 0.9133, Val: 0.8900, Test: 0.8900\n",
      "Epoch: 347, Loss: 0.1766, Train: 0.9021, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 348, Loss: 0.1924, Train: 0.9279, Val: 0.9100, Test: 0.9100\n",
      "Epoch: 349, Loss: 0.1826, Train: 0.8525, Val: 0.8467, Test: 0.8467\n",
      "Epoch: 350, Loss: 0.1632, Train: 0.8992, Val: 0.8733, Test: 0.8600\n",
      "Epoch: 351, Loss: 0.1918, Train: 0.9113, Val: 0.9000, Test: 0.9000\n",
      "Epoch: 352, Loss: 0.1757, Train: 0.8962, Val: 0.8833, Test: 0.8733\n",
      "Epoch: 353, Loss: 0.1752, Train: 0.9417, Val: 0.9167, Test: 0.9100\n",
      "Epoch: 354, Loss: 0.1813, Train: 0.9421, Val: 0.9300, Test: 0.9133\n",
      "Epoch: 355, Loss: 0.1759, Train: 0.9467, Val: 0.9300, Test: 0.9133\n",
      "Epoch: 356, Loss: 0.1884, Train: 0.9104, Val: 0.9033, Test: 0.8767\n",
      "Epoch: 357, Loss: 0.1863, Train: 0.9437, Val: 0.9133, Test: 0.9200\n",
      "Epoch: 358, Loss: 0.1772, Train: 0.9513, Val: 0.9300, Test: 0.9233\n",
      "Epoch: 359, Loss: 0.1799, Train: 0.9275, Val: 0.8933, Test: 0.8967\n",
      "Epoch: 360, Loss: 0.1790, Train: 0.9463, Val: 0.9267, Test: 0.9100\n",
      "Epoch: 361, Loss: 0.1860, Train: 0.9400, Val: 0.9267, Test: 0.9167\n",
      "Epoch: 362, Loss: 0.1979, Train: 0.9450, Val: 0.9367, Test: 0.9133\n",
      "Epoch: 363, Loss: 0.1776, Train: 0.9308, Val: 0.8900, Test: 0.9000\n",
      "Epoch: 364, Loss: 0.1887, Train: 0.9463, Val: 0.9100, Test: 0.9033\n",
      "Epoch: 365, Loss: 0.1809, Train: 0.8962, Val: 0.8800, Test: 0.8667\n",
      "Epoch: 366, Loss: 0.1804, Train: 0.9450, Val: 0.9133, Test: 0.9233\n",
      "Epoch: 367, Loss: 0.1717, Train: 0.9371, Val: 0.9133, Test: 0.9000\n",
      "Epoch: 368, Loss: 0.1908, Train: 0.9183, Val: 0.9100, Test: 0.9033\n",
      "Epoch: 369, Loss: 0.2002, Train: 0.9237, Val: 0.8967, Test: 0.9000\n",
      "Epoch: 370, Loss: 0.1787, Train: 0.9121, Val: 0.9000, Test: 0.8867\n",
      "Epoch: 371, Loss: 0.1887, Train: 0.9425, Val: 0.9200, Test: 0.9167\n",
      "Epoch: 372, Loss: 0.1717, Train: 0.9287, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 373, Loss: 0.1674, Train: 0.9392, Val: 0.9300, Test: 0.9067\n",
      "Epoch: 374, Loss: 0.1646, Train: 0.8862, Val: 0.8800, Test: 0.8867\n",
      "Epoch: 375, Loss: 0.1796, Train: 0.8562, Val: 0.8600, Test: 0.8400\n",
      "Epoch: 376, Loss: 0.1707, Train: 0.8825, Val: 0.8867, Test: 0.8567\n",
      "Epoch: 377, Loss: 0.1688, Train: 0.9492, Val: 0.9267, Test: 0.9167\n",
      "Epoch: 378, Loss: 0.1754, Train: 0.9304, Val: 0.9100, Test: 0.9000\n",
      "Epoch: 379, Loss: 0.1760, Train: 0.9454, Val: 0.9167, Test: 0.9033\n",
      "Epoch: 380, Loss: 0.1940, Train: 0.9217, Val: 0.9067, Test: 0.9000\n",
      "Epoch: 381, Loss: 0.1702, Train: 0.9196, Val: 0.9000, Test: 0.8967\n",
      "Epoch: 382, Loss: 0.1757, Train: 0.9513, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 383, Loss: 0.1887, Train: 0.8775, Val: 0.8600, Test: 0.8600\n",
      "Epoch: 384, Loss: 0.1796, Train: 0.9337, Val: 0.9100, Test: 0.9100\n",
      "Epoch: 385, Loss: 0.1750, Train: 0.9463, Val: 0.9167, Test: 0.9167\n",
      "Epoch: 386, Loss: 0.2176, Train: 0.8817, Val: 0.8733, Test: 0.8567\n",
      "Epoch: 387, Loss: 0.1719, Train: 0.9487, Val: 0.9233, Test: 0.9167\n",
      "Epoch: 388, Loss: 0.1722, Train: 0.9487, Val: 0.9133, Test: 0.9167\n",
      "Epoch: 389, Loss: 0.1780, Train: 0.9542, Val: 0.9200, Test: 0.9067\n",
      "Epoch: 390, Loss: 0.1562, Train: 0.9329, Val: 0.9333, Test: 0.8933\n",
      "Epoch: 391, Loss: 0.1732, Train: 0.9454, Val: 0.9300, Test: 0.9000\n",
      "Epoch: 392, Loss: 0.1724, Train: 0.9021, Val: 0.8800, Test: 0.8733\n",
      "Epoch: 393, Loss: 0.1758, Train: 0.9413, Val: 0.9200, Test: 0.9333\n",
      "Epoch: 394, Loss: 0.1706, Train: 0.9425, Val: 0.9133, Test: 0.9133\n",
      "Epoch: 395, Loss: 0.1681, Train: 0.9504, Val: 0.9167, Test: 0.9067\n",
      "Epoch: 396, Loss: 0.1602, Train: 0.9317, Val: 0.9000, Test: 0.9067\n",
      "Epoch: 397, Loss: 0.1723, Train: 0.9204, Val: 0.9000, Test: 0.8967\n",
      "Epoch: 398, Loss: 0.1804, Train: 0.9300, Val: 0.9067, Test: 0.8933\n",
      "Epoch: 399, Loss: 0.1938, Train: 0.9471, Val: 0.9067, Test: 0.9200\n",
      "Epoch: 400, Loss: 0.1818, Train: 0.9396, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 401, Loss: 0.1745, Train: 0.9446, Val: 0.8967, Test: 0.9000\n",
      "Epoch: 402, Loss: 0.1594, Train: 0.9533, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 403, Loss: 0.1872, Train: 0.9467, Val: 0.9167, Test: 0.9067\n",
      "Epoch: 404, Loss: 0.1726, Train: 0.9537, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 405, Loss: 0.1591, Train: 0.9142, Val: 0.9033, Test: 0.8867\n",
      "Epoch: 406, Loss: 0.1706, Train: 0.9387, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 407, Loss: 0.1594, Train: 0.9421, Val: 0.9033, Test: 0.8933\n",
      "Epoch: 408, Loss: 0.1666, Train: 0.9275, Val: 0.9133, Test: 0.8900\n",
      "Epoch: 409, Loss: 0.1591, Train: 0.9229, Val: 0.8967, Test: 0.9033\n",
      "Epoch: 410, Loss: 0.1653, Train: 0.9146, Val: 0.8867, Test: 0.8833\n",
      "Epoch: 411, Loss: 0.1831, Train: 0.9563, Val: 0.9333, Test: 0.9167\n",
      "Epoch: 412, Loss: 0.1760, Train: 0.9367, Val: 0.9200, Test: 0.8967\n",
      "Epoch: 413, Loss: 0.1632, Train: 0.9496, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 414, Loss: 0.1624, Train: 0.9500, Val: 0.9067, Test: 0.9200\n",
      "Epoch: 415, Loss: 0.1624, Train: 0.9062, Val: 0.8933, Test: 0.8833\n",
      "Epoch: 416, Loss: 0.1716, Train: 0.9450, Val: 0.9200, Test: 0.9067\n",
      "Epoch: 417, Loss: 0.1657, Train: 0.9425, Val: 0.9400, Test: 0.8900\n",
      "Epoch: 418, Loss: 0.1724, Train: 0.9354, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 419, Loss: 0.1653, Train: 0.9475, Val: 0.9133, Test: 0.9167\n",
      "Epoch: 420, Loss: 0.1605, Train: 0.9454, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 421, Loss: 0.1510, Train: 0.9587, Val: 0.9233, Test: 0.9200\n",
      "Epoch: 422, Loss: 0.1618, Train: 0.9471, Val: 0.9233, Test: 0.9133\n",
      "Epoch: 423, Loss: 0.1648, Train: 0.9546, Val: 0.9267, Test: 0.9067\n",
      "Epoch: 424, Loss: 0.1928, Train: 0.9067, Val: 0.9000, Test: 0.8733\n",
      "Epoch: 425, Loss: 0.1757, Train: 0.9529, Val: 0.9100, Test: 0.9200\n",
      "Epoch: 426, Loss: 0.1761, Train: 0.9475, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 427, Loss: 0.1762, Train: 0.9333, Val: 0.9067, Test: 0.9033\n",
      "Epoch: 428, Loss: 0.1775, Train: 0.9521, Val: 0.9300, Test: 0.9067\n",
      "Epoch: 429, Loss: 0.1632, Train: 0.9496, Val: 0.9333, Test: 0.9067\n",
      "Epoch: 430, Loss: 0.1835, Train: 0.9554, Val: 0.9333, Test: 0.9100\n",
      "Epoch: 431, Loss: 0.1549, Train: 0.9463, Val: 0.9067, Test: 0.9133\n",
      "Epoch: 432, Loss: 0.1553, Train: 0.9446, Val: 0.9133, Test: 0.9000\n",
      "Epoch: 433, Loss: 0.1665, Train: 0.9296, Val: 0.9333, Test: 0.8967\n",
      "Epoch: 434, Loss: 0.1582, Train: 0.9413, Val: 0.9200, Test: 0.9067\n",
      "Epoch: 435, Loss: 0.1650, Train: 0.9513, Val: 0.9300, Test: 0.9167\n",
      "Epoch: 436, Loss: 0.1716, Train: 0.9542, Val: 0.9233, Test: 0.9067\n",
      "Epoch: 437, Loss: 0.1619, Train: 0.9396, Val: 0.8967, Test: 0.9033\n",
      "Epoch: 438, Loss: 0.1994, Train: 0.9583, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 439, Loss: 0.1561, Train: 0.9500, Val: 0.9167, Test: 0.9267\n",
      "Epoch: 440, Loss: 0.1736, Train: 0.9021, Val: 0.8900, Test: 0.8833\n",
      "Epoch: 441, Loss: 0.1639, Train: 0.9500, Val: 0.9233, Test: 0.9200\n",
      "Epoch: 442, Loss: 0.1513, Train: 0.9483, Val: 0.9067, Test: 0.9067\n",
      "Epoch: 443, Loss: 0.1728, Train: 0.9542, Val: 0.9300, Test: 0.8900\n",
      "Epoch: 444, Loss: 0.1541, Train: 0.9213, Val: 0.9233, Test: 0.8967\n",
      "Epoch: 445, Loss: 0.1596, Train: 0.9496, Val: 0.9233, Test: 0.9267\n",
      "Epoch: 446, Loss: 0.1601, Train: 0.9533, Val: 0.9300, Test: 0.9100\n",
      "Epoch: 447, Loss: 0.1805, Train: 0.9275, Val: 0.9033, Test: 0.8967\n",
      "Epoch: 448, Loss: 0.1633, Train: 0.9546, Val: 0.9300, Test: 0.9100\n",
      "Epoch: 449, Loss: 0.1624, Train: 0.9542, Val: 0.9267, Test: 0.8967\n",
      "Epoch: 450, Loss: 0.1830, Train: 0.9529, Val: 0.9067, Test: 0.9033\n",
      "Epoch: 451, Loss: 0.1400, Train: 0.9396, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 452, Loss: 0.1556, Train: 0.9567, Val: 0.9200, Test: 0.9067\n",
      "Epoch: 453, Loss: 0.1522, Train: 0.9433, Val: 0.9067, Test: 0.9167\n",
      "Epoch: 454, Loss: 0.1625, Train: 0.9525, Val: 0.9233, Test: 0.9000\n",
      "Epoch: 455, Loss: 0.1744, Train: 0.9546, Val: 0.9200, Test: 0.9233\n",
      "Epoch: 456, Loss: 0.1701, Train: 0.9587, Val: 0.9267, Test: 0.9167\n",
      "Epoch: 457, Loss: 0.1782, Train: 0.8929, Val: 0.8767, Test: 0.8667\n",
      "Epoch: 458, Loss: 0.1726, Train: 0.9558, Val: 0.9333, Test: 0.9067\n",
      "Epoch: 459, Loss: 0.1533, Train: 0.9508, Val: 0.9200, Test: 0.8967\n",
      "Epoch: 460, Loss: 0.1489, Train: 0.9608, Val: 0.9300, Test: 0.9133\n",
      "Epoch: 461, Loss: 0.1610, Train: 0.9537, Val: 0.9233, Test: 0.9133\n",
      "Epoch: 462, Loss: 0.1563, Train: 0.9592, Val: 0.9367, Test: 0.9167\n",
      "Epoch: 463, Loss: 0.1614, Train: 0.9371, Val: 0.9233, Test: 0.9067\n",
      "Epoch: 464, Loss: 0.1850, Train: 0.9254, Val: 0.9133, Test: 0.8833\n",
      "Epoch: 465, Loss: 0.1498, Train: 0.9383, Val: 0.9100, Test: 0.9100\n",
      "Epoch: 466, Loss: 0.1951, Train: 0.9200, Val: 0.9033, Test: 0.9000\n",
      "Epoch: 467, Loss: 0.1530, Train: 0.9621, Val: 0.9300, Test: 0.9000\n",
      "Epoch: 468, Loss: 0.1703, Train: 0.9537, Val: 0.9200, Test: 0.9100\n",
      "Epoch: 469, Loss: 0.1540, Train: 0.9558, Val: 0.9233, Test: 0.9100\n",
      "Epoch: 470, Loss: 0.1832, Train: 0.9371, Val: 0.9033, Test: 0.9067\n",
      "Epoch: 471, Loss: 0.1546, Train: 0.9546, Val: 0.9167, Test: 0.9133\n",
      "Epoch: 472, Loss: 0.1561, Train: 0.9421, Val: 0.9300, Test: 0.9000\n",
      "Epoch: 473, Loss: 0.1502, Train: 0.9571, Val: 0.9267, Test: 0.9167\n",
      "Epoch: 474, Loss: 0.1591, Train: 0.9571, Val: 0.9300, Test: 0.9133\n",
      "Epoch: 475, Loss: 0.1518, Train: 0.9363, Val: 0.9133, Test: 0.9033\n",
      "Epoch: 476, Loss: 0.1474, Train: 0.9546, Val: 0.9233, Test: 0.9167\n",
      "Epoch: 477, Loss: 0.1754, Train: 0.9083, Val: 0.8967, Test: 0.8767\n",
      "Epoch: 478, Loss: 0.1488, Train: 0.9025, Val: 0.8967, Test: 0.8767\n",
      "Epoch: 479, Loss: 0.1415, Train: 0.9558, Val: 0.9233, Test: 0.9200\n",
      "Epoch: 480, Loss: 0.1446, Train: 0.9404, Val: 0.9100, Test: 0.9000\n",
      "Epoch: 481, Loss: 0.1381, Train: 0.9550, Val: 0.9300, Test: 0.9000\n",
      "Epoch: 482, Loss: 0.1546, Train: 0.9579, Val: 0.9200, Test: 0.9100\n",
      "Epoch: 483, Loss: 0.1586, Train: 0.9542, Val: 0.9300, Test: 0.9067\n",
      "Epoch: 484, Loss: 0.1400, Train: 0.9592, Val: 0.9267, Test: 0.9033\n",
      "Epoch: 485, Loss: 0.1604, Train: 0.9354, Val: 0.9167, Test: 0.8833\n",
      "Epoch: 486, Loss: 0.1639, Train: 0.9496, Val: 0.9167, Test: 0.9100\n",
      "Epoch: 487, Loss: 0.1628, Train: 0.9413, Val: 0.9233, Test: 0.9067\n",
      "Epoch: 488, Loss: 0.1508, Train: 0.9400, Val: 0.9100, Test: 0.8933\n",
      "Epoch: 489, Loss: 0.1586, Train: 0.9487, Val: 0.9167, Test: 0.9233\n",
      "Epoch: 490, Loss: 0.1707, Train: 0.9450, Val: 0.9133, Test: 0.9100\n",
      "Epoch: 491, Loss: 0.1620, Train: 0.9492, Val: 0.9033, Test: 0.8933\n",
      "Epoch: 492, Loss: 0.1449, Train: 0.9537, Val: 0.9233, Test: 0.8933\n",
      "Epoch: 493, Loss: 0.1651, Train: 0.9567, Val: 0.9233, Test: 0.9100\n",
      "Epoch: 494, Loss: 0.1613, Train: 0.9579, Val: 0.9300, Test: 0.8933\n",
      "Epoch: 495, Loss: 0.1548, Train: 0.9521, Val: 0.9000, Test: 0.8967\n",
      "Epoch: 496, Loss: 0.1422, Train: 0.9563, Val: 0.9333, Test: 0.8967\n",
      "Epoch: 497, Loss: 0.1479, Train: 0.9558, Val: 0.9200, Test: 0.9133\n",
      "Epoch: 498, Loss: 0.1439, Train: 0.9200, Val: 0.9067, Test: 0.8900\n",
      "Epoch: 499, Loss: 0.1564, Train: 0.9563, Val: 0.9233, Test: 0.9133\n",
      "Epoch: 500, Loss: 0.1657, Train: 0.9513, Val: 0.9233, Test: 0.9133\n",
      "### Run 0 - val loss: 0.162, test acc: 0.900\n",
      "Accuracies in each run:  [0.9]\n",
      "test acc - mean: 0.900, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'dense-random'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp-graclus\n",
      "Epoch: 001, Loss: 0.7728, Train: 0.5038, Val: 0.5367, Test: 0.5300\n",
      "Epoch: 002, Loss: 0.6387, Train: 0.7362, Val: 0.7267, Test: 0.7100\n",
      "Epoch: 003, Loss: 0.5851, Train: 0.6558, Val: 0.6367, Test: 0.6567\n",
      "Epoch: 004, Loss: 0.5485, Train: 0.7179, Val: 0.7133, Test: 0.7000\n",
      "Epoch: 005, Loss: 0.5180, Train: 0.7342, Val: 0.7033, Test: 0.7200\n",
      "Epoch: 006, Loss: 0.4851, Train: 0.6333, Val: 0.6067, Test: 0.6267\n",
      "Epoch: 007, Loss: 0.4877, Train: 0.8100, Val: 0.7833, Test: 0.7767\n",
      "Epoch: 008, Loss: 0.4687, Train: 0.6929, Val: 0.7067, Test: 0.6900\n",
      "Epoch: 009, Loss: 0.4487, Train: 0.6275, Val: 0.6133, Test: 0.6133\n",
      "Epoch: 010, Loss: 0.4358, Train: 0.7771, Val: 0.7633, Test: 0.7400\n",
      "Epoch: 011, Loss: 0.4503, Train: 0.8004, Val: 0.8333, Test: 0.7733\n",
      "Epoch: 012, Loss: 0.4399, Train: 0.6583, Val: 0.6467, Test: 0.6300\n",
      "Epoch: 013, Loss: 0.4200, Train: 0.8346, Val: 0.8533, Test: 0.8233\n",
      "Epoch: 014, Loss: 0.4300, Train: 0.6871, Val: 0.7200, Test: 0.7033\n",
      "Epoch: 015, Loss: 0.4133, Train: 0.8538, Val: 0.8467, Test: 0.8300\n",
      "Epoch: 016, Loss: 0.4045, Train: 0.6017, Val: 0.5733, Test: 0.5733\n",
      "Epoch: 017, Loss: 0.3965, Train: 0.8337, Val: 0.8067, Test: 0.7800\n",
      "Epoch: 018, Loss: 0.4069, Train: 0.7971, Val: 0.8267, Test: 0.7900\n",
      "Epoch: 019, Loss: 0.3974, Train: 0.8175, Val: 0.8100, Test: 0.7700\n",
      "Epoch: 020, Loss: 0.3807, Train: 0.7300, Val: 0.7100, Test: 0.7067\n",
      "Epoch: 021, Loss: 0.3733, Train: 0.7987, Val: 0.8500, Test: 0.7767\n",
      "Epoch: 022, Loss: 0.4088, Train: 0.7688, Val: 0.7567, Test: 0.7500\n",
      "Epoch: 023, Loss: 0.3892, Train: 0.8225, Val: 0.8700, Test: 0.8300\n",
      "Epoch: 024, Loss: 0.3621, Train: 0.7929, Val: 0.7633, Test: 0.7733\n",
      "Epoch: 025, Loss: 0.3780, Train: 0.7446, Val: 0.7133, Test: 0.7200\n",
      "Epoch: 026, Loss: 0.3751, Train: 0.4988, Val: 0.5267, Test: 0.5167\n",
      "Epoch: 027, Loss: 0.3494, Train: 0.8037, Val: 0.8200, Test: 0.7967\n",
      "Epoch: 028, Loss: 0.3676, Train: 0.7896, Val: 0.8200, Test: 0.7800\n",
      "Epoch: 029, Loss: 0.3647, Train: 0.5217, Val: 0.5467, Test: 0.5300\n",
      "Epoch: 030, Loss: 0.3322, Train: 0.6438, Val: 0.6233, Test: 0.6067\n",
      "Epoch: 031, Loss: 0.3516, Train: 0.8133, Val: 0.7767, Test: 0.7867\n",
      "Epoch: 032, Loss: 0.3485, Train: 0.6104, Val: 0.5767, Test: 0.5667\n",
      "Epoch: 033, Loss: 0.3350, Train: 0.5275, Val: 0.5600, Test: 0.5433\n",
      "Epoch: 034, Loss: 0.3226, Train: 0.6288, Val: 0.6633, Test: 0.6467\n",
      "Epoch: 035, Loss: 0.3487, Train: 0.6950, Val: 0.6800, Test: 0.6700\n",
      "Epoch: 036, Loss: 0.3309, Train: 0.6958, Val: 0.7400, Test: 0.7167\n",
      "Epoch: 037, Loss: 0.3326, Train: 0.6913, Val: 0.7333, Test: 0.6933\n",
      "Epoch: 038, Loss: 0.3175, Train: 0.6558, Val: 0.6967, Test: 0.6833\n",
      "Epoch: 039, Loss: 0.3383, Train: 0.5646, Val: 0.5900, Test: 0.5867\n",
      "Epoch: 040, Loss: 0.3363, Train: 0.7883, Val: 0.7700, Test: 0.7633\n",
      "Epoch: 041, Loss: 0.3448, Train: 0.5050, Val: 0.5467, Test: 0.5300\n",
      "Epoch: 042, Loss: 0.3199, Train: 0.6621, Val: 0.6400, Test: 0.6300\n",
      "Epoch: 043, Loss: 0.3308, Train: 0.7050, Val: 0.6700, Test: 0.6800\n",
      "Epoch: 044, Loss: 0.3371, Train: 0.8317, Val: 0.7967, Test: 0.7967\n",
      "Epoch: 045, Loss: 0.3466, Train: 0.8313, Val: 0.8033, Test: 0.8000\n",
      "Epoch: 046, Loss: 0.3244, Train: 0.8729, Val: 0.8667, Test: 0.8400\n",
      "Epoch: 047, Loss: 0.3213, Train: 0.6083, Val: 0.6633, Test: 0.6167\n",
      "Epoch: 048, Loss: 0.3239, Train: 0.8192, Val: 0.8367, Test: 0.7967\n",
      "Epoch: 049, Loss: 0.3090, Train: 0.7046, Val: 0.6800, Test: 0.6733\n",
      "Epoch: 050, Loss: 0.3148, Train: 0.8917, Val: 0.8967, Test: 0.8767\n",
      "Epoch: 051, Loss: 0.3132, Train: 0.8667, Val: 0.8433, Test: 0.8400\n",
      "Epoch: 052, Loss: 0.3399, Train: 0.8862, Val: 0.8600, Test: 0.8700\n",
      "Epoch: 053, Loss: 0.3202, Train: 0.8471, Val: 0.8567, Test: 0.8300\n",
      "Epoch: 054, Loss: 0.3123, Train: 0.6717, Val: 0.6533, Test: 0.6233\n",
      "Epoch: 055, Loss: 0.3411, Train: 0.8800, Val: 0.8600, Test: 0.8467\n",
      "Epoch: 056, Loss: 0.3190, Train: 0.8150, Val: 0.7733, Test: 0.7900\n",
      "Epoch: 057, Loss: 0.3237, Train: 0.7429, Val: 0.7200, Test: 0.7100\n",
      "Epoch: 058, Loss: 0.3021, Train: 0.6379, Val: 0.6100, Test: 0.6067\n",
      "Epoch: 059, Loss: 0.3326, Train: 0.8600, Val: 0.8200, Test: 0.8167\n",
      "Epoch: 060, Loss: 0.3159, Train: 0.7367, Val: 0.7800, Test: 0.7767\n",
      "Epoch: 061, Loss: 0.3034, Train: 0.6858, Val: 0.7133, Test: 0.6833\n",
      "Epoch: 062, Loss: 0.2975, Train: 0.8846, Val: 0.8933, Test: 0.8700\n",
      "Epoch: 063, Loss: 0.3190, Train: 0.8050, Val: 0.8033, Test: 0.7833\n",
      "Epoch: 064, Loss: 0.3192, Train: 0.7650, Val: 0.7867, Test: 0.7733\n",
      "Epoch: 065, Loss: 0.2977, Train: 0.7588, Val: 0.7733, Test: 0.7400\n",
      "Epoch: 066, Loss: 0.3123, Train: 0.7746, Val: 0.7467, Test: 0.7500\n",
      "Epoch: 067, Loss: 0.3247, Train: 0.7904, Val: 0.7800, Test: 0.7467\n",
      "Epoch: 068, Loss: 0.3191, Train: 0.8812, Val: 0.8800, Test: 0.8633\n",
      "Epoch: 069, Loss: 0.2892, Train: 0.8363, Val: 0.8600, Test: 0.8300\n",
      "Epoch: 070, Loss: 0.3084, Train: 0.8117, Val: 0.7933, Test: 0.7833\n",
      "Epoch: 071, Loss: 0.3154, Train: 0.7667, Val: 0.7367, Test: 0.7367\n",
      "Epoch: 072, Loss: 0.2920, Train: 0.8208, Val: 0.7933, Test: 0.7900\n",
      "Epoch: 073, Loss: 0.2960, Train: 0.8471, Val: 0.8400, Test: 0.8367\n",
      "Epoch: 074, Loss: 0.2934, Train: 0.8821, Val: 0.8933, Test: 0.8600\n",
      "Epoch: 075, Loss: 0.2878, Train: 0.7738, Val: 0.7533, Test: 0.7433\n",
      "Epoch: 076, Loss: 0.2871, Train: 0.8738, Val: 0.8967, Test: 0.8600\n",
      "Epoch: 077, Loss: 0.3062, Train: 0.8771, Val: 0.8367, Test: 0.8400\n",
      "Epoch: 078, Loss: 0.2987, Train: 0.8392, Val: 0.8367, Test: 0.8100\n",
      "Epoch: 079, Loss: 0.3100, Train: 0.6317, Val: 0.6700, Test: 0.6200\n",
      "Epoch: 080, Loss: 0.3036, Train: 0.8108, Val: 0.7800, Test: 0.7567\n",
      "Epoch: 081, Loss: 0.2876, Train: 0.7492, Val: 0.7900, Test: 0.7633\n",
      "Epoch: 082, Loss: 0.3343, Train: 0.9038, Val: 0.8967, Test: 0.8600\n",
      "Epoch: 083, Loss: 0.2774, Train: 0.8858, Val: 0.8833, Test: 0.8467\n",
      "Epoch: 084, Loss: 0.2772, Train: 0.9017, Val: 0.9100, Test: 0.8600\n",
      "Epoch: 085, Loss: 0.2854, Train: 0.8683, Val: 0.8733, Test: 0.8767\n",
      "Epoch: 086, Loss: 0.3016, Train: 0.8925, Val: 0.8900, Test: 0.8567\n",
      "Epoch: 087, Loss: 0.2703, Train: 0.8950, Val: 0.8700, Test: 0.8567\n",
      "Epoch: 088, Loss: 0.2975, Train: 0.8912, Val: 0.9067, Test: 0.8733\n",
      "Epoch: 089, Loss: 0.2923, Train: 0.6808, Val: 0.7100, Test: 0.6633\n",
      "Epoch: 090, Loss: 0.2810, Train: 0.8929, Val: 0.8933, Test: 0.8933\n",
      "Epoch: 091, Loss: 0.2853, Train: 0.7688, Val: 0.7533, Test: 0.7333\n",
      "Epoch: 092, Loss: 0.2838, Train: 0.6708, Val: 0.6900, Test: 0.6500\n",
      "Epoch: 093, Loss: 0.2805, Train: 0.8942, Val: 0.8967, Test: 0.8633\n",
      "Epoch: 094, Loss: 0.2841, Train: 0.8467, Val: 0.8233, Test: 0.8233\n",
      "Epoch: 095, Loss: 0.2924, Train: 0.8850, Val: 0.8700, Test: 0.8567\n",
      "Epoch: 096, Loss: 0.3047, Train: 0.8475, Val: 0.8167, Test: 0.8000\n",
      "Epoch: 097, Loss: 0.2842, Train: 0.8979, Val: 0.9000, Test: 0.8733\n",
      "Epoch: 098, Loss: 0.2913, Train: 0.8442, Val: 0.8433, Test: 0.8300\n",
      "Epoch: 099, Loss: 0.2839, Train: 0.7896, Val: 0.8033, Test: 0.7933\n",
      "Epoch: 100, Loss: 0.2865, Train: 0.8921, Val: 0.8767, Test: 0.8833\n",
      "Epoch: 101, Loss: 0.2829, Train: 0.8888, Val: 0.8867, Test: 0.8767\n",
      "Epoch: 102, Loss: 0.2769, Train: 0.8904, Val: 0.8933, Test: 0.8800\n",
      "Epoch: 103, Loss: 0.2641, Train: 0.8954, Val: 0.8733, Test: 0.8567\n",
      "Epoch: 104, Loss: 0.3007, Train: 0.9071, Val: 0.9000, Test: 0.8933\n",
      "Epoch: 105, Loss: 0.2845, Train: 0.8596, Val: 0.8467, Test: 0.8167\n",
      "Epoch: 106, Loss: 0.2830, Train: 0.7646, Val: 0.7933, Test: 0.7633\n",
      "Epoch: 107, Loss: 0.2835, Train: 0.8896, Val: 0.8633, Test: 0.8533\n",
      "Epoch: 108, Loss: 0.2905, Train: 0.8804, Val: 0.8533, Test: 0.8333\n",
      "Epoch: 109, Loss: 0.3129, Train: 0.8021, Val: 0.7567, Test: 0.7867\n",
      "Epoch: 110, Loss: 0.2742, Train: 0.9012, Val: 0.8967, Test: 0.8733\n",
      "Epoch: 111, Loss: 0.2727, Train: 0.8892, Val: 0.8767, Test: 0.8700\n",
      "Epoch: 112, Loss: 0.2988, Train: 0.8867, Val: 0.8633, Test: 0.8533\n",
      "Epoch: 113, Loss: 0.2970, Train: 0.8846, Val: 0.8567, Test: 0.8467\n",
      "Epoch: 114, Loss: 0.2681, Train: 0.8967, Val: 0.8700, Test: 0.8400\n",
      "Epoch: 115, Loss: 0.2608, Train: 0.8879, Val: 0.8533, Test: 0.8433\n",
      "Epoch: 116, Loss: 0.2865, Train: 0.9008, Val: 0.8900, Test: 0.8600\n",
      "Epoch: 117, Loss: 0.2697, Train: 0.8758, Val: 0.8567, Test: 0.8200\n",
      "Epoch: 118, Loss: 0.2635, Train: 0.8775, Val: 0.8533, Test: 0.8267\n",
      "Epoch: 119, Loss: 0.2728, Train: 0.8871, Val: 0.8833, Test: 0.8733\n",
      "Epoch: 120, Loss: 0.2708, Train: 0.8454, Val: 0.8167, Test: 0.8000\n",
      "Epoch: 121, Loss: 0.2833, Train: 0.8875, Val: 0.8467, Test: 0.8567\n",
      "Epoch: 122, Loss: 0.2807, Train: 0.9012, Val: 0.9033, Test: 0.8900\n",
      "Epoch: 123, Loss: 0.2730, Train: 0.8967, Val: 0.8867, Test: 0.9067\n",
      "Epoch: 124, Loss: 0.2739, Train: 0.8337, Val: 0.8667, Test: 0.8433\n",
      "Epoch: 125, Loss: 0.2790, Train: 0.8950, Val: 0.9033, Test: 0.8833\n",
      "Epoch: 126, Loss: 0.2757, Train: 0.8754, Val: 0.8700, Test: 0.8233\n",
      "Epoch: 127, Loss: 0.2673, Train: 0.8888, Val: 0.8900, Test: 0.8500\n",
      "Epoch: 128, Loss: 0.2671, Train: 0.9079, Val: 0.9033, Test: 0.8700\n",
      "Epoch: 129, Loss: 0.2729, Train: 0.8838, Val: 0.8533, Test: 0.8500\n",
      "Epoch: 130, Loss: 0.2726, Train: 0.8988, Val: 0.9000, Test: 0.8900\n",
      "Epoch: 131, Loss: 0.2843, Train: 0.8858, Val: 0.8767, Test: 0.8733\n",
      "Epoch: 132, Loss: 0.2532, Train: 0.8954, Val: 0.9000, Test: 0.8533\n",
      "Epoch: 133, Loss: 0.2649, Train: 0.8962, Val: 0.9100, Test: 0.8800\n",
      "Epoch: 134, Loss: 0.2737, Train: 0.9029, Val: 0.9000, Test: 0.9000\n",
      "Epoch: 135, Loss: 0.2728, Train: 0.8712, Val: 0.8867, Test: 0.8600\n",
      "Epoch: 136, Loss: 0.2615, Train: 0.9058, Val: 0.9100, Test: 0.8800\n",
      "Epoch: 137, Loss: 0.2705, Train: 0.9054, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 138, Loss: 0.2822, Train: 0.8829, Val: 0.8500, Test: 0.8267\n",
      "Epoch: 139, Loss: 0.2935, Train: 0.8825, Val: 0.8733, Test: 0.8633\n",
      "Epoch: 140, Loss: 0.2751, Train: 0.9079, Val: 0.9133, Test: 0.8867\n",
      "Epoch: 141, Loss: 0.2572, Train: 0.9008, Val: 0.8800, Test: 0.8467\n",
      "Epoch: 142, Loss: 0.2674, Train: 0.8967, Val: 0.8767, Test: 0.8567\n",
      "Epoch: 143, Loss: 0.2773, Train: 0.8704, Val: 0.8467, Test: 0.8333\n",
      "Epoch: 144, Loss: 0.2899, Train: 0.8938, Val: 0.8433, Test: 0.8667\n",
      "Epoch: 145, Loss: 0.2734, Train: 0.8629, Val: 0.8667, Test: 0.8633\n",
      "Epoch: 146, Loss: 0.2664, Train: 0.9038, Val: 0.8767, Test: 0.9033\n",
      "Epoch: 147, Loss: 0.2682, Train: 0.8950, Val: 0.9000, Test: 0.8600\n",
      "Epoch: 148, Loss: 0.2544, Train: 0.8946, Val: 0.8833, Test: 0.8633\n",
      "Epoch: 149, Loss: 0.2868, Train: 0.8975, Val: 0.9067, Test: 0.8833\n",
      "Epoch: 150, Loss: 0.2654, Train: 0.7913, Val: 0.7967, Test: 0.7500\n",
      "Epoch: 151, Loss: 0.2724, Train: 0.8896, Val: 0.8433, Test: 0.8300\n",
      "Epoch: 152, Loss: 0.2529, Train: 0.8900, Val: 0.8967, Test: 0.8533\n",
      "Epoch: 153, Loss: 0.2776, Train: 0.9046, Val: 0.8833, Test: 0.8767\n",
      "Epoch: 154, Loss: 0.2646, Train: 0.9000, Val: 0.8833, Test: 0.8900\n",
      "Epoch: 155, Loss: 0.2585, Train: 0.8483, Val: 0.8200, Test: 0.8000\n",
      "Epoch: 156, Loss: 0.2508, Train: 0.8742, Val: 0.8833, Test: 0.8800\n",
      "Epoch: 157, Loss: 0.2635, Train: 0.8912, Val: 0.8800, Test: 0.8767\n",
      "Epoch: 158, Loss: 0.2576, Train: 0.9100, Val: 0.9000, Test: 0.8933\n",
      "Epoch: 159, Loss: 0.2513, Train: 0.7937, Val: 0.8433, Test: 0.7800\n",
      "Epoch: 160, Loss: 0.2716, Train: 0.8971, Val: 0.8933, Test: 0.8533\n",
      "Epoch: 161, Loss: 0.2619, Train: 0.8387, Val: 0.8033, Test: 0.8033\n",
      "Epoch: 162, Loss: 0.2675, Train: 0.9008, Val: 0.8800, Test: 0.8633\n",
      "Epoch: 163, Loss: 0.2701, Train: 0.9062, Val: 0.8900, Test: 0.8733\n",
      "Epoch: 164, Loss: 0.2680, Train: 0.9004, Val: 0.8967, Test: 0.8867\n",
      "Epoch: 165, Loss: 0.2578, Train: 0.9000, Val: 0.8967, Test: 0.8867\n",
      "Epoch: 166, Loss: 0.2583, Train: 0.7246, Val: 0.7500, Test: 0.7100\n",
      "Epoch: 167, Loss: 0.2563, Train: 0.9096, Val: 0.9000, Test: 0.8933\n",
      "Epoch: 168, Loss: 0.2395, Train: 0.8533, Val: 0.8267, Test: 0.8067\n",
      "Epoch: 169, Loss: 0.2520, Train: 0.9054, Val: 0.9133, Test: 0.8767\n",
      "Epoch: 170, Loss: 0.2641, Train: 0.9075, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 171, Loss: 0.2614, Train: 0.9038, Val: 0.8967, Test: 0.8933\n",
      "Epoch: 172, Loss: 0.2567, Train: 0.9021, Val: 0.8933, Test: 0.8667\n",
      "Epoch: 173, Loss: 0.2530, Train: 0.8679, Val: 0.8200, Test: 0.8100\n",
      "Epoch: 174, Loss: 0.2675, Train: 0.7638, Val: 0.8067, Test: 0.7900\n",
      "Epoch: 175, Loss: 0.2448, Train: 0.9038, Val: 0.9200, Test: 0.8967\n",
      "Epoch: 176, Loss: 0.2467, Train: 0.9054, Val: 0.9100, Test: 0.8833\n",
      "Epoch: 177, Loss: 0.2631, Train: 0.8900, Val: 0.8900, Test: 0.9000\n",
      "Epoch: 178, Loss: 0.2621, Train: 0.8929, Val: 0.8733, Test: 0.8933\n",
      "Epoch: 179, Loss: 0.2721, Train: 0.8933, Val: 0.8800, Test: 0.8933\n",
      "Epoch: 180, Loss: 0.2582, Train: 0.8988, Val: 0.8967, Test: 0.8833\n",
      "Epoch: 181, Loss: 0.2559, Train: 0.8942, Val: 0.8900, Test: 0.8633\n",
      "Epoch: 182, Loss: 0.2541, Train: 0.9067, Val: 0.9100, Test: 0.8800\n",
      "Epoch: 183, Loss: 0.2296, Train: 0.9038, Val: 0.8967, Test: 0.8767\n",
      "Epoch: 184, Loss: 0.2526, Train: 0.8950, Val: 0.9133, Test: 0.8800\n",
      "Epoch: 185, Loss: 0.2679, Train: 0.9004, Val: 0.8833, Test: 0.8700\n",
      "Epoch: 186, Loss: 0.2455, Train: 0.8925, Val: 0.8667, Test: 0.8467\n",
      "Epoch: 187, Loss: 0.2618, Train: 0.8013, Val: 0.8100, Test: 0.7900\n",
      "Epoch: 188, Loss: 0.2727, Train: 0.8979, Val: 0.8800, Test: 0.8667\n",
      "Epoch: 189, Loss: 0.2591, Train: 0.9046, Val: 0.9000, Test: 0.9067\n",
      "Epoch: 190, Loss: 0.2631, Train: 0.9058, Val: 0.8800, Test: 0.8767\n",
      "Epoch: 191, Loss: 0.2439, Train: 0.8746, Val: 0.8933, Test: 0.8533\n",
      "Epoch: 192, Loss: 0.2526, Train: 0.8933, Val: 0.8967, Test: 0.8900\n",
      "Epoch: 193, Loss: 0.2444, Train: 0.9000, Val: 0.9167, Test: 0.8633\n",
      "Epoch: 194, Loss: 0.2566, Train: 0.9083, Val: 0.8933, Test: 0.8767\n",
      "Epoch: 195, Loss: 0.2545, Train: 0.8821, Val: 0.8900, Test: 0.8900\n",
      "Epoch: 196, Loss: 0.2611, Train: 0.8350, Val: 0.8300, Test: 0.7900\n",
      "Epoch: 197, Loss: 0.2508, Train: 0.9113, Val: 0.9000, Test: 0.9200\n",
      "Epoch: 198, Loss: 0.2354, Train: 0.9012, Val: 0.9233, Test: 0.9033\n",
      "Epoch: 199, Loss: 0.2388, Train: 0.7446, Val: 0.7767, Test: 0.7567\n",
      "Epoch: 200, Loss: 0.2448, Train: 0.6646, Val: 0.6867, Test: 0.6767\n",
      "Epoch: 201, Loss: 0.2505, Train: 0.8946, Val: 0.8633, Test: 0.8667\n",
      "Epoch: 202, Loss: 0.2403, Train: 0.9092, Val: 0.8833, Test: 0.8767\n",
      "Epoch: 203, Loss: 0.2428, Train: 0.7712, Val: 0.7867, Test: 0.7767\n",
      "Epoch: 204, Loss: 0.2677, Train: 0.9100, Val: 0.9033, Test: 0.8700\n",
      "Epoch: 205, Loss: 0.2508, Train: 0.9046, Val: 0.9067, Test: 0.8933\n",
      "Epoch: 206, Loss: 0.2434, Train: 0.8167, Val: 0.8333, Test: 0.8067\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(net_model, train_loader, opt)\n\u001b[1;32m---> 33\u001b[0m     train_acc, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     val_acc, val_loss \u001b[38;5;241m=\u001b[39m test(net_model, val_loader)\n\u001b[0;32m     35\u001b[0m     test_acc, _ \u001b[38;5;241m=\u001b[39m test(net_model, test_loader)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\un\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     10\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss) \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_graphs\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\un\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[20], line 140\u001b[0m, in \u001b[0;36mGIN_Pool_Net.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    138\u001b[0m     cluster \u001b[38;5;241m=\u001b[39m graclus(adj, num_nodes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     complement \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_negative_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_undirected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     cluster \u001b[38;5;241m=\u001b[39m graclus(complement, num_nodes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    142\u001b[0m data \u001b[38;5;241m=\u001b[39m sum_pool(cluster, data)\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mbatched_negative_edges\u001b[1;34m(edge_index, batch, num_neg_samples, force_undirected)\u001b[0m\n\u001b[0;32m      9\u001b[0m neg_edge_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m edge_index, N, C \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(edge_indices, num_nodes\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     11\u001b[0m                             cum_nodes\u001b[38;5;241m.\u001b[39mtolist()):\n\u001b[1;32m---> 12\u001b[0m     neg_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mnegative_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neg_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mforce_undirected\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m C\n\u001b[0;32m     14\u001b[0m     neg_edge_indices\u001b[38;5;241m.\u001b[39mappend(neg_edge_index)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(neg_edge_indices, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m, in \u001b[0;36mnegative_edges\u001b[1;34m(edge_index, num_nodes, num_neg_samples, force_undirected)\u001b[0m\n\u001b[0;32m     25\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m num_nodes \u001b[38;5;241m+\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m perm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rng)\n\u001b[1;32m---> 28\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m     29\u001b[0m rest \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m rest\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\un\\lib\\site-packages\\numpy\\lib\\arraysetops.py:890\u001b[0m, in \u001b[0;36misin\u001b[1;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;124;03mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m       [ True, False]])\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    889\u001b[0m element \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(element)\n\u001b[1;32m--> 890\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43min1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_unique\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(element\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\un\\lib\\site-packages\\numpy\\lib\\arraysetops.py:694\u001b[0m, in \u001b[0;36min1d\u001b[1;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[0;32m    692\u001b[0m     isin_helper_ar[ar2 \u001b[38;5;241m-\u001b[39m ar2_min] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     isin_helper_ar \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar2_range\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m     isin_helper_ar[ar2 \u001b[38;5;241m-\u001b[39m ar2_min] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;66;03m# Mask out elements we know won't work\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'comp-graclus'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GIN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_Pool_Net(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,           # Size of node features\n",
    "                 out_channels,          # Number of classes\n",
    "                 num_layers_pre=1,      # Number of GIN layers before pooling\n",
    "                 num_layers_post=1,     # Number of GIN layers after pooling\n",
    "                 hidden_channels=64,    # Dimensionality of node embeddings\n",
    "                 norm=True,             # Normalise Layers in the GIN MLP\n",
    "                 activation='ELU',      # Activation of the MLP in GIN \n",
    "                 average_nodes=None,    # Needed for dense pooling methods\n",
    "                 max_nodes=None,        # Needed for random pool\n",
    "                 pooling=None,          # Pooling method\n",
    "                 pool_ratio=0.1,        # Ratio = nodes_after_pool/nodes_before_pool\n",
    "                 ):\n",
    "        super(GCN_Pool_Net, self).__init__()\n",
    "        \n",
    "        self.num_layers_pre = num_layers_pre\n",
    "        self.num_layers_post = num_layers_post\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.act = activation_resolver(activation)\n",
    "        self.pooling = pooling\n",
    "        self.pool_ratio = pool_ratio\n",
    "  \n",
    "        # Pre-pooling block            \n",
    "        self.conv_layers_pre = torch.nn.ModuleList()\n",
    "        if pooling=='panpool':\n",
    "            for _ in range(num_layers_pre):\n",
    "                self.conv_layers_pre.append(PANConv(in_channels, hidden_channels, filter_size=2))\n",
    "                in_channels = hidden_channels\n",
    "        else:\n",
    "            for _ in range(num_layers_pre):\n",
    "                self.conv_layers_pre.append(GCNConv(in_channels, hidden_channels))\n",
    "                in_channels = hidden_channels\n",
    "                        \n",
    "        # Pooling block\n",
    "        pooled_nodes = ceil(pool_ratio * average_nodes)\n",
    "        if pooling in ['diffpool','mincut']:\n",
    "            self.pool = Linear(hidden_channels, pooled_nodes)\n",
    "        elif pooling=='dmon':\n",
    "            self.pool = DMoNPooling(hidden_channels, pooled_nodes)\n",
    "        elif pooling=='dense-random':\n",
    "            self.s_rnd = torch.randn(max_nodes, pooled_nodes)\n",
    "            self.s_rnd.requires_grad = False\n",
    "        elif pooling=='topk':\n",
    "            self.pool = TopKPooling(hidden_channels, ratio=pool_ratio)\n",
    "        elif pooling=='panpool':\n",
    "            self.pool = PANPooling(hidden_channels, ratio=pool_ratio)\n",
    "        elif pooling=='sagpool':\n",
    "            self.pool = SAGPooling(hidden_channels, ratio=pool_ratio)\n",
    "        elif pooling=='asapool':\n",
    "            self.pool = ASAPooling(hidden_channels, ratio=pool_ratio)  \n",
    "        elif pooling=='edgepool':\n",
    "            self.pool = EdgePooling(hidden_channels)\n",
    "        elif pooling=='kmis':\n",
    "            self.pool = KMISPooling(hidden_channels, k=5, aggr_x='sum')\n",
    "        elif pooling in ['graclus', 'comp-graclus']:\n",
    "            pass\n",
    "        elif pooling=='sparse-random':\n",
    "            self.pool = RndSparse(pool_ratio, max_nodes)\n",
    "        else:\n",
    "            assert pooling==None\n",
    "        \n",
    "        # Post-pooling block\n",
    "        self.conv_layers_post = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers_post):\n",
    "            mlp = MLP([hidden_channels, hidden_channels, hidden_channels], act=activation, norm=None)\n",
    "            if pooling in ['diffpool','mincut','dmon','dense-random']:\n",
    "                self.conv_layers_post.append(DenseGCNConv(hidden_channels, hidden_channels, mlp))                 \n",
    "            else:\n",
    "                (self.conv_layers_post.append(GCNConv(hidden_channels, hidden_channels))\n",
    ")\n",
    "\n",
    "        # Readout\n",
    "        self.mlp = MLP([hidden_channels, hidden_channels, hidden_channels//2, out_channels], \n",
    "                        act=activation,\n",
    "                        norm=None,\n",
    "                        dropout=0.5)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for (name, module) in self._modules.items():\n",
    "            try:\n",
    "                module.reset_parameters()\n",
    "            except AttributeError:\n",
    "                if name != 'act':\n",
    "                    for x in module:\n",
    "                        x.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x    \n",
    "        adj = data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        ### pre-pooling block\n",
    "        if self.pooling=='panpool':\n",
    "            M = adj\n",
    "            for layer in self.conv_layers_pre:  \n",
    "                x, M = layer(x, M)\n",
    "                x = self.act(x)\n",
    "        else:\n",
    "            for layer in self.conv_layers_pre:  \n",
    "                x = self.act(layer(x, adj))\n",
    "    \n",
    "        ### pooling block\n",
    "        if self.pooling in ['diffpool','mincut','dmon','dense-random']:\n",
    "            x, mask = to_dense_batch(x, batch)\n",
    "            adj = to_dense_adj(adj, batch)\n",
    "            if self.pooling=='diffpool':\n",
    "                s = self.pool(x)\n",
    "                x, adj, l1, l2 = dense_diff_pool(x, adj, s, mask)\n",
    "                aux_loss = 0.1*l1 + 0.1*l2\n",
    "            elif self.pooling=='mincut':\n",
    "                s = self.pool(x)\n",
    "                x, adj, l1, l2 = dense_mincut_pool(x, adj, s, mask)\n",
    "                aux_loss = 0.5*l1 + l2\n",
    "            elif self.pooling=='dmon':  \n",
    "                _, x, adj, l1, l2, l3 = self.pool(x, adj, mask)\n",
    "                aux_loss = 0.3*l1 + 0.3*l2 + 0.3*l3\n",
    "            elif self.pooling=='dense-random':\n",
    "                s = self.s_rnd[:x.size(1), :].unsqueeze(dim=0).expand(x.size(0), -1, -1).to(x.device)\n",
    "                x, adj, _, _ = dense_diff_pool(x, adj, s, mask)\n",
    "        elif self.pooling in ['topk', 'sagpool', 'sparse-random']:\n",
    "            x, adj, _, batch, _, _ = self.pool(x, adj, edge_attr=None, batch=batch)\n",
    "        elif self.pooling=='asapool':\n",
    "            x, adj, _, batch, _ = self.pool(x, adj, batch=batch)\n",
    "        elif self.pooling=='panpool':\n",
    "            x, adj, _, batch, _, _ = self.pool(x, M, batch=batch)\n",
    "        elif self.pooling=='edgepool':\n",
    "            x, adj, batch, _ = self.pool(x, adj, batch=batch)\n",
    "        elif self.pooling=='kmis':\n",
    "            x, adj, _, batch, _, _ = self.pool(x, adj, None, batch=batch)\n",
    "        elif self.pooling in ['graclus', 'comp-graclus']:\n",
    "            data.x = x    \n",
    "            if self.pooling == 'graclus':\n",
    "                cluster = graclus(adj, num_nodes=data.x.size(0))\n",
    "            else:\n",
    "                complement = batched_negative_edges(edge_index=adj, batch=batch, force_undirected=True)\n",
    "                cluster = graclus(complement, num_nodes=data.x.size(0))\n",
    "            data = sum_pool(cluster, data)\n",
    "            x = data.x    \n",
    "            adj = data.edge_index\n",
    "            batch = data.batch\n",
    "        elif self.pooling==None:\n",
    "            pass\n",
    "        else:\n",
    "            raise KeyError(\"unrecognized pooling method\")\n",
    "                \n",
    "        ### post-pooling block\n",
    "        for layer in self.conv_layers_post:  \n",
    "            x = self.act(layer(x, adj))\n",
    "\n",
    "        ### readout\n",
    "        if self.pooling in ['diffpool','mincut','dmon','dense-random']:\n",
    "            x = torch.sum(x, dim=1)\n",
    "        else:\n",
    "            x = global_add_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        if 'aux_loss' not in locals():\n",
    "            aux_loss=0\n",
    "        return F.log_softmax(x, dim=-1), aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch: 001, Loss: 0.9501, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 002, Loss: 0.7609, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 003, Loss: 0.7361, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 004, Loss: 0.7309, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 005, Loss: 0.7173, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 006, Loss: 0.7073, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 007, Loss: 0.7122, Train: 0.4983, Val: 0.5133, Test: 0.5000\n",
      "Epoch: 008, Loss: 0.7136, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 009, Loss: 0.7067, Train: 0.5012, Val: 0.4967, Test: 0.5000\n",
      "Epoch: 010, Loss: 0.7078, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 011, Loss: 0.7078, Train: 0.4983, Val: 0.5133, Test: 0.5000\n",
      "Epoch: 012, Loss: 0.7079, Train: 0.4992, Val: 0.5067, Test: 0.5000\n",
      "Epoch: 013, Loss: 0.7054, Train: 0.4983, Val: 0.5133, Test: 0.5000\n",
      "Epoch: 014, Loss: 0.7015, Train: 0.5083, Val: 0.4767, Test: 0.5267\n",
      "Epoch: 015, Loss: 0.7071, Train: 0.4983, Val: 0.5133, Test: 0.5000\n",
      "Epoch: 016, Loss: 0.6984, Train: 0.4983, Val: 0.5133, Test: 0.5000\n",
      "Epoch: 017, Loss: 0.7047, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 018, Loss: 0.7050, Train: 0.5008, Val: 0.4933, Test: 0.5000\n",
      "Epoch: 019, Loss: 0.7016, Train: 0.5038, Val: 0.4733, Test: 0.4967\n",
      "Epoch: 020, Loss: 0.7023, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 021, Loss: 0.7036, Train: 0.5025, Val: 0.5067, Test: 0.5133\n",
      "Epoch: 022, Loss: 0.7020, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 023, Loss: 0.6996, Train: 0.5008, Val: 0.4833, Test: 0.5100\n",
      "Epoch: 024, Loss: 0.7030, Train: 0.5017, Val: 0.4833, Test: 0.5033\n",
      "Epoch: 025, Loss: 0.7015, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 026, Loss: 0.6996, Train: 0.4983, Val: 0.5133, Test: 0.5000\n",
      "Epoch: 027, Loss: 0.6998, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 028, Loss: 0.7027, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 029, Loss: 0.7001, Train: 0.5012, Val: 0.4867, Test: 0.5067\n",
      "Epoch: 030, Loss: 0.7004, Train: 0.4983, Val: 0.5100, Test: 0.5033\n",
      "Epoch: 031, Loss: 0.6964, Train: 0.5050, Val: 0.4667, Test: 0.4933\n",
      "Epoch: 032, Loss: 0.7023, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 033, Loss: 0.6979, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 034, Loss: 0.6982, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 035, Loss: 0.6986, Train: 0.4983, Val: 0.5133, Test: 0.5000\n",
      "Epoch: 036, Loss: 0.6982, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 037, Loss: 0.6991, Train: 0.5038, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 038, Loss: 0.6980, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 039, Loss: 0.6971, Train: 0.5033, Val: 0.5133, Test: 0.5067\n",
      "Epoch: 040, Loss: 0.6976, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 041, Loss: 0.6992, Train: 0.4996, Val: 0.5100, Test: 0.5033\n",
      "Epoch: 042, Loss: 0.7004, Train: 0.5050, Val: 0.4933, Test: 0.5200\n",
      "Epoch: 043, Loss: 0.7001, Train: 0.5012, Val: 0.4867, Test: 0.5033\n",
      "Epoch: 044, Loss: 0.6971, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 045, Loss: 0.6968, Train: 0.5004, Val: 0.4833, Test: 0.5133\n",
      "Epoch: 046, Loss: 0.6932, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 047, Loss: 0.6994, Train: 0.5008, Val: 0.4867, Test: 0.5067\n",
      "Epoch: 048, Loss: 0.7002, Train: 0.5033, Val: 0.4833, Test: 0.5133\n",
      "Epoch: 049, Loss: 0.6972, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "Epoch: 050, Loss: 0.6976, Train: 0.5017, Val: 0.4867, Test: 0.5000\n",
      "### Run 0 - val loss: 0.693, test acc: 0.500\n",
      "Accuracies in each run:  [0.5]\n",
      "test acc - mean: 0.500, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = None\n",
    "for r in range(args.runs):  \n",
    "    print(args.pooling)\n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffpool\n",
      "Epoch: 001, Loss: 1.0025, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 002, Loss: 0.9284, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 003, Loss: 0.8966, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 004, Loss: 0.8822, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 005, Loss: 0.8838, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 006, Loss: 0.8705, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 007, Loss: 0.8749, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 008, Loss: 0.8695, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 009, Loss: 0.8660, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 010, Loss: 0.8610, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 011, Loss: 0.8550, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 012, Loss: 0.8550, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 013, Loss: 0.8501, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 014, Loss: 0.8474, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 015, Loss: 0.8455, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 016, Loss: 0.8397, Train: 0.4975, Val: 0.5067, Test: 0.4900\n",
      "Epoch: 017, Loss: 0.8375, Train: 0.5000, Val: 0.4767, Test: 0.5233\n",
      "Epoch: 018, Loss: 0.8434, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 019, Loss: 0.8350, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 020, Loss: 0.8322, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 021, Loss: 0.8290, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 022, Loss: 0.8300, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 023, Loss: 0.8280, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 024, Loss: 0.8229, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 025, Loss: 0.8227, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 026, Loss: 0.8240, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 027, Loss: 0.8178, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 028, Loss: 0.8143, Train: 0.5017, Val: 0.5100, Test: 0.4767\n",
      "Epoch: 029, Loss: 0.8116, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 030, Loss: 0.8014, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 031, Loss: 0.7998, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 032, Loss: 0.7909, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 033, Loss: 0.7809, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 034, Loss: 0.7743, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 035, Loss: 0.7633, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 036, Loss: 0.7488, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 037, Loss: 0.7413, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 038, Loss: 0.7359, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 039, Loss: 0.7237, Train: 0.5012, Val: 0.5367, Test: 0.4667\n",
      "Epoch: 040, Loss: 0.7202, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 041, Loss: 0.7174, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "Epoch: 042, Loss: 0.7155, Train: 0.4979, Val: 0.5133, Test: 0.4867\n",
      "Epoch: 043, Loss: 0.7146, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 044, Loss: 0.7095, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 045, Loss: 0.7072, Train: 0.5000, Val: 0.5167, Test: 0.4800\n",
      "Epoch: 046, Loss: 0.7049, Train: 0.4971, Val: 0.5300, Test: 0.4733\n",
      "Epoch: 047, Loss: 0.7060, Train: 0.5008, Val: 0.5167, Test: 0.4767\n",
      "Epoch: 048, Loss: 0.7050, Train: 0.5000, Val: 0.5167, Test: 0.4833\n",
      "Epoch: 049, Loss: 0.7061, Train: 0.5004, Val: 0.4833, Test: 0.5200\n",
      "Epoch: 050, Loss: 0.7050, Train: 0.4992, Val: 0.4833, Test: 0.5233\n",
      "### Run 0 - val loss: 0.693, test acc: 0.477\n",
      "Accuracies in each run:  [0.4766666666666667]\n",
      "test acc - mean: 0.477, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'diffpool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mincut\n",
      "Epoch: 001, Loss: 1.4766, Train: 0.5008, Val: 0.5300, Test: 0.4633\n",
      "Epoch: 002, Loss: 1.3965, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 003, Loss: 1.3680, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 004, Loss: 1.3560, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 005, Loss: 1.3486, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 006, Loss: 1.3495, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 007, Loss: 1.3473, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 008, Loss: 1.3457, Train: 0.5046, Val: 0.5100, Test: 0.4533\n",
      "Epoch: 009, Loss: 1.3475, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 010, Loss: 1.3414, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 011, Loss: 1.3408, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 012, Loss: 1.3464, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 013, Loss: 1.3441, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 014, Loss: 1.3429, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 015, Loss: 1.3361, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 016, Loss: 1.3418, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 017, Loss: 1.3425, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 018, Loss: 1.3407, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 019, Loss: 1.3401, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 020, Loss: 1.3387, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 021, Loss: 1.3394, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 022, Loss: 1.3400, Train: 0.5004, Val: 0.5500, Test: 0.4500\n",
      "Epoch: 023, Loss: 1.3370, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 024, Loss: 1.3389, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 025, Loss: 1.3339, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 026, Loss: 1.3400, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 027, Loss: 1.3370, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 028, Loss: 1.3353, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 029, Loss: 1.3394, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 030, Loss: 1.3379, Train: 0.5000, Val: 0.5500, Test: 0.4600\n",
      "Epoch: 031, Loss: 1.3380, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 032, Loss: 1.3366, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 033, Loss: 1.3384, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 034, Loss: 1.3373, Train: 0.5017, Val: 0.5267, Test: 0.4600\n",
      "Epoch: 035, Loss: 1.3372, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 036, Loss: 1.3359, Train: 0.4988, Val: 0.5367, Test: 0.4700\n",
      "Epoch: 037, Loss: 1.3358, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 038, Loss: 1.3359, Train: 0.5017, Val: 0.5267, Test: 0.4600\n",
      "Epoch: 039, Loss: 1.3366, Train: 0.5021, Val: 0.5233, Test: 0.4633\n",
      "Epoch: 040, Loss: 1.3326, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 041, Loss: 1.3360, Train: 0.5071, Val: 0.5133, Test: 0.4300\n",
      "Epoch: 042, Loss: 1.3340, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 043, Loss: 1.3352, Train: 0.5008, Val: 0.5100, Test: 0.4800\n",
      "Epoch: 044, Loss: 1.3363, Train: 0.4983, Val: 0.4600, Test: 0.5533\n",
      "Epoch: 045, Loss: 1.3335, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 046, Loss: 1.3355, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "Epoch: 047, Loss: 1.3293, Train: 0.5012, Val: 0.5267, Test: 0.4633\n",
      "Epoch: 048, Loss: 1.3341, Train: 0.5042, Val: 0.5533, Test: 0.4400\n",
      "Epoch: 049, Loss: 1.3361, Train: 0.5067, Val: 0.4967, Test: 0.4533\n",
      "Epoch: 050, Loss: 1.3353, Train: 0.4988, Val: 0.4733, Test: 0.5367\n",
      "### Run 0 - val loss: 0.692, test acc: 0.463\n",
      "Accuracies in each run:  [0.4633333333333333]\n",
      "test acc - mean: 0.463, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tot_acc = []\n",
    "args.pooling = 'mincut'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmon\n",
      "Epoch: 001, Loss: 2.1830, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 002, Loss: 2.1152, Train: 0.5012, Val: 0.4767, Test: 0.5133\n",
      "Epoch: 003, Loss: 2.1136, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 004, Loss: 2.1099, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 005, Loss: 2.1022, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 006, Loss: 2.0942, Train: 0.5079, Val: 0.4767, Test: 0.4933\n",
      "Epoch: 007, Loss: 2.0988, Train: 0.4979, Val: 0.5267, Test: 0.4867\n",
      "Epoch: 008, Loss: 2.0978, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 009, Loss: 2.0914, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 010, Loss: 2.1034, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 011, Loss: 2.0958, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 012, Loss: 2.0977, Train: 0.5083, Val: 0.4733, Test: 0.4833\n",
      "Epoch: 013, Loss: 2.0926, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 014, Loss: 2.1013, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 015, Loss: 2.0950, Train: 0.5083, Val: 0.4733, Test: 0.4800\n",
      "Epoch: 016, Loss: 2.0950, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 017, Loss: 2.0966, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 018, Loss: 2.0955, Train: 0.5125, Val: 0.4667, Test: 0.4733\n",
      "Epoch: 019, Loss: 2.0899, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 020, Loss: 2.0943, Train: 0.5025, Val: 0.4700, Test: 0.5167\n",
      "Epoch: 021, Loss: 2.0979, Train: 0.5088, Val: 0.4767, Test: 0.4767\n",
      "Epoch: 022, Loss: 2.0951, Train: 0.5038, Val: 0.4900, Test: 0.4900\n",
      "Epoch: 023, Loss: 2.0934, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 024, Loss: 2.0912, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 025, Loss: 2.0961, Train: 0.5008, Val: 0.4800, Test: 0.5167\n",
      "Epoch: 026, Loss: 2.0927, Train: 0.5092, Val: 0.4867, Test: 0.5067\n",
      "Epoch: 027, Loss: 2.0914, Train: 0.5021, Val: 0.4867, Test: 0.4967\n",
      "Epoch: 028, Loss: 2.0962, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 029, Loss: 2.0874, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 030, Loss: 2.0930, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 031, Loss: 2.0945, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 032, Loss: 2.0934, Train: 0.5121, Val: 0.4767, Test: 0.4600\n",
      "Epoch: 033, Loss: 2.0970, Train: 0.5054, Val: 0.4600, Test: 0.5133\n",
      "Epoch: 034, Loss: 2.0918, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 035, Loss: 2.0974, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 036, Loss: 2.0926, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 037, Loss: 2.0905, Train: 0.4992, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 038, Loss: 2.1004, Train: 0.5008, Val: 0.4767, Test: 0.5200\n",
      "Epoch: 039, Loss: 2.0894, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 040, Loss: 2.0925, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 041, Loss: 2.0874, Train: 0.5025, Val: 0.4667, Test: 0.5200\n",
      "Epoch: 042, Loss: 2.0841, Train: 0.4992, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 043, Loss: 2.0932, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 044, Loss: 2.0923, Train: 0.5096, Val: 0.4667, Test: 0.4967\n",
      "Epoch: 045, Loss: 2.0925, Train: 0.5125, Val: 0.4833, Test: 0.5033\n",
      "Epoch: 046, Loss: 2.0913, Train: 0.5129, Val: 0.4600, Test: 0.4733\n",
      "Epoch: 047, Loss: 2.0920, Train: 0.5058, Val: 0.4833, Test: 0.4700\n",
      "Epoch: 048, Loss: 2.0970, Train: 0.5012, Val: 0.5200, Test: 0.4733\n",
      "Epoch: 049, Loss: 2.0903, Train: 0.4988, Val: 0.5233, Test: 0.4867\n",
      "Epoch: 050, Loss: 2.0929, Train: 0.5062, Val: 0.4833, Test: 0.4733\n",
      "### Run 0 - val loss: 0.693, test acc: 0.487\n",
      "Accuracies in each run:  [0.4866666666666667]\n",
      "test acc - mean: 0.487, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tot_acc = []\n",
    "args.pooling = 'dmon'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edgepool\n",
      "Epoch: 001, Loss: 0.7993, Train: 0.4938, Val: 0.5200, Test: 0.4867\n",
      "Epoch: 002, Loss: 0.7306, Train: 0.4908, Val: 0.4733, Test: 0.4900\n",
      "Epoch: 003, Loss: 0.7193, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 004, Loss: 0.7123, Train: 0.5058, Val: 0.4700, Test: 0.4800\n",
      "Epoch: 005, Loss: 0.7081, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 006, Loss: 0.7075, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 007, Loss: 0.7036, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 008, Loss: 0.7084, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 009, Loss: 0.7092, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 010, Loss: 0.7080, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 011, Loss: 0.7046, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 012, Loss: 0.7061, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 013, Loss: 0.7036, Train: 0.4908, Val: 0.5033, Test: 0.4733\n",
      "Epoch: 014, Loss: 0.7004, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 015, Loss: 0.6990, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 016, Loss: 0.7004, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 017, Loss: 0.7031, Train: 0.4883, Val: 0.5200, Test: 0.5100\n",
      "Epoch: 018, Loss: 0.6982, Train: 0.4954, Val: 0.4733, Test: 0.4833\n",
      "Epoch: 019, Loss: 0.7014, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 020, Loss: 0.7031, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 021, Loss: 0.7004, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 022, Loss: 0.6967, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 023, Loss: 0.7034, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 024, Loss: 0.7009, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 025, Loss: 0.6986, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 026, Loss: 0.6973, Train: 0.5058, Val: 0.4700, Test: 0.4800\n",
      "Epoch: 027, Loss: 0.6998, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 028, Loss: 0.6994, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 029, Loss: 0.6980, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 030, Loss: 0.6969, Train: 0.4913, Val: 0.5033, Test: 0.4767\n",
      "Epoch: 031, Loss: 0.6972, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 032, Loss: 0.6994, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 033, Loss: 0.6981, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 034, Loss: 0.6963, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 035, Loss: 0.7023, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 036, Loss: 0.6958, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 037, Loss: 0.6980, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 038, Loss: 0.6968, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 039, Loss: 0.6998, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 040, Loss: 0.6997, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 041, Loss: 0.6976, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 042, Loss: 0.6993, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 043, Loss: 0.6987, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 044, Loss: 0.6955, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 045, Loss: 0.6977, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 046, Loss: 0.6957, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 047, Loss: 0.6970, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 048, Loss: 0.6956, Train: 0.5054, Val: 0.4700, Test: 0.4867\n",
      "Epoch: 049, Loss: 0.6967, Train: 0.4946, Val: 0.5300, Test: 0.5133\n",
      "Epoch: 050, Loss: 0.6956, Train: 0.5100, Val: 0.4967, Test: 0.5133\n",
      "### Run 0 - val loss: 0.691, test acc: 0.513\n",
      "Accuracies in each run:  [0.5133333333333333]\n",
      "test acc - mean: 0.513, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tot_acc = []\n",
    "args.pooling = 'edgepool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graclus\n",
      "Epoch: 001, Loss: 0.8851, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 002, Loss: 0.7244, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 003, Loss: 0.7228, Train: 0.5054, Val: 0.4933, Test: 0.4900\n",
      "Epoch: 004, Loss: 0.7118, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 005, Loss: 0.7088, Train: 0.5008, Val: 0.4800, Test: 0.5133\n",
      "Epoch: 006, Loss: 0.7048, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 007, Loss: 0.7087, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 008, Loss: 0.7082, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 009, Loss: 0.7088, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 010, Loss: 0.7042, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 011, Loss: 0.7043, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 012, Loss: 0.7042, Train: 0.5008, Val: 0.5167, Test: 0.4833\n",
      "Epoch: 013, Loss: 0.7078, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 014, Loss: 0.7035, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 015, Loss: 0.7032, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 016, Loss: 0.7030, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 017, Loss: 0.7026, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 018, Loss: 0.7070, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 019, Loss: 0.7064, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 020, Loss: 0.6982, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 021, Loss: 0.7011, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 022, Loss: 0.7015, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 023, Loss: 0.7013, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 024, Loss: 0.7036, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 025, Loss: 0.6987, Train: 0.4992, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 026, Loss: 0.7008, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 027, Loss: 0.6982, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 028, Loss: 0.7005, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 029, Loss: 0.7004, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 030, Loss: 0.6979, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 031, Loss: 0.6982, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 032, Loss: 0.6985, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 033, Loss: 0.7002, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 034, Loss: 0.7030, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 035, Loss: 0.6982, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 036, Loss: 0.6959, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 037, Loss: 0.6986, Train: 0.5025, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 038, Loss: 0.6997, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 039, Loss: 0.6974, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 040, Loss: 0.6973, Train: 0.5138, Val: 0.5033, Test: 0.5100\n",
      "Epoch: 041, Loss: 0.6962, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 042, Loss: 0.6978, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 043, Loss: 0.6991, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 044, Loss: 0.6968, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 045, Loss: 0.6958, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 046, Loss: 0.6969, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 047, Loss: 0.6951, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 048, Loss: 0.6978, Train: 0.4988, Val: 0.5200, Test: 0.4900\n",
      "Epoch: 049, Loss: 0.6985, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "Epoch: 050, Loss: 0.6966, Train: 0.5012, Val: 0.4800, Test: 0.5100\n",
      "### Run 0 - val loss: 0.693, test acc: 0.490\n",
      "Accuracies in each run:  [0.49]\n",
      "test acc - mean: 0.490, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'graclus'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmis\n",
      "Epoch: 001, Loss: 0.7271, Train: 0.4992, Val: 0.4967, Test: 0.5100\n",
      "Epoch: 002, Loss: 0.7152, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 003, Loss: 0.7057, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 004, Loss: 0.7050, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 005, Loss: 0.7008, Train: 0.5017, Val: 0.5067, Test: 0.4600\n",
      "Epoch: 006, Loss: 0.7071, Train: 0.4829, Val: 0.5167, Test: 0.5533\n",
      "Epoch: 007, Loss: 0.7002, Train: 0.5004, Val: 0.5000, Test: 0.5600\n",
      "Epoch: 008, Loss: 0.7004, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 009, Loss: 0.6998, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 010, Loss: 0.6999, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 011, Loss: 0.6974, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 012, Loss: 0.7001, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 013, Loss: 0.6986, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 014, Loss: 0.6998, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 015, Loss: 0.6965, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 016, Loss: 0.6982, Train: 0.4846, Val: 0.4967, Test: 0.5233\n",
      "Epoch: 017, Loss: 0.7038, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 018, Loss: 0.6980, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 019, Loss: 0.6955, Train: 0.5046, Val: 0.5000, Test: 0.4600\n",
      "Epoch: 020, Loss: 0.6964, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 021, Loss: 0.6982, Train: 0.4942, Val: 0.5033, Test: 0.5333\n",
      "Epoch: 022, Loss: 0.6949, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 023, Loss: 0.6995, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 024, Loss: 0.6969, Train: 0.4892, Val: 0.5367, Test: 0.5500\n",
      "Epoch: 025, Loss: 0.6951, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 026, Loss: 0.6958, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 027, Loss: 0.6977, Train: 0.4963, Val: 0.5000, Test: 0.5367\n",
      "Epoch: 028, Loss: 0.6953, Train: 0.5021, Val: 0.5033, Test: 0.4633\n",
      "Epoch: 029, Loss: 0.6949, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 030, Loss: 0.6982, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 031, Loss: 0.6961, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 032, Loss: 0.6942, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 033, Loss: 0.6930, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 034, Loss: 0.6955, Train: 0.5050, Val: 0.5000, Test: 0.4600\n",
      "Epoch: 035, Loss: 0.6996, Train: 0.4858, Val: 0.5000, Test: 0.5533\n",
      "Epoch: 036, Loss: 0.6937, Train: 0.5054, Val: 0.5033, Test: 0.4600\n",
      "Epoch: 037, Loss: 0.6962, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 038, Loss: 0.6950, Train: 0.4954, Val: 0.5200, Test: 0.5433\n",
      "Epoch: 039, Loss: 0.6955, Train: 0.5038, Val: 0.5033, Test: 0.4633\n",
      "Epoch: 040, Loss: 0.6948, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 041, Loss: 0.6980, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 042, Loss: 0.6947, Train: 0.4963, Val: 0.4633, Test: 0.5633\n",
      "Epoch: 043, Loss: 0.6985, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 044, Loss: 0.6967, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 045, Loss: 0.6977, Train: 0.4879, Val: 0.5133, Test: 0.5467\n",
      "Epoch: 046, Loss: 0.6954, Train: 0.5050, Val: 0.4967, Test: 0.4600\n",
      "Epoch: 047, Loss: 0.6948, Train: 0.5058, Val: 0.4967, Test: 0.4567\n",
      "Epoch: 048, Loss: 0.6956, Train: 0.5033, Val: 0.5033, Test: 0.4633\n",
      "Epoch: 049, Loss: 0.6958, Train: 0.4942, Val: 0.5033, Test: 0.5433\n",
      "Epoch: 050, Loss: 0.6952, Train: 0.5046, Val: 0.5000, Test: 0.4600\n",
      "### Run 0 - val loss: 0.693, test acc: 0.560\n",
      "Accuracies in each run:  [0.56]\n",
      "test acc - mean: 0.560, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'kmis'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk\n",
      "Epoch: 001, Loss: 0.7006, Train: 0.4992, Val: 0.5333, Test: 0.4733\n",
      "Epoch: 002, Loss: 0.6938, Train: 0.4979, Val: 0.5400, Test: 0.5100\n",
      "Epoch: 003, Loss: 0.6946, Train: 0.4988, Val: 0.5367, Test: 0.4733\n",
      "Epoch: 004, Loss: 0.6941, Train: 0.5142, Val: 0.5100, Test: 0.5033\n",
      "Epoch: 005, Loss: 0.6934, Train: 0.4988, Val: 0.5367, Test: 0.4733\n",
      "Epoch: 006, Loss: 0.6940, Train: 0.5225, Val: 0.4900, Test: 0.5233\n",
      "Epoch: 007, Loss: 0.6931, Train: 0.4963, Val: 0.4900, Test: 0.5000\n",
      "Epoch: 008, Loss: 0.6950, Train: 0.4913, Val: 0.5333, Test: 0.5033\n",
      "Epoch: 009, Loss: 0.6955, Train: 0.5046, Val: 0.4967, Test: 0.5033\n",
      "Epoch: 010, Loss: 0.6954, Train: 0.5050, Val: 0.4967, Test: 0.5000\n",
      "Epoch: 011, Loss: 0.6950, Train: 0.5192, Val: 0.5133, Test: 0.5100\n",
      "Epoch: 012, Loss: 0.6944, Train: 0.4913, Val: 0.5333, Test: 0.5033\n",
      "Epoch: 013, Loss: 0.6937, Train: 0.5225, Val: 0.4900, Test: 0.5233\n",
      "Epoch: 014, Loss: 0.6927, Train: 0.5200, Val: 0.5233, Test: 0.5267\n",
      "Epoch: 015, Loss: 0.6940, Train: 0.5100, Val: 0.5100, Test: 0.5100\n",
      "Epoch: 016, Loss: 0.6948, Train: 0.5138, Val: 0.5167, Test: 0.5100\n",
      "Epoch: 017, Loss: 0.6939, Train: 0.5058, Val: 0.4967, Test: 0.5000\n",
      "Epoch: 018, Loss: 0.6927, Train: 0.5075, Val: 0.5400, Test: 0.5067\n",
      "Epoch: 019, Loss: 0.6960, Train: 0.4988, Val: 0.5367, Test: 0.4733\n",
      "Epoch: 020, Loss: 0.6944, Train: 0.4896, Val: 0.5333, Test: 0.5000\n",
      "Epoch: 021, Loss: 0.6928, Train: 0.5200, Val: 0.5233, Test: 0.5267\n",
      "Epoch: 022, Loss: 0.6934, Train: 0.4988, Val: 0.5367, Test: 0.4733\n",
      "Epoch: 023, Loss: 0.6947, Train: 0.4929, Val: 0.4867, Test: 0.4900\n",
      "Epoch: 024, Loss: 0.6941, Train: 0.5083, Val: 0.5167, Test: 0.4933\n",
      "Epoch: 025, Loss: 0.6947, Train: 0.5204, Val: 0.4800, Test: 0.5267\n",
      "Epoch: 026, Loss: 0.6958, Train: 0.5042, Val: 0.5400, Test: 0.5067\n",
      "Epoch: 027, Loss: 0.6948, Train: 0.4983, Val: 0.5367, Test: 0.4733\n",
      "Epoch: 028, Loss: 0.6952, Train: 0.4988, Val: 0.5367, Test: 0.4733\n",
      "Epoch: 029, Loss: 0.6942, Train: 0.4992, Val: 0.5333, Test: 0.4733\n",
      "Epoch: 030, Loss: 0.6943, Train: 0.4983, Val: 0.5367, Test: 0.4733\n",
      "Epoch: 031, Loss: 0.6932, Train: 0.5283, Val: 0.4967, Test: 0.5300\n",
      "Epoch: 032, Loss: 0.6940, Train: 0.5054, Val: 0.5433, Test: 0.5033\n",
      "Epoch: 033, Loss: 0.6941, Train: 0.5212, Val: 0.4800, Test: 0.5267\n",
      "Epoch: 034, Loss: 0.6941, Train: 0.5083, Val: 0.5000, Test: 0.5100\n",
      "Epoch: 035, Loss: 0.6941, Train: 0.5000, Val: 0.5467, Test: 0.5067\n",
      "Epoch: 036, Loss: 0.6942, Train: 0.5204, Val: 0.5233, Test: 0.5300\n",
      "Epoch: 037, Loss: 0.6942, Train: 0.5183, Val: 0.5200, Test: 0.5100\n",
      "Epoch: 038, Loss: 0.6937, Train: 0.4992, Val: 0.5333, Test: 0.4733\n",
      "Epoch: 039, Loss: 0.6959, Train: 0.4992, Val: 0.5333, Test: 0.4733\n",
      "Epoch: 040, Loss: 0.6943, Train: 0.5046, Val: 0.5433, Test: 0.5033\n",
      "Epoch: 041, Loss: 0.6930, Train: 0.4975, Val: 0.5367, Test: 0.5033\n",
      "Epoch: 042, Loss: 0.6927, Train: 0.5042, Val: 0.4933, Test: 0.5100\n",
      "Epoch: 043, Loss: 0.6938, Train: 0.5083, Val: 0.4967, Test: 0.5133\n",
      "Epoch: 044, Loss: 0.6945, Train: 0.4917, Val: 0.5300, Test: 0.4900\n",
      "Epoch: 045, Loss: 0.6940, Train: 0.5146, Val: 0.5067, Test: 0.5033\n",
      "Epoch: 046, Loss: 0.6928, Train: 0.5054, Val: 0.5400, Test: 0.5033\n",
      "Epoch: 047, Loss: 0.6918, Train: 0.5162, Val: 0.4900, Test: 0.5467\n",
      "Epoch: 048, Loss: 0.6936, Train: 0.5204, Val: 0.5133, Test: 0.5300\n",
      "Epoch: 049, Loss: 0.6935, Train: 0.5062, Val: 0.4667, Test: 0.5067\n",
      "Epoch: 050, Loss: 0.6937, Train: 0.4908, Val: 0.4867, Test: 0.4867\n",
      "### Run 0 - val loss: 0.691, test acc: 0.473\n",
      "Accuracies in each run:  [0.47333333333333333]\n",
      "test acc - mean: 0.473, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'topk'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panpool\n",
      "Epoch: 001, Loss: 0.7029, Train: 0.4996, Val: 0.4967, Test: 0.5167\n",
      "Epoch: 002, Loss: 0.6946, Train: 0.5017, Val: 0.5067, Test: 0.5267\n",
      "Epoch: 003, Loss: 0.6920, Train: 0.5004, Val: 0.4967, Test: 0.5133\n",
      "Epoch: 004, Loss: 0.6903, Train: 0.5667, Val: 0.5567, Test: 0.5800\n",
      "Epoch: 005, Loss: 0.6924, Train: 0.5000, Val: 0.5000, Test: 0.5167\n",
      "Epoch: 006, Loss: 0.6908, Train: 0.5621, Val: 0.5433, Test: 0.6133\n",
      "Epoch: 007, Loss: 0.6942, Train: 0.5621, Val: 0.5567, Test: 0.5567\n",
      "Epoch: 008, Loss: 0.6865, Train: 0.5513, Val: 0.5333, Test: 0.5233\n",
      "Epoch: 009, Loss: 0.6906, Train: 0.5613, Val: 0.5667, Test: 0.5800\n",
      "Epoch: 010, Loss: 0.6863, Train: 0.5413, Val: 0.5200, Test: 0.5167\n",
      "Epoch: 011, Loss: 0.6892, Train: 0.5046, Val: 0.5067, Test: 0.4867\n",
      "Epoch: 012, Loss: 0.6893, Train: 0.5600, Val: 0.5367, Test: 0.5533\n",
      "Epoch: 013, Loss: 0.6892, Train: 0.5658, Val: 0.5533, Test: 0.6033\n",
      "Epoch: 014, Loss: 0.6920, Train: 0.5179, Val: 0.5200, Test: 0.5633\n",
      "Epoch: 015, Loss: 0.6890, Train: 0.5408, Val: 0.5267, Test: 0.5133\n",
      "Epoch: 016, Loss: 0.6854, Train: 0.5667, Val: 0.5233, Test: 0.6267\n",
      "Epoch: 017, Loss: 0.6855, Train: 0.5333, Val: 0.5100, Test: 0.5100\n",
      "Epoch: 018, Loss: 0.6879, Train: 0.5129, Val: 0.5067, Test: 0.5100\n",
      "Epoch: 019, Loss: 0.6893, Train: 0.5608, Val: 0.5700, Test: 0.5633\n",
      "Epoch: 020, Loss: 0.6887, Train: 0.5613, Val: 0.5700, Test: 0.5633\n",
      "Epoch: 021, Loss: 0.6859, Train: 0.5679, Val: 0.5567, Test: 0.6000\n",
      "Epoch: 022, Loss: 0.6870, Train: 0.5517, Val: 0.5467, Test: 0.5367\n",
      "Epoch: 023, Loss: 0.6877, Train: 0.5617, Val: 0.5700, Test: 0.5800\n",
      "Epoch: 024, Loss: 0.6861, Train: 0.5592, Val: 0.5300, Test: 0.6233\n",
      "Epoch: 025, Loss: 0.6872, Train: 0.5583, Val: 0.5400, Test: 0.5500\n",
      "Epoch: 026, Loss: 0.6882, Train: 0.5408, Val: 0.5167, Test: 0.5233\n",
      "Epoch: 027, Loss: 0.6872, Train: 0.5663, Val: 0.5533, Test: 0.6033\n",
      "Epoch: 028, Loss: 0.6861, Train: 0.5550, Val: 0.5467, Test: 0.5933\n",
      "Epoch: 029, Loss: 0.6849, Train: 0.5637, Val: 0.5333, Test: 0.6200\n",
      "Epoch: 030, Loss: 0.6883, Train: 0.5596, Val: 0.5467, Test: 0.5533\n",
      "Epoch: 031, Loss: 0.6864, Train: 0.5675, Val: 0.5400, Test: 0.6267\n",
      "Epoch: 032, Loss: 0.6869, Train: 0.5604, Val: 0.5400, Test: 0.5567\n",
      "Epoch: 033, Loss: 0.6849, Train: 0.5583, Val: 0.5433, Test: 0.5500\n",
      "Epoch: 034, Loss: 0.6862, Train: 0.5396, Val: 0.5233, Test: 0.5167\n",
      "Epoch: 035, Loss: 0.6841, Train: 0.5608, Val: 0.5433, Test: 0.5467\n",
      "Epoch: 036, Loss: 0.6900, Train: 0.5425, Val: 0.5367, Test: 0.5933\n",
      "Epoch: 037, Loss: 0.6878, Train: 0.5667, Val: 0.5433, Test: 0.6100\n",
      "Epoch: 038, Loss: 0.6870, Train: 0.5642, Val: 0.5700, Test: 0.5800\n",
      "Epoch: 039, Loss: 0.6848, Train: 0.5663, Val: 0.5467, Test: 0.6067\n",
      "Epoch: 040, Loss: 0.6829, Train: 0.5650, Val: 0.5467, Test: 0.6000\n",
      "Epoch: 041, Loss: 0.6879, Train: 0.5646, Val: 0.5700, Test: 0.5867\n",
      "Epoch: 042, Loss: 0.6830, Train: 0.5617, Val: 0.5667, Test: 0.5933\n",
      "Epoch: 043, Loss: 0.6820, Train: 0.5458, Val: 0.5367, Test: 0.5900\n",
      "Epoch: 044, Loss: 0.6880, Train: 0.5608, Val: 0.5467, Test: 0.5533\n",
      "Epoch: 045, Loss: 0.6843, Train: 0.5579, Val: 0.5467, Test: 0.5467\n",
      "Epoch: 046, Loss: 0.6851, Train: 0.5646, Val: 0.5467, Test: 0.5967\n",
      "Epoch: 047, Loss: 0.6864, Train: 0.5579, Val: 0.5433, Test: 0.5500\n",
      "Epoch: 048, Loss: 0.6858, Train: 0.5658, Val: 0.5367, Test: 0.6267\n",
      "Epoch: 049, Loss: 0.6826, Train: 0.5696, Val: 0.5467, Test: 0.6133\n",
      "Epoch: 050, Loss: 0.6828, Train: 0.5617, Val: 0.5467, Test: 0.5567\n",
      "### Run 0 - val loss: 0.685, test acc: 0.597\n",
      "Accuracies in each run:  [0.5966666666666667]\n",
      "test acc - mean: 0.597, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'panpool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asapool\n",
      "Epoch: 001, Loss: 0.6986, Train: 0.4858, Val: 0.4800, Test: 0.5300\n",
      "Epoch: 002, Loss: 0.6982, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 003, Loss: 0.6979, Train: 0.5154, Val: 0.5167, Test: 0.5700\n",
      "Epoch: 004, Loss: 0.6957, Train: 0.4950, Val: 0.4933, Test: 0.5467\n",
      "Epoch: 005, Loss: 0.6938, Train: 0.4983, Val: 0.5067, Test: 0.5100\n",
      "Epoch: 006, Loss: 0.6965, Train: 0.4938, Val: 0.5267, Test: 0.5367\n",
      "Epoch: 007, Loss: 0.6946, Train: 0.5021, Val: 0.4933, Test: 0.4900\n",
      "Epoch: 008, Loss: 0.6958, Train: 0.5133, Val: 0.5300, Test: 0.5467\n",
      "Epoch: 009, Loss: 0.6935, Train: 0.5167, Val: 0.5167, Test: 0.5533\n",
      "Epoch: 010, Loss: 0.6951, Train: 0.5283, Val: 0.4900, Test: 0.5367\n",
      "Epoch: 011, Loss: 0.6941, Train: 0.5033, Val: 0.5267, Test: 0.5500\n",
      "Epoch: 012, Loss: 0.6949, Train: 0.4950, Val: 0.5133, Test: 0.4967\n",
      "Epoch: 013, Loss: 0.6946, Train: 0.4963, Val: 0.5000, Test: 0.4700\n",
      "Epoch: 014, Loss: 0.6938, Train: 0.4942, Val: 0.5033, Test: 0.5333\n",
      "Epoch: 015, Loss: 0.6933, Train: 0.4846, Val: 0.4933, Test: 0.4833\n",
      "Epoch: 016, Loss: 0.6948, Train: 0.4875, Val: 0.4900, Test: 0.4900\n",
      "Epoch: 017, Loss: 0.6952, Train: 0.4913, Val: 0.5267, Test: 0.5333\n",
      "Epoch: 018, Loss: 0.6951, Train: 0.4875, Val: 0.5200, Test: 0.5233\n",
      "Epoch: 019, Loss: 0.6938, Train: 0.4888, Val: 0.5200, Test: 0.5333\n",
      "Epoch: 020, Loss: 0.6951, Train: 0.4917, Val: 0.5200, Test: 0.5367\n",
      "Epoch: 021, Loss: 0.6918, Train: 0.4933, Val: 0.4900, Test: 0.5433\n",
      "Epoch: 022, Loss: 0.6941, Train: 0.4938, Val: 0.4967, Test: 0.4900\n",
      "Epoch: 023, Loss: 0.6940, Train: 0.4900, Val: 0.4867, Test: 0.4800\n",
      "Epoch: 024, Loss: 0.6962, Train: 0.4938, Val: 0.4867, Test: 0.5433\n",
      "Epoch: 025, Loss: 0.6944, Train: 0.4917, Val: 0.5233, Test: 0.5400\n",
      "Epoch: 026, Loss: 0.6956, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 027, Loss: 0.6935, Train: 0.4842, Val: 0.4900, Test: 0.4967\n",
      "Epoch: 028, Loss: 0.6935, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 029, Loss: 0.6941, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 030, Loss: 0.6953, Train: 0.4896, Val: 0.5233, Test: 0.5367\n",
      "Epoch: 031, Loss: 0.6951, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 032, Loss: 0.6942, Train: 0.4863, Val: 0.5033, Test: 0.5333\n",
      "Epoch: 033, Loss: 0.6947, Train: 0.4942, Val: 0.4967, Test: 0.4967\n",
      "Epoch: 034, Loss: 0.6935, Train: 0.4913, Val: 0.4967, Test: 0.5433\n",
      "Epoch: 035, Loss: 0.6934, Train: 0.4975, Val: 0.5033, Test: 0.5033\n",
      "Epoch: 036, Loss: 0.6931, Train: 0.5012, Val: 0.4933, Test: 0.4967\n",
      "Epoch: 037, Loss: 0.6943, Train: 0.4967, Val: 0.5067, Test: 0.5133\n",
      "Epoch: 038, Loss: 0.6946, Train: 0.4967, Val: 0.5033, Test: 0.5100\n",
      "Epoch: 039, Loss: 0.6944, Train: 0.4921, Val: 0.4933, Test: 0.4833\n",
      "Epoch: 040, Loss: 0.6934, Train: 0.4942, Val: 0.4933, Test: 0.4767\n",
      "Epoch: 041, Loss: 0.6949, Train: 0.4954, Val: 0.4967, Test: 0.5067\n",
      "Epoch: 042, Loss: 0.6947, Train: 0.4921, Val: 0.5267, Test: 0.5367\n",
      "Epoch: 043, Loss: 0.6938, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 044, Loss: 0.6930, Train: 0.5038, Val: 0.4867, Test: 0.4867\n",
      "Epoch: 045, Loss: 0.6950, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 046, Loss: 0.6934, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 047, Loss: 0.6934, Train: 0.4967, Val: 0.5133, Test: 0.5133\n",
      "Epoch: 048, Loss: 0.6939, Train: 0.4979, Val: 0.5033, Test: 0.5133\n",
      "Epoch: 049, Loss: 0.6938, Train: 0.5017, Val: 0.4967, Test: 0.4900\n",
      "Epoch: 050, Loss: 0.6932, Train: 0.4983, Val: 0.5067, Test: 0.5133\n",
      "### Run 0 - val loss: 0.692, test acc: 0.553\n",
      "Accuracies in each run:  [0.5533333333333333]\n",
      "test acc - mean: 0.553, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'asapool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagpool\n",
      "Epoch: 001, Loss: 0.6960, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 002, Loss: 0.6951, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 003, Loss: 0.6962, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 004, Loss: 0.6942, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 005, Loss: 0.6939, Train: 0.5150, Val: 0.5300, Test: 0.4233\n",
      "Epoch: 006, Loss: 0.6932, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 007, Loss: 0.6924, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 008, Loss: 0.6945, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 009, Loss: 0.6944, Train: 0.5138, Val: 0.5100, Test: 0.4400\n",
      "Epoch: 010, Loss: 0.6940, Train: 0.5021, Val: 0.5167, Test: 0.4700\n",
      "Epoch: 011, Loss: 0.6938, Train: 0.5025, Val: 0.5100, Test: 0.5167\n",
      "Epoch: 012, Loss: 0.6937, Train: 0.4975, Val: 0.4767, Test: 0.5400\n",
      "Epoch: 013, Loss: 0.6943, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 014, Loss: 0.6942, Train: 0.5025, Val: 0.5100, Test: 0.5233\n",
      "Epoch: 015, Loss: 0.6947, Train: 0.5146, Val: 0.5267, Test: 0.4600\n",
      "Epoch: 016, Loss: 0.6921, Train: 0.4988, Val: 0.4800, Test: 0.5300\n",
      "Epoch: 017, Loss: 0.6932, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 018, Loss: 0.6929, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 019, Loss: 0.6938, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 020, Loss: 0.6947, Train: 0.5042, Val: 0.5167, Test: 0.4667\n",
      "Epoch: 021, Loss: 0.6930, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 022, Loss: 0.6942, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 023, Loss: 0.6947, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 024, Loss: 0.6932, Train: 0.4988, Val: 0.4800, Test: 0.5300\n",
      "Epoch: 025, Loss: 0.6940, Train: 0.5033, Val: 0.5133, Test: 0.4733\n",
      "Epoch: 026, Loss: 0.6953, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 027, Loss: 0.6934, Train: 0.5096, Val: 0.5167, Test: 0.4800\n",
      "Epoch: 028, Loss: 0.6935, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 029, Loss: 0.6945, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 030, Loss: 0.6935, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 031, Loss: 0.6936, Train: 0.5121, Val: 0.5267, Test: 0.5100\n",
      "Epoch: 032, Loss: 0.6928, Train: 0.4988, Val: 0.4800, Test: 0.5300\n",
      "Epoch: 033, Loss: 0.6927, Train: 0.5100, Val: 0.5167, Test: 0.4800\n",
      "Epoch: 034, Loss: 0.6943, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 035, Loss: 0.6928, Train: 0.5092, Val: 0.5167, Test: 0.4800\n",
      "Epoch: 036, Loss: 0.6929, Train: 0.5000, Val: 0.4733, Test: 0.5467\n",
      "Epoch: 037, Loss: 0.6938, Train: 0.5067, Val: 0.5167, Test: 0.4733\n",
      "Epoch: 038, Loss: 0.6939, Train: 0.5017, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 039, Loss: 0.6937, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 040, Loss: 0.6936, Train: 0.5025, Val: 0.5167, Test: 0.4733\n",
      "Epoch: 041, Loss: 0.6934, Train: 0.5138, Val: 0.5133, Test: 0.4867\n",
      "Epoch: 042, Loss: 0.6940, Train: 0.5017, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 043, Loss: 0.6938, Train: 0.5275, Val: 0.5633, Test: 0.5000\n",
      "Epoch: 044, Loss: 0.6945, Train: 0.5012, Val: 0.5200, Test: 0.4700\n",
      "Epoch: 045, Loss: 0.6934, Train: 0.5238, Val: 0.5367, Test: 0.5067\n",
      "Epoch: 046, Loss: 0.6928, Train: 0.4992, Val: 0.4800, Test: 0.5333\n",
      "Epoch: 047, Loss: 0.6926, Train: 0.4992, Val: 0.4933, Test: 0.5433\n",
      "Epoch: 048, Loss: 0.6935, Train: 0.5208, Val: 0.5233, Test: 0.4933\n",
      "Epoch: 049, Loss: 0.6932, Train: 0.5371, Val: 0.5500, Test: 0.5100\n",
      "Epoch: 050, Loss: 0.6929, Train: 0.5325, Val: 0.5333, Test: 0.5000\n",
      "### Run 0 - val loss: 0.692, test acc: 0.470\n",
      "Accuracies in each run:  [0.47]\n",
      "test acc - mean: 0.470, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'sagpool'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense-random\n",
      "Epoch: 001, Loss: 0.8809, Train: 0.4958, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 002, Loss: 0.7698, Train: 0.4958, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 003, Loss: 0.7363, Train: 0.4958, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 004, Loss: 0.7252, Train: 0.5008, Val: 0.5033, Test: 0.5033\n",
      "Epoch: 005, Loss: 0.7157, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 006, Loss: 0.7102, Train: 0.4958, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 007, Loss: 0.7086, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 008, Loss: 0.7063, Train: 0.5033, Val: 0.5133, Test: 0.4600\n",
      "Epoch: 009, Loss: 0.7053, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 010, Loss: 0.7044, Train: 0.4958, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 011, Loss: 0.7109, Train: 0.5108, Val: 0.5000, Test: 0.4767\n",
      "Epoch: 012, Loss: 0.7036, Train: 0.5088, Val: 0.4700, Test: 0.4667\n",
      "Epoch: 013, Loss: 0.7057, Train: 0.5029, Val: 0.4867, Test: 0.4900\n",
      "Epoch: 014, Loss: 0.7035, Train: 0.5012, Val: 0.4933, Test: 0.4967\n",
      "Epoch: 015, Loss: 0.7034, Train: 0.5125, Val: 0.5033, Test: 0.4667\n",
      "Epoch: 016, Loss: 0.7021, Train: 0.4958, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 017, Loss: 0.7057, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 018, Loss: 0.7052, Train: 0.5029, Val: 0.4867, Test: 0.4900\n",
      "Epoch: 019, Loss: 0.6976, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 020, Loss: 0.7011, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 021, Loss: 0.7006, Train: 0.5083, Val: 0.4600, Test: 0.4800\n",
      "Epoch: 022, Loss: 0.7051, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 023, Loss: 0.7004, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 024, Loss: 0.7006, Train: 0.5175, Val: 0.5067, Test: 0.4667\n",
      "Epoch: 025, Loss: 0.7025, Train: 0.5075, Val: 0.4633, Test: 0.4833\n",
      "Epoch: 026, Loss: 0.7028, Train: 0.5038, Val: 0.5067, Test: 0.4700\n",
      "Epoch: 027, Loss: 0.7007, Train: 0.4958, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 028, Loss: 0.7026, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 029, Loss: 0.6978, Train: 0.5062, Val: 0.5167, Test: 0.4567\n",
      "Epoch: 030, Loss: 0.7037, Train: 0.5042, Val: 0.5100, Test: 0.4633\n",
      "Epoch: 031, Loss: 0.6981, Train: 0.5108, Val: 0.4733, Test: 0.5100\n",
      "Epoch: 032, Loss: 0.6982, Train: 0.5071, Val: 0.4933, Test: 0.4733\n",
      "Epoch: 033, Loss: 0.7004, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 034, Loss: 0.6946, Train: 0.5067, Val: 0.4633, Test: 0.4867\n",
      "Epoch: 035, Loss: 0.6958, Train: 0.4979, Val: 0.5200, Test: 0.5067\n",
      "Epoch: 036, Loss: 0.6995, Train: 0.4958, Val: 0.5200, Test: 0.5133\n",
      "Epoch: 037, Loss: 0.7022, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 038, Loss: 0.6951, Train: 0.5108, Val: 0.4700, Test: 0.5067\n",
      "Epoch: 039, Loss: 0.6987, Train: 0.5096, Val: 0.4800, Test: 0.4633\n",
      "Epoch: 040, Loss: 0.6993, Train: 0.5012, Val: 0.4933, Test: 0.4967\n",
      "Epoch: 041, Loss: 0.6943, Train: 0.5046, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 042, Loss: 0.6990, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 043, Loss: 0.7004, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 044, Loss: 0.6979, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 045, Loss: 0.6991, Train: 0.5071, Val: 0.4800, Test: 0.4633\n",
      "Epoch: 046, Loss: 0.6963, Train: 0.5046, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 047, Loss: 0.6959, Train: 0.5042, Val: 0.4800, Test: 0.4867\n",
      "Epoch: 048, Loss: 0.6976, Train: 0.5075, Val: 0.4800, Test: 0.4767\n",
      "Epoch: 049, Loss: 0.6987, Train: 0.5058, Val: 0.4867, Test: 0.4700\n",
      "Epoch: 050, Loss: 0.6976, Train: 0.5033, Val: 0.5133, Test: 0.4600\n",
      "### Run 0 - val loss: 0.692, test acc: 0.513\n",
      "Accuracies in each run:  [0.5133333333333333]\n",
      "test acc - mean: 0.513, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'dense-random'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp-graclus\n",
      "Epoch: 001, Loss: 0.7440, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 002, Loss: 0.7172, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 003, Loss: 0.7097, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 004, Loss: 0.7085, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 005, Loss: 0.7039, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 006, Loss: 0.7018, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 007, Loss: 0.7027, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 008, Loss: 0.7076, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 009, Loss: 0.7050, Train: 0.5012, Val: 0.4900, Test: 0.4900\n",
      "Epoch: 010, Loss: 0.7013, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 011, Loss: 0.7041, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 012, Loss: 0.7003, Train: 0.5054, Val: 0.4733, Test: 0.4833\n",
      "Epoch: 013, Loss: 0.6999, Train: 0.4958, Val: 0.5167, Test: 0.5133\n",
      "Epoch: 014, Loss: 0.7029, Train: 0.5042, Val: 0.4767, Test: 0.4833\n",
      "Epoch: 015, Loss: 0.7035, Train: 0.4963, Val: 0.5100, Test: 0.5200\n",
      "Epoch: 016, Loss: 0.7043, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 017, Loss: 0.6999, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 018, Loss: 0.6964, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 019, Loss: 0.6953, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 020, Loss: 0.6984, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 021, Loss: 0.7030, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 022, Loss: 0.6997, Train: 0.4963, Val: 0.5100, Test: 0.5200\n",
      "Epoch: 023, Loss: 0.6967, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 024, Loss: 0.7016, Train: 0.5021, Val: 0.4767, Test: 0.4833\n",
      "Epoch: 025, Loss: 0.6982, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 026, Loss: 0.6961, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 027, Loss: 0.6971, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 028, Loss: 0.6992, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 029, Loss: 0.6952, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 030, Loss: 0.6971, Train: 0.5025, Val: 0.4800, Test: 0.4900\n",
      "Epoch: 031, Loss: 0.6981, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 032, Loss: 0.6982, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 033, Loss: 0.6955, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 034, Loss: 0.6993, Train: 0.5071, Val: 0.4633, Test: 0.4800\n",
      "Epoch: 035, Loss: 0.6987, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 036, Loss: 0.6958, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 037, Loss: 0.6988, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 038, Loss: 0.6972, Train: 0.5033, Val: 0.4900, Test: 0.4833\n",
      "Epoch: 039, Loss: 0.6992, Train: 0.4971, Val: 0.5100, Test: 0.5133\n",
      "Epoch: 040, Loss: 0.6976, Train: 0.4963, Val: 0.5100, Test: 0.5200\n",
      "Epoch: 041, Loss: 0.6951, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 042, Loss: 0.6968, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 043, Loss: 0.6989, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 044, Loss: 0.6980, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 045, Loss: 0.6960, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 046, Loss: 0.6942, Train: 0.5079, Val: 0.4733, Test: 0.4933\n",
      "Epoch: 047, Loss: 0.6973, Train: 0.5029, Val: 0.4900, Test: 0.4867\n",
      "Epoch: 048, Loss: 0.6970, Train: 0.4971, Val: 0.5233, Test: 0.4867\n",
      "Epoch: 049, Loss: 0.6968, Train: 0.5042, Val: 0.4800, Test: 0.4900\n",
      "Epoch: 050, Loss: 0.6965, Train: 0.5067, Val: 0.4600, Test: 0.4833\n",
      "### Run 0 - val loss: 0.693, test acc: 0.520\n",
      "Accuracies in each run:  [0.52]\n",
      "test acc - mean: 0.520, std: 0.000\n"
     ]
    }
   ],
   "source": [
    "tot_acc = []\n",
    "args.pooling = 'comp-graclus'\n",
    "print(args.pooling)\n",
    "for r in range(args.runs):  \n",
    "    # Random shuffle the data\n",
    "    rnd_idx = rng.permutation(len(dataset))\n",
    "    dataset = dataset[list(rnd_idx)]\n",
    "    \n",
    "    train_dataset = dataset[len(dataset) // 5:]\n",
    "    val_dataset = dataset[:len(dataset) // 10]\n",
    "    test_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, args.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, args.batch_size)\n",
    "    \n",
    "    # Init the GNN\n",
    "    net_model = GCN_Pool_Net(in_channels=train_dataset.num_features, \n",
    "                        out_channels=train_dataset.num_classes,\n",
    "                        num_layers_pre=args.num_layers_pre,\n",
    "                        num_layers_post=args.num_layers_post,\n",
    "                        hidden_channels=args.hidden_channels,\n",
    "                        average_nodes=avg_nodes,\n",
    "                        pooling=args.pooling,\n",
    "                        pool_ratio=args.pool_ratio,\n",
    "                        max_nodes=max_nodes\n",
    "                        ).to(device)\n",
    "    opt = torch.optim.Adam(net_model.parameters(), lr=args.lr) \n",
    "    best_val=np.inf\n",
    "    best_test=0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss = train(net_model, train_loader, opt)\n",
    "        train_acc, _ = test(net_model, train_loader)\n",
    "        val_acc, val_loss = test(net_model, val_loader)\n",
    "        test_acc, _ = test(net_model, test_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_test = test_acc\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "        \n",
    "    tot_acc.append(best_test)\n",
    "    print(f\"### Run {r:d} - val loss: {best_val:.3f}, test acc: {best_test:.3f}\")\n",
    "    \n",
    "print(\"Accuracies in each run: \", tot_acc)    \n",
    "print(f\"test acc - mean: {np.mean(tot_acc):.3f}, std: {np.std(tot_acc):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
